{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " ## Load pre-trained models in PyTorch, modify them to fit your dataset, and perform fine-tuning to make the most of the pre-trained knowledge"
      ],
      "metadata": {
        "id": "thfEQPp9O1xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This code loads the VGG16 model with pre-trained weights. You can now use this model directly on the same type of data (i.e., ImageNet-like images) that it was trained on. However, in most cases, you’ll want to adapt it to work with your own data,"
      ],
      "metadata": {
        "id": "kQ1bW9jaPYji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1-puqCeKGD_",
        "outputId": "12525cad-8e2e-4002-efdd-6b139d955737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230M/230M [00:01<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load pre-trained Res152 model\n",
        "model = models.resnet152(pretrained=True)\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since the original VGG16 model was trained to classify images into 1,000 classes (from ImageNet), we need to make a few adjustments to the model architecture.\n",
        "\n",
        "* Common Modifications:\n",
        "Changing the Output Layer: The number of output features (classes) in the final layer needs to match the number of classes in your dataset.\n",
        "Modifying Pooling Layers: In some cases, the image size might not be compatible with the pooling layers in the original model, so you might need to modify these layers."
      ],
      "metadata": {
        "id": "eOsAYJYEPoBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qEU_we7g-oY",
        "outputId": "378c7be6-a5a3-4a94-af46-0d9f2da462ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=2048, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Replace the classifier of the ResNet152 model\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "# Print the modified model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0ofveU0KcOp",
        "outputId": "2fe43640-8f22-4f7e-b2c4-d272a5a9123c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (7): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (12): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (13): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (14): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (15): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (16): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (17): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (18): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (19): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (20): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (21): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (22): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (23): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (24): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (25): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (26): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (27): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (28): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (29): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (30): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (31): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (32): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (33): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (34): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (35): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "DUH6FicsKj3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "X10AQS03KsTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet replaces the final layer of the VGG16 model (which originally outputs 1,000 classes for ImageNet) with a new linear layer that outputs 10 classes (for CIFAR-10)."
      ],
      "metadata": {
        "id": "s-raib6cP5Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code shows how you can freeze all the layers except the classifier. Now, only the classifier layers will be updated during training, making training much faster and computationally less expensive."
      ],
      "metadata": {
        "id": "BPdt9XE1QTBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the classifier for CIFAR-10 (10 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "m8x8r2LqKxl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])"
      ],
      "metadata": {
        "id": "npgpEbbXK6eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We are using PyTorch’s built-in torchvision.datasets to load the CIFAR-10 dataset.\n",
        "\n",
        "* CIFAR-10  is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "\n",
        "* We apply basic transformations like resizing and normalization.\n"
      ],
      "metadata": {
        "id": "kc0mepbqN3z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gucJEL8hK-w7",
        "outputId": "f41474cd-4cb5-4c76-8a13-21920e421aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hACVCTeLajO",
        "outputId": "6dbf2e0a-4702-489a-a1b1-02ca05d51d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (14): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (15): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (16): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (17): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (18): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (19): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (20): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (21): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (22): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (23): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (24): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (25): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (26): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (27): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (28): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (29): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (30): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (31): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (32): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (33): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (34): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (35): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "# Training loop\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        print(running_loss/len(train_loader))\n",
        "        #print(f'Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%’)\n",
        "#### Step 4: E"
      ],
      "metadata": {
        "id": "nvsETpuELiWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6613bf-f2f2-42e4-87ad-e780bf0e2d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.8158626454260648\n",
            "0.817590249926233\n",
            "0.8189364622925859\n",
            "0.8205659035831461\n",
            "0.8224546602924766\n",
            "0.8243435359062137\n",
            "0.8262334479700268\n",
            "0.8282219464211817\n",
            "0.8299720483972594\n",
            "0.8318072196162875\n",
            "0.8332972494537568\n",
            "0.8350962403485233\n",
            "0.836628890860721\n",
            "0.8384551834267424\n",
            "0.8402344756723975\n",
            "0.842171171132256\n",
            "0.8435898223496459\n",
            "0.8452271852651825\n",
            "0.84689326176558\n",
            "0.8485071095054412\n",
            "0.8502574213935287\n",
            "0.8520351660526012\n",
            "0.8535897111343911\n",
            "0.8550708365562322\n",
            "0.8568749860729403\n",
            "0.8585504476371628\n",
            "0.8603668740338377\n",
            "0.8618824783798373\n",
            "0.863469260732841\n",
            "0.8654185152419692\n",
            "0.8671456764421195\n",
            "0.8687280616187074\n",
            "0.8704616339005473\n",
            "0.8723287742461086\n",
            "0.8740019560469996\n",
            "0.8758602268860468\n",
            "0.8775554685031667\n",
            "0.8794714331322009\n",
            "0.8815186795066384\n",
            "0.8836206479755508\n",
            "0.8853256251196118\n",
            "0.8871153010736645\n",
            "0.8889820908036683\n",
            "0.8904990038603467\n",
            "0.8922722252738445\n",
            "0.8940028590924295\n",
            "0.8958009937230278\n",
            "0.8972839460043651\n",
            "0.8991103564077021\n",
            "0.9007161688011931\n",
            "0.9026413120881981\n",
            "0.9041888026325294\n",
            "0.9060481958986853\n",
            "0.9078107174400174\n",
            "0.9098422537984141\n",
            "0.9117445939642084\n",
            "0.9134005573399536\n",
            "0.9150070833123248\n",
            "0.9166647278134475\n",
            "0.9185388722383153\n",
            "0.9203273086901516\n",
            "0.9221097956533018\n",
            "0.9236191255052376\n",
            "0.9254870718092565\n",
            "0.9271205079829906\n",
            "0.9289160881505902\n",
            "0.9304583694623865\n",
            "0.9320790261563743\n",
            "0.9340448644764893\n",
            "0.9355409794756214\n",
            "0.9374153194829936\n",
            "0.9392047193654053\n",
            "0.940730525709479\n",
            "0.9423621876160507\n",
            "0.9440781257646468\n",
            "0.9455957633759969\n",
            "0.9472454665871837\n",
            "0.9487214434482253\n",
            "0.9506145830044661\n",
            "0.952420969143548\n",
            "0.9540494034052505\n",
            "0.9560148642800957\n",
            "0.9578576075756337\n",
            "0.9592901437788668\n",
            "0.9611256608877645\n",
            "0.963407048026619\n",
            "0.9651488763901889\n",
            "0.9668828907525143\n",
            "0.9683748831224563\n",
            "0.9701276685270812\n",
            "0.971698150915258\n",
            "0.973593169466004\n",
            "0.9756378499443269\n",
            "0.9773643513774628\n",
            "0.9789181766302689\n",
            "0.9804180951984337\n",
            "0.9822850344735948\n",
            "0.9840373814563312\n",
            "0.9854343804861884\n",
            "0.987049974901292\n",
            "0.9885993435254792\n",
            "0.9904143229469924\n",
            "0.9923081712039841\n",
            "0.9941908252208739\n",
            "0.9957777551372947\n",
            "0.9975626983910876\n",
            "0.9993191919363368\n",
            "1.00083853094779\n",
            "1.00262810903437\n",
            "1.004259460417511\n",
            "1.005738605928543\n",
            "1.0075186687662168\n",
            "1.008982120420012\n",
            "1.0105077942923817\n",
            "1.0124012149508348\n",
            "1.014151455953603\n",
            "1.0161086862044566\n",
            "1.0177614739179002\n",
            "1.0196384101572549\n",
            "1.0215423380017585\n",
            "1.0230613864596239\n",
            "1.024792195738429\n",
            "1.0262664443696552\n",
            "1.0278249744259182\n",
            "1.0295224583057492\n",
            "1.0312285584866847\n",
            "1.0328004290075863\n",
            "1.0344050467166754\n",
            "1.0359646234365985\n",
            "1.0377179747042449\n",
            "1.0395620607049263\n",
            "1.0414747379320053\n",
            "1.043126637368556\n",
            "1.0447765430221168\n",
            "1.0466871895753513\n",
            "1.0485874682741092\n",
            "1.0502339241754672\n",
            "1.0518572554563927\n",
            "1.05375796083904\n",
            "1.055559229667839\n",
            "1.057326121403433\n",
            "1.0590694801276908\n",
            "1.0608309641518556\n",
            "1.062918890772573\n",
            "1.064566251871836\n",
            "1.0663493335094598\n",
            "1.0682670640213716\n",
            "1.0702597616273728\n",
            "1.0719815392018583\n",
            "1.0736742871801566\n",
            "1.075263413779266\n",
            "1.076877756954154\n",
            "1.0788314238838528\n",
            "1.0804133872546808\n",
            "1.0821663016248542\n",
            "1.0838064325739964\n",
            "1.0855630866401946\n",
            "1.0873226038635235\n",
            "1.0889973030675708\n",
            "1.0907140313207035\n",
            "1.0922915124527328\n",
            "1.0939289160701624\n",
            "1.0956548690186132\n",
            "1.0971480519570354\n",
            "1.0992468947644733\n",
            "1.1012346343616086\n",
            "1.1032479957241537\n",
            "1.1044463159330666\n",
            "1.1058679479924614\n",
            "1.1075249522390878\n",
            "1.1091203719301297\n",
            "1.1110061299617944\n",
            "1.1128178770127504\n",
            "1.1144905822813664\n",
            "1.1161486078863558\n",
            "1.1178817417463074\n",
            "1.1197098219943473\n",
            "1.1209447128541024\n",
            "1.1226565311174563\n",
            "1.124559314125944\n",
            "1.1260880878971666\n",
            "1.1276500530712439\n",
            "1.12927764402631\n",
            "1.1313574090790566\n",
            "1.132744889353852\n",
            "1.134105832299308\n",
            "1.135836979998347\n",
            "1.137713039698808\n",
            "1.1397112590425156\n",
            "1.1418226540393537\n",
            "1.1434710163746953\n",
            "1.1454345237111192\n",
            "1.1472668401573016\n",
            "1.1488461065322846\n",
            "1.1511019665719298\n",
            "1.152676663359108\n",
            "1.1545855614078013\n",
            "1.1565148896725892\n",
            "1.1584152891050519\n",
            "1.160186587011113\n",
            "1.1622924249038062\n",
            "1.163722732106743\n",
            "1.165506639367784\n",
            "1.166864931049859\n",
            "1.1688660467829546\n",
            "1.170772565371545\n",
            "1.1725422367263023\n",
            "1.1741476036855936\n",
            "1.1756317594929424\n",
            "1.1772920330009802\n",
            "1.1791117262200017\n",
            "1.1811224741246693\n",
            "1.1831290884243557\n",
            "1.184494614982239\n",
            "1.1863096233676462\n",
            "1.187937072094749\n",
            "1.1896842403332595\n",
            "1.1914018718025567\n",
            "1.1930817050854567\n",
            "1.1949732400419768\n",
            "1.1965208235756515\n",
            "1.198121683448172\n",
            "1.2000678397353044\n",
            "1.2016316325310856\n",
            "1.2035040125974914\n",
            "1.204981242528047\n",
            "1.2065306382868297\n",
            "1.2082162603850255\n",
            "1.2099013582367422\n",
            "1.2119749408701193\n",
            "1.2134667016813516\n",
            "1.2152634978751697\n",
            "1.2171987564210087\n",
            "1.2188767463807255\n",
            "1.2207825364511642\n",
            "1.2222160054442217\n",
            "1.2239420442172633\n",
            "1.2256008401093885\n",
            "1.2271439824110406\n",
            "1.2289486304116066\n",
            "1.2304694561854652\n",
            "1.2320432871046578\n",
            "1.2340039895166217\n",
            "1.235808535838676\n",
            "1.2377827538706152\n",
            "1.2394360861052638\n",
            "1.2410993035644522\n",
            "1.242734006070115\n",
            "1.2443089528614297\n",
            "1.245982306463944\n",
            "1.2479810049314328\n",
            "1.249471580966964\n",
            "1.2509766134916973\n",
            "1.2528234435926617\n",
            "1.2544657692427525\n",
            "1.256408980664085\n",
            "1.2583280547957896\n",
            "1.2602229446858702\n",
            "1.2620513642687932\n",
            "1.2637134759170014\n",
            "1.265581437770058\n",
            "1.2670722394190785\n",
            "1.2686944433947658\n",
            "1.2706316714853887\n",
            "1.2724418697302298\n",
            "1.274138310300115\n",
            "1.2760444656204994\n",
            "1.277671674252166\n",
            "1.279445052375574\n",
            "1.2813306290017978\n",
            "1.2829251793186989\n",
            "1.284721793802193\n",
            "1.2864085909198313\n",
            "1.288041363972837\n",
            "1.2899449523300162\n",
            "1.2915766863414393\n",
            "1.293477719473412\n",
            "1.2952776312675622\n",
            "1.2969729745631937\n",
            "1.2986871559754052\n",
            "1.3005228483920817\n",
            "1.3019922117290594\n",
            "1.3035856152282042\n",
            "1.3051519219375327\n",
            "1.306962003107266\n",
            "1.3088252357662182\n",
            "1.3103002261780108\n",
            "1.3119896323327214\n",
            "1.3137822290668097\n",
            "1.3155738611507903\n",
            "1.3172932755764184\n",
            "1.3193933892890315\n",
            "1.3209949517646409\n",
            "1.3226660178293048\n",
            "1.324602893017747\n",
            "1.326284922068686\n",
            "1.3278924267157874\n",
            "1.329309349398479\n",
            "1.3310561536065757\n",
            "1.3329075189959971\n",
            "1.3347384195650935\n",
            "1.336768455593787\n",
            "1.3384718514616838\n",
            "1.3401922761174418\n",
            "1.3419570943431172\n",
            "1.3438355952425076\n",
            "1.3452563297260753\n",
            "1.3476841880384918\n",
            "0.0017464944468739698\n",
            "0.0033718118887118365\n",
            "0.005043063017413439\n",
            "0.006733748461584301\n",
            "0.008757545667536119\n",
            "0.010147388481423068\n",
            "0.011439510775953912\n",
            "0.012990087042074374\n",
            "0.015018513135592956\n",
            "0.01664251897036267\n",
            "0.01861690910880828\n",
            "0.020492153094552667\n",
            "0.0220644053290872\n",
            "0.023706042095828238\n",
            "0.025412496246035445\n",
            "0.026943344289384535\n",
            "0.028509471117687958\n",
            "0.030237548643975612\n",
            "0.03195218204537316\n",
            "0.03352879700453385\n",
            "0.03489983310479947\n",
            "0.036535999659077285\n",
            "0.03826328997721758\n",
            "0.03998300699931581\n",
            "0.04193693262231929\n",
            "0.04369417633242009\n",
            "0.04532239077341221\n",
            "0.04688105513067806\n",
            "0.04846479200646091\n",
            "0.050349432946471\n",
            "0.052089711741718184\n",
            "0.05387288072834844\n",
            "0.0554384587670836\n",
            "0.05709338310124624\n",
            "0.05897031400514686\n",
            "0.06075407187347217\n",
            "0.06228590850025187\n",
            "0.06421972860765579\n",
            "0.06621172071417884\n",
            "0.06797450247323117\n",
            "0.06969490075660179\n",
            "0.07136188352199467\n",
            "0.07321273518340363\n",
            "0.07496451660800163\n",
            "0.07685006244103317\n",
            "0.07852067758360178\n",
            "0.08034576418454689\n",
            "0.08240398787476523\n",
            "0.08381023522837998\n",
            "0.0856580816571365\n",
            "0.08756883187062295\n",
            "0.08908686400069606\n",
            "0.09085939973211654\n",
            "0.09285984761879572\n",
            "0.09424050933564715\n",
            "0.09602516386515039\n",
            "0.0977443445978872\n",
            "0.09968967221277145\n",
            "0.10166670401078051\n",
            "0.10355900544339738\n",
            "0.10546977486451874\n",
            "0.10698083004988063\n",
            "0.10847830238854489\n",
            "0.1103197057228869\n",
            "0.1119711917379628\n",
            "0.11349601148034605\n",
            "0.1148798900187168\n",
            "0.11679145487982903\n",
            "0.11842949737978103\n",
            "0.12010037487425158\n",
            "0.12187219519749322\n",
            "0.12351383150690962\n",
            "0.12531700280621227\n",
            "0.12678888508730837\n",
            "0.1280696215227132\n",
            "0.1297369253300035\n",
            "0.13118563512402118\n",
            "0.13291418674351918\n",
            "0.13463720229580578\n",
            "0.13616651144174055\n",
            "0.13776600894415775\n",
            "0.13948191126899037\n",
            "0.14140496656412968\n",
            "0.14302833763229877\n",
            "0.14475882785094668\n",
            "0.1464131637607389\n",
            "0.1483544766750482\n",
            "0.150104864783909\n",
            "0.1520379516474731\n",
            "0.1538489127098142\n",
            "0.1554877581193929\n",
            "0.1573728088222806\n",
            "0.15914539485941154\n",
            "0.16105454077806008\n",
            "0.1626579481012681\n",
            "0.1643497096303174\n",
            "0.16609311073332492\n",
            "0.16787943480264805\n",
            "0.16946281435544533\n",
            "0.17111694645088957\n",
            "0.1725471649328461\n",
            "0.17438901065255674\n",
            "0.17630327403392937\n",
            "0.17809694121255898\n",
            "0.17975716029896455\n",
            "0.18122661814970129\n",
            "0.1827278478676096\n",
            "0.1844545201877194\n",
            "0.186033282590949\n",
            "0.18761546791666914\n",
            "0.1895225281300752\n",
            "0.1912091113722233\n",
            "0.19282364799543414\n",
            "0.19446953620447222\n",
            "0.19613770725172194\n",
            "0.19786104735206156\n",
            "0.19960766283752363\n",
            "0.20113661831906995\n",
            "0.20282822496750774\n",
            "0.20493934206340625\n",
            "0.20657771063582672\n",
            "0.208196478731492\n",
            "0.21018178414200883\n",
            "0.2120547962310674\n",
            "0.21360910121742113\n",
            "0.21556935987204237\n",
            "0.21772476535319063\n",
            "0.2194076279545074\n",
            "0.221371450844933\n",
            "0.22306788562203916\n",
            "0.22469819339035113\n",
            "0.22624408939610358\n",
            "0.2279068184325762\n",
            "0.2293181899563431\n",
            "0.23100898561575223\n",
            "0.23284568399419564\n",
            "0.23476117757885048\n",
            "0.23631086846446747\n",
            "0.23800642334896585\n",
            "0.2396176166241736\n",
            "0.2413009906668797\n",
            "0.2428809479069527\n",
            "0.24439383757388805\n",
            "0.2462299825895168\n",
            "0.2479932029228991\n",
            "0.24965147030018175\n",
            "0.2514153258574893\n",
            "0.2532184273385636\n",
            "0.2549420020464436\n",
            "0.25700265169143677\n",
            "0.25882136776014364\n",
            "0.26115053221392814\n",
            "0.26291600014547556\n",
            "0.26458417256469924\n",
            "0.266285261839552\n",
            "0.26772288349278445\n",
            "0.2695872071758865\n",
            "0.2712287084220925\n",
            "0.27309911284605254\n",
            "0.274888309676324\n",
            "0.2769408433333687\n",
            "0.27865927695008497\n",
            "0.2805891030889643\n",
            "0.2821580397198572\n",
            "0.28391969203948975\n",
            "0.2858351182449809\n",
            "0.28733309074436003\n",
            "0.2888852168836862\n",
            "0.2905951529512625\n",
            "0.29248454518940137\n",
            "0.2940315973118443\n",
            "0.29575602371064597\n",
            "0.29743402769498506\n",
            "0.2993972342642372\n",
            "0.3012290121344349\n",
            "0.30270996377291276\n",
            "0.3043663731926238\n",
            "0.3059742052841674\n",
            "0.30767138854926807\n",
            "0.3091799080219415\n",
            "0.3109203322464243\n",
            "0.313022335929334\n",
            "0.31464968816093775\n",
            "0.31636970793194785\n",
            "0.318107622053922\n",
            "0.31967081438245065\n",
            "0.32164465100564005\n",
            "0.3236888552565709\n",
            "0.32547510280023756\n",
            "0.3274515236125273\n",
            "0.32914206332257945\n",
            "0.33084821868735503\n",
            "0.3324129372606497\n",
            "0.3342584894441278\n",
            "0.33610844749318974\n",
            "0.33808976045959743\n",
            "0.3397783307773073\n",
            "0.34143110156973916\n",
            "0.34276654790429506\n",
            "0.34467274392657266\n",
            "0.3462761357007429\n",
            "0.34799655624057935\n",
            "0.34977423687420234\n",
            "0.35163572697383366\n",
            "0.35347802590226274\n",
            "0.3554813076772958\n",
            "0.35699547373730206\n",
            "0.3590365504974599\n",
            "0.36058764232089147\n",
            "0.3624338375028137\n",
            "0.3641057685208138\n",
            "0.3658482680845139\n",
            "0.3672772877661468\n",
            "0.36894761784302305\n",
            "0.3706984242514881\n",
            "0.3728588596939126\n",
            "0.37441101754108047\n",
            "0.37593469610604485\n",
            "0.37797405805124346\n",
            "0.3801335221361321\n",
            "0.382139847711529\n",
            "0.3838446164679954\n",
            "0.3857784384046979\n",
            "0.38785320337471146\n",
            "0.38948448219567616\n",
            "0.3910553552915373\n",
            "0.3925699201386298\n",
            "0.3941313830178107\n",
            "0.39611730002381307\n",
            "0.39755857554847934\n",
            "0.3993137265410265\n",
            "0.40098842955611247\n",
            "0.4027120928325312\n",
            "0.40407617576896687\n",
            "0.4057500618498039\n",
            "0.40769702302830296\n",
            "0.40975002224183144\n",
            "0.41168323044886673\n",
            "0.4134021812997511\n",
            "0.4151165880205686\n",
            "0.41691268923337504\n",
            "0.41888894510391117\n",
            "0.42079591735854477\n",
            "0.42294678297798954\n",
            "0.4246630888155964\n",
            "0.4264106238284684\n",
            "0.42802886081778485\n",
            "0.4296289840927514\n",
            "0.4315296478588563\n",
            "0.43322388397153383\n",
            "0.43469372048707267\n",
            "0.4362320596604701\n",
            "0.43815259784078964\n",
            "0.43986300586739463\n",
            "0.44141312801014737\n",
            "0.4431640481400063\n",
            "0.4444452882422816\n",
            "0.44626293142738244\n",
            "0.4480379156749267\n",
            "0.45000677813044593\n",
            "0.4519346767984083\n",
            "0.4539751425728469\n",
            "0.4557977315714902\n",
            "0.4574986162697873\n",
            "0.4593286343547694\n",
            "0.4607130688474611\n",
            "0.4624442791999758\n",
            "0.46424010769485513\n",
            "0.46598180541601936\n",
            "0.46758702633630894\n",
            "0.46921049069870463\n",
            "0.47073745803759837\n",
            "0.47268965680276037\n",
            "0.474574964826979\n",
            "0.4764496703891803\n",
            "0.4779973273996807\n",
            "0.47962307929992676\n",
            "0.4809884378672256\n",
            "0.4824461095473346\n",
            "0.48407065959842616\n",
            "0.4855916934549961\n",
            "0.4872023521176994\n",
            "0.48909020957434574\n",
            "0.4910412308810007\n",
            "0.492531593345925\n",
            "0.4940350211184958\n",
            "0.49578013078635913\n",
            "0.4974500142095034\n",
            "0.49886675015130005\n",
            "0.5007615961382151\n",
            "0.5022730866966345\n",
            "0.5041093204332434\n",
            "0.5056163604607058\n",
            "0.507405202559498\n",
            "0.5091551613929631\n",
            "0.5108248640204329\n",
            "0.5125295640257619\n",
            "0.5144369096097434\n",
            "0.5159018683006696\n",
            "0.5175082659172585\n",
            "0.5189530032370097\n",
            "0.520815724607014\n",
            "0.5222929463057262\n",
            "0.5241523574075431\n",
            "0.525911163948381\n",
            "0.5277496995523457\n",
            "0.5292058248654046\n",
            "0.530908326816071\n",
            "0.5324878709395523\n",
            "0.5343138773727905\n",
            "0.5360826124315676\n",
            "0.537697918732148\n",
            "0.5397080925419507\n",
            "0.5414125519945189\n",
            "0.5431546372220949\n",
            "0.5448235649891826\n",
            "0.5466541321686161\n",
            "0.5481277422222031\n",
            "0.5498881727228384\n",
            "0.5516907871531709\n",
            "0.5532874050347701\n",
            "0.5551689009532295\n",
            "0.5569989603498707\n",
            "0.558670392884013\n",
            "0.5602944444512468\n",
            "0.5620086746447531\n",
            "0.5635975084036512\n",
            "0.5655619701765993\n",
            "0.5671034318101985\n",
            "0.5689277851673038\n",
            "0.5704814387709284\n",
            "0.5720413068066472\n",
            "0.5737411142005335\n",
            "0.575065377270779\n",
            "0.5766885574821317\n",
            "0.5786199176402957\n",
            "0.5802395968790859\n",
            "0.5819108353551391\n",
            "0.5835949565138658\n",
            "0.5850729501765707\n",
            "0.586792437469258\n",
            "0.5884334475488004\n",
            "0.5903006502429543\n",
            "0.5921769988201463\n",
            "0.5940306304056017\n",
            "0.5957240846455859\n",
            "0.5973872801531916\n",
            "0.5991654330507263\n",
            "0.6006989235158466\n",
            "0.602390489614833\n",
            "0.604195982446451\n",
            "0.6059164208219484\n",
            "0.6076517675233923\n",
            "0.6095524727535979\n",
            "0.6113235006856796\n",
            "0.6126307102725329\n",
            "0.6145494243373042\n",
            "0.6161518596932102\n",
            "0.6178690515210866\n",
            "0.6192616773078509\n",
            "0.6212224635626654\n",
            "0.6228252397778699\n",
            "0.6244048128652451\n",
            "0.626179139022632\n",
            "0.6277237548242749\n",
            "0.6295551209498549\n",
            "0.6312564780645054\n",
            "0.6328951981671326\n",
            "0.6345484892425635\n",
            "0.6362525183526452\n",
            "0.637704017522085\n",
            "0.6398503984636663\n",
            "0.6418308272691029\n",
            "0.6436703526760306\n",
            "0.6451169253920045\n",
            "0.6466527658960094\n",
            "0.6485952620616044\n",
            "0.6500021444867029\n",
            "0.6519526734071619\n",
            "0.6538256518066387\n",
            "0.6556917419823844\n",
            "0.657264645294765\n",
            "0.658753356512855\n",
            "0.6604308467691816\n",
            "0.6621529106288919\n",
            "0.663914884295305\n",
            "0.6655553766833547\n",
            "0.6671025626494756\n",
            "0.668715709767988\n",
            "0.6704538879187211\n",
            "0.6720313749959707\n",
            "0.67369429885274\n",
            "0.6754963212001049\n",
            "0.6771203782552343\n",
            "0.6788888535536158\n",
            "0.6807289830863933\n",
            "0.6825098680413287\n",
            "0.6840901219326517\n",
            "0.6855158546696538\n",
            "0.6873384352840121\n",
            "0.6891008534699755\n",
            "0.6908302630304985\n",
            "0.6923501903138807\n",
            "0.6941603765158397\n",
            "0.695842626149697\n",
            "0.6973400911711671\n",
            "0.6987786947004021\n",
            "0.7006582142142079\n",
            "0.7024596068255432\n",
            "0.7039351745334732\n",
            "0.7054425889573743\n",
            "0.7070758165910725\n",
            "0.7086814669391993\n",
            "0.7102285913189353\n",
            "0.7120418585169955\n",
            "0.7139013179427828\n",
            "0.7157332008452062\n",
            "0.7175839238459497\n",
            "0.7192030506365744\n",
            "0.7210508275519857\n",
            "0.7228459133516492\n",
            "0.7246037805476762\n",
            "0.726255262904155\n",
            "0.7282289501346285\n",
            "0.7298794728715706\n",
            "0.7316646588123058\n",
            "0.7334806772754016\n",
            "0.7352449262843412\n",
            "0.7371389394830865\n",
            "0.7385176119901945\n",
            "0.7406221440685984\n",
            "0.7424898754300364\n",
            "0.7446130460790356\n",
            "0.7463621893502257\n",
            "0.7480219852588975\n",
            "0.7496668987566858\n",
            "0.7511746839184286\n",
            "0.7525610402417\n",
            "0.7541448159900772\n",
            "0.755685615265156\n",
            "0.7573871014977965\n",
            "0.7590274146145872\n",
            "0.7606329448387751\n",
            "0.7622254282007437\n",
            "0.7639249933650122\n",
            "0.7657454780605443\n",
            "0.7677374610205745\n",
            "0.7694231155888199\n",
            "0.7710867309204453\n",
            "0.7724315521052426\n",
            "0.7740703106231397\n",
            "0.7755698290322443\n",
            "0.7770840784777766\n",
            "0.7789064209784389\n",
            "0.7807772696170661\n",
            "0.7825580643266058\n",
            "0.7841965115588644\n",
            "0.785937549970339\n",
            "0.7876646364741313\n",
            "0.7893572919204107\n",
            "0.790940276649602\n",
            "0.7929527844919269\n",
            "0.794782034423955\n",
            "0.7966196059875781\n",
            "0.7981990318164192\n",
            "0.799938454804823\n",
            "0.8016167514769318\n",
            "0.803651391240337\n",
            "0.8052969992618122\n",
            "0.8069481619483675\n",
            "0.808530048488656\n",
            "0.8104068524087481\n",
            "0.8124269536693992\n",
            "0.8140805807259991\n",
            "0.8157227234462338\n",
            "0.817319995149627\n",
            "0.818954095968505\n",
            "0.8204195272282261\n",
            "0.8222080937127019\n",
            "0.8240807246978935\n",
            "0.825860091487465\n",
            "0.8276762669653539\n",
            "0.8296680456537116\n",
            "0.8313722403153129\n",
            "0.8328699157061175\n",
            "0.8344875148800023\n",
            "0.8361684512299346\n",
            "0.837947662986453\n",
            "0.8399138840872918\n",
            "0.8418254986443483\n",
            "0.8434016296015981\n",
            "0.8448845068816944\n",
            "0.8465004845348465\n",
            "0.8481431967766998\n",
            "0.8497539223612421\n",
            "0.8515202418312697\n",
            "0.8531622159511537\n",
            "0.8548880315497708\n",
            "0.8566470324535809\n",
            "0.8585463949786428\n",
            "0.8604709885614302\n",
            "0.8621848842981831\n",
            "0.8638665430686053\n",
            "0.8653070309277996\n",
            "0.867258904992467\n",
            "0.8688187955895348\n",
            "0.870445912603832\n",
            "0.8723548931234023\n",
            "0.8740467423063409\n",
            "0.8758157139543987\n",
            "0.8775115124404888\n",
            "0.8791521467516185\n",
            "0.8809196412410882\n",
            "0.882441294010338\n",
            "0.8844600652184937\n",
            "0.8861871367830145\n",
            "0.8881113128284054\n",
            "0.889844983435043\n",
            "0.8916516688168811\n",
            "0.8934574735439037\n",
            "0.895075258239151\n",
            "0.8971069747834559\n",
            "0.8988486889683073\n",
            "0.9004085216375873\n",
            "0.9020514965362256\n",
            "0.9037668996149927\n",
            "0.9054913013182637\n",
            "0.9075018505915962\n",
            "0.9093759172712751\n",
            "0.9111258575068716\n",
            "0.9129385661591044\n",
            "0.9147640497178373\n",
            "0.9166706875157173\n",
            "0.9184022250077913\n",
            "0.9201138056147738\n",
            "0.9216380689455115\n",
            "0.9233683119039706\n",
            "0.9252189711841476\n",
            "0.9270463041637255\n",
            "0.9288654199341679\n",
            "0.9308481606681024\n",
            "0.9324665039091768\n",
            "0.934141709676484\n",
            "0.9359816651210151\n",
            "0.9377937859586437\n",
            "0.9392527578127049\n",
            "0.9407434497038116\n",
            "0.9423690397111352\n",
            "0.9440243786863048\n",
            "0.9456526900801208\n",
            "0.9472475108283255\n",
            "0.9487717122677952\n",
            "0.9506276880993563\n",
            "0.952519193482216\n",
            "0.9541226018725149\n",
            "0.9556896009713488\n",
            "0.9574634167544372\n",
            "0.9588081736089019\n",
            "0.9605926375864716\n",
            "0.9624836600345114\n",
            "0.9640622084098094\n",
            "0.9661241583811963\n",
            "0.9680021086617199\n",
            "0.9697337871622247\n",
            "0.9714390537927827\n",
            "0.9733756829405684\n",
            "0.9753207078065409\n",
            "0.9769185278421778\n",
            "0.9785450134435882\n",
            "0.9804355576824959\n",
            "0.9818881186073088\n",
            "0.9839310810694\n",
            "0.9852996749036452\n",
            "0.987157890406411\n",
            "0.9888281223109311\n",
            "0.9907863992254448\n",
            "0.9922777418895146\n",
            "0.9940586155637756\n",
            "0.9960372833644643\n",
            "0.9973772841951122\n",
            "0.9993291259421717\n",
            "1.0012796118741145\n",
            "1.0032652724734352\n",
            "1.0048388469859462\n",
            "1.006502982143246\n",
            "1.0083493050711845\n",
            "1.0100884727199975\n",
            "1.0118258540587657\n",
            "1.0135597868648636\n",
            "1.0152902124482956\n",
            "1.0169693012066814\n",
            "1.0184775063448854\n",
            "1.020187417107165\n",
            "1.0217400927982672\n",
            "1.0234181769668598\n",
            "1.0252035601669565\n",
            "1.0269994782974652\n",
            "1.0285842279948847\n",
            "1.0300553367875727\n",
            "1.0320533061271433\n",
            "1.0339578428231846\n",
            "1.0358145089100694\n",
            "1.0373502193814348\n",
            "1.0393711124234797\n",
            "1.0412735804877318\n",
            "1.0431029432265044\n",
            "1.0446748613091685\n",
            "1.0462950914717086\n",
            "1.047804185039247\n",
            "1.049559903419231\n",
            "1.051549340910314\n",
            "1.0533595702532308\n",
            "1.0552051907305218\n",
            "1.056772765143753\n",
            "1.0587133986260884\n",
            "1.0604713703970166\n",
            "1.0623604699473856\n",
            "1.0638287568946019\n",
            "1.065687984304355\n",
            "1.067502930493611\n",
            "1.0688737505842048\n",
            "1.0705803881215927\n",
            "1.0721562186165539\n",
            "1.0736648457129594\n",
            "1.0755967702097295\n",
            "1.0771479836815154\n",
            "1.0787129967718783\n",
            "1.0804073995031664\n",
            "1.0821837889568886\n",
            "1.0841383394377921\n",
            "1.0857068661533658\n",
            "1.0874686078037448\n",
            "1.0892341007357058\n",
            "1.091307801053957\n",
            "1.0928066319516858\n",
            "1.0944523471395682\n",
            "1.096140806174949\n",
            "1.0976799453615838\n",
            "1.099236743986759\n",
            "1.1008193418193046\n",
            "1.1026225658633826\n",
            "1.10432906833756\n",
            "1.1060640525330059\n",
            "1.1077497302723662\n",
            "1.1095763885456582\n",
            "1.111153530190363\n",
            "1.1128377431189007\n",
            "1.1143418001701764\n",
            "1.1158205217412671\n",
            "1.1175540979865872\n",
            "1.1193563079894961\n",
            "1.1210827842697768\n",
            "1.122503210211654\n",
            "1.124140119613589\n",
            "1.1256925341723216\n",
            "1.1275614583888627\n",
            "1.1294520500370913\n",
            "1.130963731147444\n",
            "1.1327800241577657\n",
            "1.1343922683649965\n",
            "1.1358794358075428\n",
            "1.1375796038781285\n",
            "1.1397023801608464\n",
            "1.1416081388283263\n",
            "1.1433938063319078\n",
            "1.1451673044268127\n",
            "1.1470791613659286\n",
            "1.1484906548429328\n",
            "1.1502795127956458\n",
            "1.1517859329957791\n",
            "1.1535928516131837\n",
            "1.1555947807743727\n",
            "1.1573118160447806\n",
            "1.158931101069731\n",
            "1.160644208073921\n",
            "1.1621937318836026\n",
            "1.1641868544966363\n",
            "1.1656241980964874\n",
            "1.167419791831385\n",
            "1.1693668301453066\n",
            "1.171340079563658\n",
            "1.1730359807953505\n",
            "1.1745553248373748\n",
            "1.1765965092212647\n",
            "1.1782552810276257\n",
            "1.180287187818981\n",
            "1.1818483383454326\n",
            "1.1835564315471503\n",
            "1.1852277697199751\n",
            "1.1866387159318266\n",
            "1.1882813400624659\n",
            "1.1899960769716735\n",
            "1.19184885122587\n",
            "1.1939228020055825\n",
            "1.1952242069232188\n",
            "1.1969625387350311\n",
            "1.1988501134126082\n",
            "1.2004628138773887\n",
            "1.2022034344465837\n",
            "1.2039060687165126\n",
            "1.205734704766432\n",
            "1.2071901084212087\n",
            "1.2089882861927648\n",
            "1.2109306738199785\n",
            "1.2126506007540867\n",
            "1.2139821108954643\n",
            "1.2156588118094618\n",
            "1.2175670096941311\n",
            "1.2191999967750686\n",
            "1.220846419596611\n",
            "1.2224532778915542\n",
            "1.2240659975639694\n",
            "1.2255776035206398\n",
            "1.227422497766402\n",
            "1.2290807090451956\n",
            "1.2307242866977097\n",
            "1.2324514965267133\n",
            "1.2341538512188455\n",
            "1.2360518341479094\n",
            "1.2380158963715633\n",
            "1.2396984161318416\n",
            "1.241527573836734\n",
            "1.243223194423539\n",
            "1.2448438505077606\n",
            "1.2465187145011198\n",
            "1.2481794807002367\n",
            "1.2497105438386082\n",
            "1.2515415512692287\n",
            "1.2533276995734486\n",
            "1.254978243347324\n",
            "1.2568045418585658\n",
            "1.2583343705252918\n",
            "1.2602328775483933\n",
            "1.2618225076619316\n",
            "1.2636720910096717\n",
            "1.265324284505966\n",
            "1.267069629696019\n",
            "1.2684605521009402\n",
            "1.2703834366615472\n",
            "1.2718916748795668\n",
            "1.2735488097685987\n",
            "1.2752986647893705\n",
            "1.2773985413029372\n",
            "1.2792775504424443\n",
            "1.281239422995721\n",
            "1.2829180400999611\n",
            "1.28450868638885\n",
            "1.2863250803154753\n",
            "1.2879679413402783\n",
            "1.2899417799452078\n",
            "1.2918009994280002\n",
            "1.2935488362751348\n",
            "1.2951182363283298\n",
            "1.2969461790740948\n",
            "1.2985816617755939\n",
            "1.3003095836590624\n",
            "1.3020147127873452\n",
            "1.3033735355757692\n",
            "1.3052550596959145\n",
            "1.3071366678113523\n",
            "1.3091152635071894\n",
            "1.3109801794257006\n",
            "1.3125656311164426\n",
            "1.3145633897817959\n",
            "1.3161862715125998\n",
            "1.3175944231660164\n",
            "1.3192812623575216\n",
            "1.3210802689537673\n",
            "1.3227468721397089\n",
            "1.324161932291582\n",
            "1.326312037075267\n",
            "1.3278726927764581\n",
            "1.3295356928540008\n",
            "1.3313432898362885\n",
            "1.3331655562686189\n",
            "1.3348560000929381\n",
            "1.3364757954921869\n",
            "1.3384151519716854\n",
            "1.3401613872679299\n",
            "1.3420106288417222\n",
            "1.3440971023896162\n",
            "1.3458422050451684\n",
            "0.001634429787735805\n",
            "0.003327234931614088\n",
            "0.0051185154853879335\n",
            "0.006719171086235729\n",
            "0.008628857562609036\n",
            "0.0104331794907065\n",
            "0.012237140437221284\n",
            "0.014367186047536942\n",
            "0.016250553948190208\n",
            "0.017906893092348143\n",
            "0.019579915591822865\n",
            "0.021319946364673506\n",
            "0.023182496695262393\n",
            "0.024803338453287967\n",
            "0.026181584276506662\n",
            "0.027784774522952107\n",
            "0.02924369881525064\n",
            "0.030858841088726698\n",
            "0.03254055702472892\n",
            "0.03388272145824969\n",
            "0.03537487023321869\n",
            "0.03710712313347155\n",
            "0.03889906558844135\n",
            "0.04066727457144071\n",
            "0.042225654625221896\n",
            "0.0441683093300256\n",
            "0.04596610477818248\n",
            "0.04779340696456792\n",
            "0.049242757622848084\n",
            "0.050921541955464944\n",
            "0.052595348156931454\n",
            "0.053990487399918345\n",
            "0.05550925155429889\n",
            "0.05728969137991786\n",
            "0.05922989939789638\n",
            "0.06070703999770572\n",
            "0.062251721044330646\n",
            "0.06390922529923032\n",
            "0.06578922165019432\n",
            "0.06756841922964892\n",
            "0.0691642325247645\n",
            "0.0709555034747209\n",
            "0.07263698922398755\n",
            "0.07438680308554178\n",
            "0.07611256639670838\n",
            "0.07758572918679708\n",
            "0.07919857462348841\n",
            "0.08071165301305863\n",
            "0.08253434902566778\n",
            "0.0841851309132393\n",
            "0.08606277844485115\n",
            "0.08784014367691391\n",
            "0.08956439415817066\n",
            "0.0911894792790913\n",
            "0.0927316756809459\n",
            "0.09438005447997462\n",
            "0.09602875157695293\n",
            "0.09794764171171066\n",
            "0.09966660094688005\n",
            "0.10158227486988468\n",
            "0.10309940180205324\n",
            "0.10463557691525316\n",
            "0.10660056994699152\n",
            "0.10813783715143228\n",
            "0.10981014424272816\n",
            "0.11137443994317213\n",
            "0.11306761491024281\n",
            "0.11477853071964\n",
            "0.1162088025561379\n",
            "0.11760161919971866\n",
            "0.11916498561649372\n",
            "0.12100460934821906\n",
            "0.12256395588140659\n",
            "0.12432879743063846\n",
            "0.12597459890043644\n",
            "0.1275893687592138\n",
            "0.12931770833252032\n",
            "0.1310354119066692\n",
            "0.1329403643108085\n",
            "0.13460551015556316\n",
            "0.13611866171707582\n",
            "0.1380235703704912\n",
            "0.14000817226326984\n",
            "0.14161221069448135\n",
            "0.14381795999644054\n",
            "0.1455301538757656\n",
            "0.14727495195310744\n",
            "0.14914683612716167\n",
            "0.15071128061055528\n",
            "0.15210256277752654\n",
            "0.15351241522128015\n",
            "0.15513657441224588\n",
            "0.15662600728861817\n",
            "0.15852762000335147\n",
            "0.16047507219607263\n",
            "0.16211457478115932\n",
            "0.1637412138912074\n",
            "0.16555808785626347\n",
            "0.16727310129443704\n",
            "0.16884578753005514\n",
            "0.17042385358029924\n",
            "0.1722746710948017\n",
            "0.17400722293292775\n",
            "0.17587025924716765\n",
            "0.17767552478843943\n",
            "0.17964936080186264\n",
            "0.18135830356032037\n",
            "0.18293223356651833\n",
            "0.18465916518969913\n",
            "0.18636454493188492\n",
            "0.188262194166403\n",
            "0.18999385696542842\n",
            "0.19173544691041913\n",
            "0.19351754514762506\n",
            "0.19522700818908184\n",
            "0.1965204448346287\n",
            "0.19832635291701997\n",
            "0.19992420023969373\n",
            "0.20138133989880458\n",
            "0.20306224454089503\n",
            "0.2044007633348255\n",
            "0.2060514603124555\n",
            "0.207705916193745\n",
            "0.20939846813221416\n",
            "0.21092616962959698\n",
            "0.2122474780777836\n",
            "0.2140959457058431\n",
            "0.2156677461036331\n",
            "0.21742319130836546\n",
            "0.21913773522657506\n",
            "0.2209997818902935\n",
            "0.22272918215188223\n",
            "0.2244716420807802\n",
            "0.22633618466994343\n",
            "0.2279683577130213\n",
            "0.22969755202608036\n",
            "0.23156829319341712\n",
            "0.23307487269496674\n",
            "0.2349967794954929\n",
            "0.23668182368778512\n",
            "0.23880211502084953\n",
            "0.24056660153371903\n",
            "0.24222386752248115\n",
            "0.24399595309401412\n",
            "0.24573999536616722\n",
            "0.24757851100028933\n",
            "0.24952287640413054\n",
            "0.2513227865214238\n",
            "0.2529008440349413\n",
            "0.25446471594788533\n",
            "0.25585871645251806\n",
            "0.25779872294277184\n",
            "0.25949137701707725\n",
            "0.26129778312600177\n",
            "0.2629971237438719\n",
            "0.2645558926760388\n",
            "0.266403728129004\n",
            "0.2678580770407186\n",
            "0.2695491920651682\n",
            "0.2709135742443602\n",
            "0.2725715737818452\n",
            "0.27424314564756114\n",
            "0.27613017504172555\n",
            "0.2779448343359906\n",
            "0.2794924594862077\n",
            "0.2808963427763156\n",
            "0.28273253443905766\n",
            "0.2841912814418373\n",
            "0.2858145836064273\n",
            "0.2876218331744299\n",
            "0.2891766491448483\n",
            "0.2908405489323999\n",
            "0.2928172554201482\n",
            "0.294520066674713\n",
            "0.2962492018404519\n",
            "0.2981497954834453\n",
            "0.2997009883756223\n",
            "0.30115697954012\n",
            "0.3030021429976539\n",
            "0.3048097207723066\n",
            "0.30642020992001\n",
            "0.30833631875874745\n",
            "0.3099834368661846\n",
            "0.311850505869102\n",
            "0.3136620811184349\n",
            "0.3152126686652298\n",
            "0.31689108347953737\n",
            "0.31871192092480866\n",
            "0.3203022262019575\n",
            "0.32221455967334833\n",
            "0.3240999930045184\n",
            "0.32572585633953516\n",
            "0.3272840581891482\n",
            "0.328987456648551\n",
            "0.33050892831724316\n",
            "0.3324714891441033\n",
            "0.33410515398015755\n",
            "0.33595994229206955\n",
            "0.3374640208376033\n",
            "0.33947327710173625\n",
            "0.34117732160841413\n",
            "0.34318108692803345\n",
            "0.3451398922049481\n",
            "0.34690901598966944\n",
            "0.34862253641533425\n",
            "0.3503010230296103\n",
            "0.35202597062606034\n",
            "0.3540599064144027\n",
            "0.3557817818563613\n",
            "0.35773049839927107\n",
            "0.35959196319360565\n",
            "0.36138950894250893\n",
            "0.36334863404178863\n",
            "0.36506225081051097\n",
            "0.3670438768918557\n",
            "0.3687331908194305\n",
            "0.37030924982427027\n",
            "0.37221280144303653\n",
            "0.37406652845690014\n",
            "0.37604763440768735\n",
            "0.37775677854142836\n",
            "0.37922131085334837\n",
            "0.38102740689616676\n",
            "0.3825343882336336\n",
            "0.3842712399904685\n",
            "0.38581758143042055\n",
            "0.3873296860233902\n",
            "0.38891425690687526\n",
            "0.39057553652912147\n",
            "0.39228473340763764\n",
            "0.39402836439249767\n",
            "0.39555082037625716\n",
            "0.3973972070247621\n",
            "0.3994685821521008\n",
            "0.40139668905521597\n",
            "0.4029917813018155\n",
            "0.4048511417930388\n",
            "0.40626722985826186\n",
            "0.4081310192337426\n",
            "0.4097703916337484\n",
            "0.4113727489395825\n",
            "0.4131123822973207\n",
            "0.4148098541342694\n",
            "0.4166117629126819\n",
            "0.41833640562603847\n",
            "0.4202994634123409\n",
            "0.42207220585449884\n",
            "0.4239197066982689\n",
            "0.42568513529989727\n",
            "0.427286098985111\n",
            "0.42877246687174453\n",
            "0.43004034448157796\n",
            "0.43181289515227\n",
            "0.4335707675313096\n",
            "0.4352448174868093\n",
            "0.43686492310460573\n",
            "0.4384528999895696\n",
            "0.4400980735526365\n",
            "0.44205405042909296\n",
            "0.4438481817922324\n",
            "0.4453019793228725\n",
            "0.44676214784307555\n",
            "0.44852425748734825\n",
            "0.4506557226333472\n",
            "0.4522799580450863\n",
            "0.45397635913261064\n",
            "0.4553437363308714\n",
            "0.45698596350372295\n",
            "0.4585661678515432\n",
            "0.46022146566749533\n",
            "0.46169343545003927\n",
            "0.4632607247975781\n",
            "0.46524317177665203\n",
            "0.4671200035935473\n",
            "0.4689164487144831\n",
            "0.4707193800707912\n",
            "0.47238239402051474\n",
            "0.4738204662909593\n",
            "0.4753755395827086\n",
            "0.47717990480420536\n",
            "0.47892556692023414\n",
            "0.4806606261169209\n",
            "0.4823115083110302\n",
            "0.4839922182852655\n",
            "0.4852758175729181\n",
            "0.4868938216315511\n",
            "0.48856723529603474\n",
            "0.49008787874980353\n",
            "0.4918194400227588\n",
            "0.49336957756210775\n",
            "0.49541461673539006\n",
            "0.49726175781710985\n",
            "0.4992530078381833\n",
            "0.5009996259151517\n",
            "0.5027284776920553\n",
            "0.5042637169665998\n",
            "0.5061557894320134\n",
            "0.5078116383241571\n",
            "0.5093652807995487\n",
            "0.5109312009171146\n",
            "0.5127164048459524\n",
            "0.5146240205563548\n",
            "0.5161063796876336\n",
            "0.5177196676621352\n",
            "0.5193128845728266\n",
            "0.5215333816797837\n",
            "0.5231383445165346\n",
            "0.5248295883540912\n",
            "0.5264975699164983\n",
            "0.5281381223665174\n",
            "0.5296497129265915\n",
            "0.5313908466140328\n",
            "0.533234729562574\n",
            "0.53488429176533\n",
            "0.5365002730770794\n",
            "0.5381729923703177\n",
            "0.5396751797260226\n",
            "0.5413611841476177\n",
            "0.5428478974667962\n",
            "0.5444656785034463\n",
            "0.5459670164548528\n",
            "0.5473717715581665\n",
            "0.5493711880253404\n",
            "0.5509175430325901\n",
            "0.5526753262333248\n",
            "0.5542756876219874\n",
            "0.5562567292424418\n",
            "0.5581063764631901\n",
            "0.5596448049673339\n",
            "0.5612738645442611\n",
            "0.5628829919316275\n",
            "0.5644801153093958\n",
            "0.5663128726165313\n",
            "0.5684776678871926\n",
            "0.5701972765233511\n",
            "0.5721811730690929\n",
            "0.5739768658909956\n",
            "0.575473233028446\n",
            "0.5768179189975914\n",
            "0.5785413081841091\n",
            "0.5799339117144074\n",
            "0.5816645946191705\n",
            "0.583159634905398\n",
            "0.5846146839811369\n",
            "0.5865081349754577\n",
            "0.5881515304603235\n",
            "0.5900137195044466\n",
            "0.5920706204593639\n",
            "0.593817330458585\n",
            "0.5953849147805168\n",
            "0.5972885601508343\n",
            "0.5994002482927668\n",
            "0.6010895744156655\n",
            "0.6024451521808839\n",
            "0.604371013467574\n",
            "0.6060432249780201\n",
            "0.6076019995505243\n",
            "0.6092046734774509\n",
            "0.6108556049101798\n",
            "0.6127746590720419\n",
            "0.6145544260206734\n",
            "0.6164158069721574\n",
            "0.6182276497564048\n",
            "0.6199651577740984\n",
            "0.62144693076763\n",
            "0.6229243110055509\n",
            "0.624738965879011\n",
            "0.626363556479554\n",
            "0.6282071226545612\n",
            "0.6302583565949784\n",
            "0.6321440573086214\n",
            "0.6339941159691043\n",
            "0.6359708931897302\n",
            "0.6375272405879272\n",
            "0.6392452249594052\n",
            "0.6408327115923548\n",
            "0.6422953900626248\n",
            "0.6438697142826627\n",
            "0.6453991026982017\n",
            "0.6473719847324254\n",
            "0.64892702799319\n",
            "0.6506936538707265\n",
            "0.6524315844563877\n",
            "0.6539874623346207\n",
            "0.6558475967715768\n",
            "0.6570612814877649\n",
            "0.6591135221521568\n",
            "0.6607056484960229\n",
            "0.6620940461640468\n",
            "0.6633863655654976\n",
            "0.6654639845461492\n",
            "0.6672875772199363\n",
            "0.6687225363291133\n",
            "0.6703964115866005\n",
            "0.6723922430859197\n",
            "0.6740118528113646\n",
            "0.6758488368652665\n",
            "0.6773554063056741\n",
            "0.6790208128254737\n",
            "0.6807243769888378\n",
            "0.682636804516663\n",
            "0.6846497966657819\n",
            "0.6863239746721809\n",
            "0.6880771099301555\n",
            "0.6899857168154948\n",
            "0.6919571875458788\n",
            "0.6938220092555141\n",
            "0.6955258386671695\n",
            "0.6974066553060966\n",
            "0.6990834010379089\n",
            "0.7006449774860422\n",
            "0.7022695507082488\n",
            "0.7039943307714389\n",
            "0.7057100073303408\n",
            "0.7075478645694225\n",
            "0.7091228397148649\n",
            "0.7108647781412315\n",
            "0.7123503115628381\n",
            "0.714056942392798\n",
            "0.7156276199061548\n",
            "0.7171120453825996\n",
            "0.7189224227462583\n",
            "0.7207064665949253\n",
            "0.7223486821822194\n",
            "0.7242272431435792\n",
            "0.7261959670297325\n",
            "0.7278395372126109\n",
            "0.7294773501355934\n",
            "0.7312095312358778\n",
            "0.732832015704011\n",
            "0.7345426917228552\n",
            "0.7363993575048569\n",
            "0.738408234646863\n",
            "0.7403531921336718\n",
            "0.7417760409814927\n",
            "0.7434165557784498\n",
            "0.7447989987747748\n",
            "0.7466638338992663\n",
            "0.7480175175782665\n",
            "0.7496290794571342\n",
            "0.7513556542146541\n",
            "0.7530758207868737\n",
            "0.7545919428243661\n",
            "0.7562054085457112\n",
            "0.758090688215802\n",
            "0.7597094933547632\n",
            "0.7616482043967527\n",
            "0.7634583880072054\n",
            "0.7652866353311807\n",
            "0.7668238714375459\n",
            "0.7685783692180653\n",
            "0.769958635653986\n",
            "0.7720671636826547\n",
            "0.7737269164503687\n",
            "0.7755009116571577\n",
            "0.7768608506225869\n",
            "0.778399355042621\n",
            "0.7799798525355356\n",
            "0.781565277244124\n",
            "0.783278667789591\n",
            "0.7850682974014136\n",
            "0.7868200266910026\n",
            "0.7883551996534742\n",
            "0.7902537804582844\n",
            "0.7919523620696933\n",
            "0.7935220761524747\n",
            "0.7954552188096449\n",
            "0.7971121331157587\n",
            "0.7992359658183954\n",
            "0.8009981416222994\n",
            "0.8026927923759841\n",
            "0.8044152144733292\n",
            "0.8058578971859134\n",
            "0.8073732067099617\n",
            "0.8095977221760908\n",
            "0.8114178233286914\n",
            "0.8132326516806317\n",
            "0.8147773011718564\n",
            "0.8164154913876672\n",
            "0.8179638862152538\n",
            "0.8197438109408864\n",
            "0.8216686443142269\n",
            "0.8233846791107636\n",
            "0.8251685162486933\n",
            "0.8271025618171448\n",
            "0.8288410777783455\n",
            "0.8304278839122304\n",
            "0.8319600809108266\n",
            "0.833686998204502\n",
            "0.8354285242764846\n",
            "0.8372869800271281\n",
            "0.8389638743894484\n",
            "0.8407933473434595\n",
            "0.8424036172802186\n",
            "0.8443153955594963\n",
            "0.8460536825534938\n",
            "0.8476654668445782\n",
            "0.8493162911108998\n",
            "0.8508872707634021\n",
            "0.8526272541269317\n",
            "0.8541718461476934\n",
            "0.8559469550924228\n",
            "0.8579433765405279\n",
            "0.8597626866739424\n",
            "0.8614252826289448\n",
            "0.8632439711057317\n",
            "0.864665917804479\n",
            "0.8666369290760411\n",
            "0.86843398259119\n",
            "0.8699689404586392\n",
            "0.8714098729898253\n",
            "0.8729643081612599\n",
            "0.8746723559353967\n",
            "0.8764987361553075\n",
            "0.8779265975860684\n",
            "0.8793736318950458\n",
            "0.8811279444590859\n",
            "0.8829506270568389\n",
            "0.8843595604305072\n",
            "0.8863145941511139\n",
            "0.8879105615646333\n",
            "0.8896454783809155\n",
            "0.8913500474389556\n",
            "0.8930927739881188\n",
            "0.8948019027252636\n",
            "0.8966582113367212\n",
            "0.8984223487584487\n",
            "0.8998166849393674\n",
            "0.9015184460241167\n",
            "0.9032401461582964\n",
            "0.9050426419891054\n",
            "0.9064783662024056\n",
            "0.9082399817836254\n",
            "0.9099316614515641\n",
            "0.9116492875854073\n",
            "0.9136393028299522\n",
            "0.9151908158493773\n",
            "0.9169011580212342\n",
            "0.9185264108278562\n",
            "0.9203185398712792\n",
            "0.9220437117092445\n",
            "0.9238694057897534\n",
            "0.9254082306419187\n",
            "0.9269668171015542\n",
            "0.9285710444078421\n",
            "0.9302504648790335\n",
            "0.9318096644585699\n",
            "0.9331829845905304\n",
            "0.9351142247771973\n",
            "0.9366556021106213\n",
            "0.9384910965057285\n",
            "0.9401141497332727\n",
            "0.9418649396780506\n",
            "0.9435166859108469\n",
            "0.9452851996244982\n",
            "0.9468219575217313\n",
            "0.948633638108173\n",
            "0.9503984767610155\n",
            "0.9522676282679029\n",
            "0.9541215188515461\n",
            "0.9560665219183773\n",
            "0.9577599313405468\n",
            "0.9595079683434323\n",
            "0.9613905135170578\n",
            "0.9631910457483033\n",
            "0.9649503236384038\n",
            "0.9665850442846108\n",
            "0.9683985032541368\n",
            "0.9701022680305764\n",
            "0.9716506918220569\n",
            "0.9730963995846946\n",
            "0.9748552613093725\n",
            "0.9766392107967221\n",
            "0.9785405602449041\n",
            "0.980115210689852\n",
            "0.9818954835919773\n",
            "0.9836251548946361\n",
            "0.9851204514351037\n",
            "0.9871008617951132\n",
            "0.9888358051362245\n",
            "0.9908464368804336\n",
            "0.9924956849011619\n",
            "0.994188375104114\n",
            "0.9959262687989208\n",
            "0.9977854599275857\n",
            "0.9997288083938687\n",
            "1.0014864391530567\n",
            "1.0032554461675531\n",
            "1.004907103374486\n",
            "1.0065814909880118\n",
            "1.00841143376687\n",
            "1.0099545849101317\n",
            "1.0115735049900192\n",
            "1.0130288575768776\n",
            "1.014752164140077\n",
            "1.0164438761256236\n",
            "1.01838411646121\n",
            "1.0201077785180963\n",
            "1.0216935297564778\n",
            "1.0237953407532723\n",
            "1.0251994594131284\n",
            "1.0266970704736003\n",
            "1.0286005172125823\n",
            "1.030254676442622\n",
            "1.0319109675676927\n",
            "1.0333084509043438\n",
            "1.0351433016912406\n",
            "1.0365173488169375\n",
            "1.0382772834252214\n",
            "1.039996592361299\n",
            "1.0414265279117447\n",
            "1.0431752681274853\n",
            "1.0446971436138348\n",
            "1.0467806061362976\n",
            "1.04855387236761\n",
            "1.0503956388177165\n",
            "1.05210212711483\n",
            "1.053957720927875\n",
            "1.05577885441463\n",
            "1.0572436842924493\n",
            "1.0590949319969967\n",
            "1.0608414987011638\n",
            "1.0628065046904338\n",
            "1.064468723962374\n",
            "1.066120844286726\n",
            "1.068016729162782\n",
            "1.0698786611904574\n",
            "1.0718044220181682\n",
            "1.0736475936744525\n",
            "1.0754478106565792\n",
            "1.0772423116904695\n",
            "1.0789504408683923\n",
            "1.080516213117658\n",
            "1.0823475330534493\n",
            "1.084094453726888\n",
            "1.0855180538066513\n",
            "1.0872284318022716\n",
            "1.0891799067749697\n",
            "1.0912473679655958\n",
            "1.0933282069690393\n",
            "1.0948009427703556\n",
            "1.0967177156444705\n",
            "1.098417590874845\n",
            "1.1001410544528376\n",
            "1.1021511660664893\n",
            "1.103615888473018\n",
            "1.1054319641016939\n",
            "1.1072300862320854\n",
            "1.1087563047781015\n",
            "1.1104673106804528\n",
            "1.1124418936574552\n",
            "1.1143587985459495\n",
            "1.1160604577997457\n",
            "1.1175295382052126\n",
            "1.1191947931981148\n",
            "1.1208731709691264\n",
            "1.122640114687288\n",
            "1.1242960634286447\n",
            "1.1264214123149052\n",
            "1.1283051358616871\n",
            "1.1300477451833009\n",
            "1.1316169543796792\n",
            "1.133175517515758\n",
            "1.1350085826023766\n",
            "1.1366459160966946\n",
            "1.1383274862985782\n",
            "1.139947846036433\n",
            "1.1417814687542294\n",
            "1.1432942427180308\n",
            "1.144925950204625\n",
            "1.1466255393022162\n",
            "1.1483988125458398\n",
            "1.1500389291654767\n",
            "1.1517340437987882\n",
            "1.153628101815348\n",
            "1.1556864799288533\n",
            "1.1574730869296872\n",
            "1.1591480914741525\n",
            "1.1611591259689282\n",
            "1.1630155678142977\n",
            "1.1648732964187631\n",
            "1.1666674886060797\n",
            "1.1681674160158542\n",
            "1.170016896922875\n",
            "1.1715916406620495\n",
            "1.1736058477702958\n",
            "1.1755286247071708\n",
            "1.1772035542503951\n",
            "1.1789617873061344\n",
            "1.1807764600152555\n",
            "1.1825550809083387\n",
            "1.184216060525621\n",
            "1.1857323459804516\n",
            "1.1877314772294916\n",
            "1.1894309382761836\n",
            "1.1910832140909131\n",
            "1.1925175869099014\n",
            "1.1942350958161951\n",
            "1.1956676679956333\n",
            "1.1972968864166522\n",
            "1.1992058760827156\n",
            "1.2009289045925335\n",
            "1.2025931578920321\n",
            "1.2045510829714559\n",
            "1.2068786583745572\n",
            "1.2085087350414843\n",
            "1.2101329033789427\n",
            "1.2116185288752437\n",
            "1.2132400996087458\n",
            "1.21488508170523\n",
            "1.2165607364128923\n",
            "1.2182291443543056\n",
            "1.2197168618059524\n",
            "1.2212334091553603\n",
            "1.2229450975385163\n",
            "1.224863106256251\n",
            "1.2265472220795235\n",
            "1.2280814181965636\n",
            "1.2295903727374113\n",
            "1.2312368853470248\n",
            "1.2327878228996112\n",
            "1.2342427468208401\n",
            "1.235838020167997\n",
            "1.2378970717682558\n",
            "1.2395929411396651\n",
            "1.241333409724638\n",
            "1.2429694668258853\n",
            "1.2445109459902624\n",
            "1.2459149628191653\n",
            "1.2479163380077734\n",
            "1.2498207925378209\n",
            "1.2516767413872283\n",
            "1.2532202093040241\n",
            "1.2550898670692883\n",
            "1.2568409125060986\n",
            "1.2584001590376315\n",
            "1.260048497896975\n",
            "1.26164996433441\n",
            "1.2635559431274834\n",
            "1.2652639841941922\n",
            "1.2668688070896033\n",
            "1.2684204519709663\n",
            "1.2700901465190342\n",
            "1.2718943708845416\n",
            "1.2735438803425225\n",
            "1.275015007123313\n",
            "1.2765606058680492\n",
            "1.278222813554432\n",
            "1.2800279141539503\n",
            "1.2819951598144248\n",
            "1.2837497371694315\n",
            "1.2851633738221415\n",
            "1.2868803923239793\n",
            "1.2884390583581022\n",
            "1.2902855722952986\n",
            "1.2922262406867484\n",
            "1.2939838679397808\n",
            "1.295541332429632\n",
            "1.2972695601108435\n",
            "1.2991089817050778\n",
            "1.3008288165644917\n",
            "1.302708881056827\n",
            "1.3042139715093481\n",
            "1.305888208053301\n",
            "1.3075079110729726\n",
            "1.3092953930882847\n",
            "1.310884385538833\n",
            "1.312604853640432\n",
            "1.3142165594241197\n",
            "1.3158522477692656\n",
            "1.3178493727351088\n",
            "1.3194669685552798\n",
            "1.3215092383229825\n",
            "1.3230566495977094\n",
            "1.325028132218534\n",
            "1.3268765060950423\n",
            "1.328488487035722\n",
            "1.3300081862665503\n",
            "1.3316729522269706\n",
            "1.3332625465167454\n",
            "1.334971664125657\n",
            "1.3363629883664954\n",
            "0.0016335765723987004\n",
            "0.0032285534207473327\n",
            "0.004670403040278598\n",
            "0.00636315025636912\n",
            "0.007880006452350666\n",
            "0.009677677965530044\n",
            "0.01133386939382919\n",
            "0.013292970254902949\n",
            "0.015267820766819713\n",
            "0.017029407231704048\n",
            "0.018787624738405426\n",
            "0.02068482899604856\n",
            "0.022413465220605017\n",
            "0.02428568095502341\n",
            "0.025669258726222437\n",
            "0.027266138197515933\n",
            "0.02886362164221761\n",
            "0.030806000427821712\n",
            "0.03256309444032362\n",
            "0.034214674511833874\n",
            "0.03597655854261745\n",
            "0.03763749334208496\n",
            "0.039495063712224936\n",
            "0.04133202916825824\n",
            "0.043289039293518454\n",
            "0.045107013581658875\n",
            "0.04686810964208735\n",
            "0.04849186410074648\n",
            "0.05004616908710021\n",
            "0.051577409667432156\n",
            "0.053310533161358455\n",
            "0.05503219625224238\n",
            "0.05653828443468684\n",
            "0.058343393113607034\n",
            "0.060330870511281824\n",
            "0.061887506633768304\n",
            "0.06323867883828595\n",
            "0.06516073076316463\n",
            "0.06690717734339292\n",
            "0.06854157786235175\n",
            "0.07039201564496131\n",
            "0.072290547516035\n",
            "0.07400145692288723\n",
            "0.07576983039031553\n",
            "0.07720260394503699\n",
            "0.07891882243363754\n",
            "0.08052302199556395\n",
            "0.0820731243209156\n",
            "0.0835513606705629\n",
            "0.08535463441058498\n",
            "0.08694907298783207\n",
            "0.08891074751953945\n",
            "0.09059213326715143\n",
            "0.09239454967591464\n",
            "0.0941659600838371\n",
            "0.09566627698176353\n",
            "0.09719809623020689\n",
            "0.09873511647934194\n",
            "0.10046125723577826\n",
            "0.10210525882823387\n",
            "0.1035333507506134\n",
            "0.10514631493926963\n",
            "0.10670363811580726\n",
            "0.10857297910753723\n",
            "0.11043619880895786\n",
            "0.11245236905944317\n",
            "0.11415951559915567\n",
            "0.11619360078021389\n",
            "0.11818896489375083\n",
            "0.11979706101405346\n",
            "0.121470148148744\n",
            "0.12315314261199874\n",
            "0.12484542975950119\n",
            "0.1263806332102822\n",
            "0.1276096971443547\n",
            "0.12928344618024118\n",
            "0.1310723454446134\n",
            "0.13290374221094428\n",
            "0.1343828643984197\n",
            "0.13618838009626968\n",
            "0.13801701340224126\n",
            "0.13992959199963934\n",
            "0.14153975400778337\n",
            "0.14343385745192427\n",
            "0.14521683299023172\n",
            "0.14673898881658567\n",
            "0.14826797341446743\n",
            "0.14967211570276323\n",
            "0.15156562035650853\n",
            "0.15320551258218867\n",
            "0.15468921624790982\n",
            "0.15646788226369093\n",
            "0.158267620579361\n",
            "0.1598720728893719\n",
            "0.16147272772801197\n",
            "0.16317644189385808\n",
            "0.16479402551870517\n",
            "0.16662820983115975\n",
            "0.16853438801777637\n",
            "0.17025195119326073\n",
            "0.17186945463385422\n",
            "0.17353804901127926\n",
            "0.17530004112311945\n",
            "0.1771395444260229\n",
            "0.1788196152128527\n",
            "0.18057560707297166\n",
            "0.18218799518502277\n",
            "0.18384184267209924\n",
            "0.18536918044395154\n",
            "0.18691694385865154\n",
            "0.18837022735639605\n",
            "0.19017322609186782\n",
            "0.1917261347136534\n",
            "0.19344516285240193\n",
            "0.19516771147623085\n",
            "0.1970343156848722\n",
            "0.19889811817032602\n",
            "0.2005798444723534\n",
            "0.20214068935350385\n",
            "0.20368970431330258\n",
            "0.2051677915751172\n",
            "0.20683320175351388\n",
            "0.20874376537854714\n",
            "0.2104219669271308\n",
            "0.21232445587587478\n",
            "0.2143260607938937\n",
            "0.21588414251956795\n",
            "0.21769721931813624\n",
            "0.21965356022500626\n",
            "0.2213969552303519\n",
            "0.2231450433011555\n",
            "0.22492805207172012\n",
            "0.22661090552654412\n",
            "0.2284558171506428\n",
            "0.2303795344994196\n",
            "0.2322031340330763\n",
            "0.23380782125551072\n",
            "0.23527145827822674\n",
            "0.23669358897392098\n",
            "0.23853218707891985\n",
            "0.240180864053614\n",
            "0.24156935211947506\n",
            "0.24339663753729038\n",
            "0.24537582424900417\n",
            "0.24716663665478797\n",
            "0.24891695159170635\n",
            "0.25064687503268346\n",
            "0.252281121280797\n",
            "0.25403088422687464\n",
            "0.2557227192327495\n",
            "0.2576243285937687\n",
            "0.2594897170810748\n",
            "0.26091025156133313\n",
            "0.2626122971020086\n",
            "0.264261312344495\n",
            "0.2661235236450839\n",
            "0.26757082899513146\n",
            "0.26927773894556345\n",
            "0.2710293170131381\n",
            "0.27300979886823296\n",
            "0.27476775798651265\n",
            "0.27695775580833026\n",
            "0.278640269318505\n",
            "0.2803924609633053\n",
            "0.28187536431090604\n",
            "0.28360228861689263\n",
            "0.2856042206744709\n",
            "0.28733230993875764\n",
            "0.2889996794483546\n",
            "0.29057799641738463\n",
            "0.292305327742301\n",
            "0.29415036909415593\n",
            "0.2957772533302112\n",
            "0.2974540508921494\n",
            "0.2992205284440609\n",
            "0.3012889898036752\n",
            "0.3030161619796168\n",
            "0.30478115033005815\n",
            "0.30648421936327847\n",
            "0.3084040127142006\n",
            "0.3101755636732292\n",
            "0.31210213152648847\n",
            "0.3134884532455288\n",
            "0.31514475321220925\n",
            "0.31663524906348695\n",
            "0.3186284793002526\n",
            "0.32042384543992064\n",
            "0.32215092020571384\n",
            "0.32417933303681784\n",
            "0.3255873054189755\n",
            "0.3273253917998975\n",
            "0.32902602710382406\n",
            "0.3302991869656936\n",
            "0.3320956179095656\n",
            "0.33403318198135745\n",
            "0.33571208811477016\n",
            "0.3372355784144243\n",
            "0.33880348667464294\n",
            "0.34082446279733075\n",
            "0.3422601810654106\n",
            "0.3437155729821881\n",
            "0.3453381269636666\n",
            "0.34702729271805804\n",
            "0.3486847463623642\n",
            "0.35033469363246733\n",
            "0.352003984972644\n",
            "0.3533351194980504\n",
            "0.35528239440125275\n",
            "0.35699410808970555\n",
            "0.35866624688553383\n",
            "0.36035594100232626\n",
            "0.3620145263726754\n",
            "0.3636933933286106\n",
            "0.3654358456354312\n",
            "0.3671773540241944\n",
            "0.3688742179242546\n",
            "0.3704021552487103\n",
            "0.3721755312379364\n",
            "0.37363477001714585\n",
            "0.3753473014020554\n",
            "0.3773207712508833\n",
            "0.3791259468516425\n",
            "0.3807917860005518\n",
            "0.38226722626734877\n",
            "0.3839302053079581\n",
            "0.3854110154806805\n",
            "0.387214828558895\n",
            "0.3888762421772608\n",
            "0.3905490622343615\n",
            "0.3922117642886803\n",
            "0.3941242855680568\n",
            "0.39592644267374905\n",
            "0.3978313414184639\n",
            "0.39938586683529415\n",
            "0.4012288259880622\n",
            "0.402974341455323\n",
            "0.4044945312430487\n",
            "0.4061193745916762\n",
            "0.40765388351877024\n",
            "0.40911944221962443\n",
            "0.41085037512852407\n",
            "0.4125600157643828\n",
            "0.41439042775832174\n",
            "0.41590684355067475\n",
            "0.4175576042488713\n",
            "0.4193886854612004\n",
            "0.42132664374683215\n",
            "0.4229396487898229\n",
            "0.4246055501348832\n",
            "0.42639161535846\n",
            "0.4282468059636138\n",
            "0.4300623930171323\n",
            "0.43183098234179074\n",
            "0.43352898604729595\n",
            "0.4352191809345694\n",
            "0.43700151743791293\n",
            "0.4385082214842062\n",
            "0.440259040087995\n",
            "0.4420310239810163\n",
            "0.4436188179361241\n",
            "0.4453652479002238\n",
            "0.4470140818897111\n",
            "0.44849129200286575\n",
            "0.4502184899414287\n",
            "0.45221342295027145\n",
            "0.4541299067952139\n",
            "0.45570708716007147\n",
            "0.45764915709910187\n",
            "0.4593993790466767\n",
            "0.4610214693009701\n",
            "0.46259719156243306\n",
            "0.46413225827314664\n",
            "0.4657623635990845\n",
            "0.46700981823379734\n",
            "0.4687208151420974\n",
            "0.4703956821842877\n",
            "0.47210215157864954\n",
            "0.47353766191645963\n",
            "0.4750858186303502\n",
            "0.47672605034335497\n",
            "0.47824526999307715\n",
            "0.47988313886210743\n",
            "0.4818262536354992\n",
            "0.48375371433889774\n",
            "0.4849175655323526\n",
            "0.48675935119009384\n",
            "0.4883112123860118\n",
            "0.4899947208821621\n",
            "0.4914219086737279\n",
            "0.49319261388705515\n",
            "0.49501598582548256\n",
            "0.4966756818849412\n",
            "0.4983733191209681\n",
            "0.4998999884366379\n",
            "0.5016511530827379\n",
            "0.5032475700463785\n",
            "0.5051836257090654\n",
            "0.5070699195727668\n",
            "0.5090661917805976\n",
            "0.5108509914344533\n",
            "0.5124056359081317\n",
            "0.5140831738786624\n",
            "0.5161071165138499\n",
            "0.5178169245305269\n",
            "0.5195259311619926\n",
            "0.5209359980910026\n",
            "0.5225075579360318\n",
            "0.5240563811243647\n",
            "0.5254899620095177\n",
            "0.5272337054962393\n",
            "0.5290690718404473\n",
            "0.5307835579833107\n",
            "0.5326003270685825\n",
            "0.5342335124759723\n",
            "0.53585385575014\n",
            "0.5375939383531165\n",
            "0.5391388611720346\n",
            "0.5407512980653807\n",
            "0.5423048652346482\n",
            "0.5438975647587301\n",
            "0.5455177211395615\n",
            "0.5474247685478776\n",
            "0.549299871982516\n",
            "0.5511388383862917\n",
            "0.5526581889833025\n",
            "0.554297659708106\n",
            "0.5560861984482202\n",
            "0.5579982346585949\n",
            "0.559859115449364\n",
            "0.5615801800547353\n",
            "0.5628471816592204\n",
            "0.5646395085717711\n",
            "0.5665442446613556\n",
            "0.5682714893994734\n",
            "0.5698703461900696\n",
            "0.5715132313006369\n",
            "0.5729002310796771\n",
            "0.5743826455472375\n",
            "0.576472075546489\n",
            "0.5780981233357774\n",
            "0.5795980469345132\n",
            "0.5811289143379387\n",
            "0.5827400916067841\n",
            "0.5844805932715725\n",
            "0.5860857373613226\n",
            "0.5879400867940215\n",
            "0.589313395797749\n",
            "0.5906203823626194\n",
            "0.5921769160443865\n",
            "0.5938499868678315\n",
            "0.5954659110140008\n",
            "0.5973153140233911\n",
            "0.599108877846652\n",
            "0.6006651173162338\n",
            "0.6019880498766594\n",
            "0.6037348931097923\n",
            "0.6056311976574266\n",
            "0.6074382071299931\n",
            "0.6090427092883898\n",
            "0.6109286176274195\n",
            "0.6126610715980725\n",
            "0.6143229911699319\n",
            "0.6160016149816001\n",
            "0.617648862664352\n",
            "0.6193082084131363\n",
            "0.6208120287226899\n",
            "0.6224197696541887\n",
            "0.6242100166542756\n",
            "0.6257193291278751\n",
            "0.6275302087864303\n",
            "0.6293763724129523\n",
            "0.6312877322401842\n",
            "0.6329284048141421\n",
            "0.6348596459154583\n",
            "0.6363626235281415\n",
            "0.6379651636113901\n",
            "0.63976409276733\n",
            "0.641398862194832\n",
            "0.6430676343191005\n",
            "0.6446637665219319\n",
            "0.6464875558453143\n",
            "0.6484417569301927\n",
            "0.6500760553133152\n",
            "0.6520863963514948\n",
            "0.6536490475125325\n",
            "0.6555364256929559\n",
            "0.6571057068417444\n",
            "0.6584893147963697\n",
            "0.6600646109837095\n",
            "0.662009870914547\n",
            "0.6636619406283054\n",
            "0.6653087998899962\n",
            "0.6669693179142749\n",
            "0.6687413374786182\n",
            "0.6702799083631666\n",
            "0.6720536242970421\n",
            "0.6740366092423344\n",
            "0.6755530126869221\n",
            "0.6774051061371709\n",
            "0.6789797888997265\n",
            "0.6804371787154156\n",
            "0.6824535494265349\n",
            "0.6842910020857516\n",
            "0.6860302229366644\n",
            "0.6877891511258567\n",
            "0.6891985643855141\n",
            "0.6910157810391673\n",
            "0.6927808512507192\n",
            "0.694756172959457\n",
            "0.6961672897533993\n",
            "0.6976466553900248\n",
            "0.6991752889150243\n",
            "0.7008670757493705\n",
            "0.7024704512122952\n",
            "0.7039344441860228\n",
            "0.7058963806123075\n",
            "0.7076646699320019\n",
            "0.709724226571105\n",
            "0.7116244688363331\n",
            "0.7132685743939237\n",
            "0.7147655240105241\n",
            "0.7163482242837891\n",
            "0.7178764687779614\n",
            "0.7191673937965842\n",
            "0.7209992215151677\n",
            "0.722456709808096\n",
            "0.7239338229684269\n",
            "0.7257807855410954\n",
            "0.727498181488203\n",
            "0.7291219550020555\n",
            "0.7310617636231815\n",
            "0.7328205988230303\n",
            "0.7343335970283469\n",
            "0.7358930037759454\n",
            "0.7373449880143871\n",
            "0.7391719871469776\n",
            "0.7409964996530577\n",
            "0.7428447576739904\n",
            "0.7446526409415029\n",
            "0.7464284139216099\n",
            "0.7481644100240429\n",
            "0.7501555240672567\n",
            "0.7517021821283013\n",
            "0.7533275980473785\n",
            "0.7550041059703778\n",
            "0.7566334855221116\n",
            "0.7588682285964946\n",
            "0.7605840266513093\n",
            "0.762303499004725\n",
            "0.7639740034747307\n",
            "0.7655576763250639\n",
            "0.7672168120093967\n",
            "0.7688080944368602\n",
            "0.7703691693523046\n",
            "0.7719520565189059\n",
            "0.7737077843502659\n",
            "0.7755647791011254\n",
            "0.7775231921459402\n",
            "0.7793490744917594\n",
            "0.7813137460242757\n",
            "0.7830878935201698\n",
            "0.7850641530493031\n",
            "0.7867668208563724\n",
            "0.7882223739038647\n",
            "0.7898961003784024\n",
            "0.7915561523888727\n",
            "0.7930251340122174\n",
            "0.7947928608226045\n",
            "0.7965581008540396\n",
            "0.7983111856538622\n",
            "0.7997957419251542\n",
            "0.8014557413432909\n",
            "0.803358415203631\n",
            "0.8048055827465204\n",
            "0.8068265873757775\n",
            "0.8084981842419071\n",
            "0.810559218039598\n",
            "0.8122503068441015\n",
            "0.8138791807472249\n",
            "0.815802338025759\n",
            "0.8177187250703192\n",
            "0.8196938659833826\n",
            "0.8216225384446361\n",
            "0.8233525188987517\n",
            "0.8253858337926743\n",
            "0.8269601881961384\n",
            "0.828959230114432\n",
            "0.8305002643019342\n",
            "0.8320860341381844\n",
            "0.8335269172783093\n",
            "0.8352681079788891\n",
            "0.8370182186441348\n",
            "0.8385587459634942\n",
            "0.8405330789363598\n",
            "0.8424259870863327\n",
            "0.8439587705275592\n",
            "0.8460347432919475\n",
            "0.8477224074971036\n",
            "0.8496926234811163\n",
            "0.8516174739279101\n",
            "0.8536541602190804\n",
            "0.8556349603721248\n",
            "0.857411363240703\n",
            "0.8589505024273377\n",
            "0.8607799395575852\n",
            "0.8623232047271241\n",
            "0.8641800345362299\n",
            "0.8655812124462079\n",
            "0.8672348858450379\n",
            "0.8688837844697411\n",
            "0.870547628311245\n",
            "0.8723712558941463\n",
            "0.8744060849899526\n",
            "0.8763589453514274\n",
            "0.878277373130974\n",
            "0.8800581644867997\n",
            "0.8817401484150411\n",
            "0.8837622695261865\n",
            "0.8857774351868788\n",
            "0.8872211999295617\n",
            "0.8887364432939788\n",
            "0.8906761985605635\n",
            "0.8924847617173743\n",
            "0.8941935207837682\n",
            "0.8957674004842558\n",
            "0.8973695178470953\n",
            "0.8989813817126672\n",
            "0.9005728950890739\n",
            "0.9022483822634763\n",
            "0.9039321918316814\n",
            "0.9058421240438281\n",
            "0.9074066359063854\n",
            "0.9091193282695682\n",
            "0.9109523805510967\n",
            "0.9127021878576644\n",
            "0.9145799997212637\n",
            "0.9162893972128553\n",
            "0.9179006865262376\n",
            "0.9195779930905003\n",
            "0.9211414392342043\n",
            "0.9229355756279147\n",
            "0.9247019899165844\n",
            "0.9264904397832768\n",
            "0.9281496588531357\n",
            "0.9300868302355032\n",
            "0.9316869829316883\n",
            "0.9330740196015829\n",
            "0.9346700719250437\n",
            "0.9363985188171992\n",
            "0.9381449783549589\n",
            "0.9397970227634206\n",
            "0.9414433137230251\n",
            "0.9433774759092599\n",
            "0.9450446723977013\n",
            "0.9464578971533519\n",
            "0.9482282605927314\n",
            "0.9500488356861008\n",
            "0.9518175994038887\n",
            "0.9534875294741463\n",
            "0.9552536125073348\n",
            "0.9571641178996971\n",
            "0.9589022615986407\n",
            "0.9604554717498057\n",
            "0.9626558705059158\n",
            "0.9642684769142619\n",
            "0.9663320444429012\n",
            "0.9681914681973665\n",
            "0.9697293378508\n",
            "0.9714208576075561\n",
            "0.9731039970427218\n",
            "0.9749686370420334\n",
            "0.9767102625059045\n",
            "0.9782874982070435\n",
            "0.9800609645941069\n",
            "0.9820203278070826\n",
            "0.9836187969388255\n",
            "0.9851759678262579\n",
            "0.9866591211779953\n",
            "0.9881925986855841\n",
            "0.9898094086695814\n",
            "0.9916331928099513\n",
            "0.9931811168980416\n",
            "0.9947419131503386\n",
            "0.9964002915050673\n",
            "0.9983181150063224\n",
            "1.0002035519960897\n",
            "1.0020245988960461\n",
            "1.003948089869126\n",
            "1.005864322947724\n",
            "1.0075231427731721\n",
            "1.009198859524544\n",
            "1.0106956996881138\n",
            "1.012247426583029\n",
            "1.0138993487333703\n",
            "1.0155306562133457\n",
            "1.0171026180467337\n",
            "1.0187133911930386\n",
            "1.0205078705802293\n",
            "1.0221236441141504\n",
            "1.0236354782758161\n",
            "1.0251899143619\n",
            "1.0271785013815935\n",
            "1.0288483309928718\n",
            "1.0303481154124756\n",
            "1.0320961780255409\n",
            "1.0336373661790053\n",
            "1.0352765079349509\n",
            "1.0372061243142618\n",
            "1.0388394335041875\n",
            "1.04061823763201\n",
            "1.0420436525283872\n",
            "1.0437231021159141\n",
            "1.0453243859283758\n",
            "1.0472435166158944\n",
            "1.049114990417305\n",
            "1.0509838350593586\n",
            "1.052321561462129\n",
            "1.053817719297336\n",
            "1.0556344705469467\n",
            "1.0574208879104965\n",
            "1.0591430475034982\n",
            "1.060805327447174\n",
            "1.0627556325834426\n",
            "1.064496106046545\n",
            "1.0660775029445853\n",
            "1.0674454659757102\n",
            "1.069548343758449\n",
            "1.0713268114477776\n",
            "1.0731139919337105\n",
            "1.074802468499869\n",
            "1.0766044879508445\n",
            "1.0781968417374983\n",
            "1.0797480233490009\n",
            "1.0815319388418856\n",
            "1.0833326575091429\n",
            "1.0847322645089816\n",
            "1.0864027558690141\n",
            "1.0883518387289608\n",
            "1.0899463279168013\n",
            "1.0917105869868833\n",
            "1.0934460280496445\n",
            "1.0950877146647715\n",
            "1.0967035099978337\n",
            "1.098335251021568\n",
            "1.1002968407957756\n",
            "1.10204949510067\n",
            "1.103644722105597\n",
            "1.1053426445597578\n",
            "1.107176472158993\n",
            "1.1088005505559388\n",
            "1.110491006270699\n",
            "1.1121859523036597\n",
            "1.1139791040774196\n",
            "1.1155412210832776\n",
            "1.1170213711841026\n",
            "1.1189394523115719\n",
            "1.1207904602255663\n",
            "1.1224902732597897\n",
            "1.1241979739245247\n",
            "1.1259221885820179\n",
            "1.1275822637636033\n",
            "1.1294228826337458\n",
            "1.1315473098584148\n",
            "1.1331425353389262\n",
            "1.1349179208126214\n",
            "1.1365483593757806\n",
            "1.138121460862172\n",
            "1.1397373293671766\n",
            "1.1415226604322644\n",
            "1.1435311528118066\n",
            "1.1451771029121125\n",
            "1.1469283930176055\n",
            "1.1486658924985724\n",
            "1.1505416488403555\n",
            "1.1522371413762613\n",
            "1.1538280858408154\n",
            "1.1558175161671456\n",
            "1.1578295517455586\n",
            "1.1596229094678483\n",
            "1.161257710298309\n",
            "1.1629229288576814\n",
            "1.1646350786813995\n",
            "1.1664521392349088\n",
            "1.168099879151415\n",
            "1.169437535431074\n",
            "1.1710209796190871\n",
            "1.1730094654175935\n",
            "1.1749152897873802\n",
            "1.1765341298354557\n",
            "1.1783080591874964\n",
            "1.179843916612513\n",
            "1.1815505386008631\n",
            "1.1832598870062767\n",
            "1.1847620911305519\n",
            "1.1864747472126465\n",
            "1.188166402032613\n",
            "1.1895648192261796\n",
            "1.191402150541925\n",
            "1.1931853099247378\n",
            "1.1947116460031866\n",
            "1.1964853873948003\n",
            "1.1982321086746957\n",
            "1.199978220950612\n",
            "1.2013663818769138\n",
            "1.2029657873046367\n",
            "1.204809441590858\n",
            "1.2063605038406293\n",
            "1.2080939925845018\n",
            "1.2097334979135361\n",
            "1.2112875760668684\n",
            "1.2130222457754032\n",
            "1.214794612906473\n",
            "1.2165962744246968\n",
            "1.218354745289249\n",
            "1.2200932697871762\n",
            "1.2220315786883653\n",
            "1.223577768906303\n",
            "1.2251105828358388\n",
            "1.226597906378529\n",
            "1.2282766402529939\n",
            "1.2297688672304763\n",
            "1.2314642280568857\n",
            "1.2332478185443927\n",
            "1.2348830385891068\n",
            "1.236426545835822\n",
            "1.2379886721406141\n",
            "1.2394430292841723\n",
            "1.2410413507000564\n",
            "1.2429923546283752\n",
            "1.2445929186118534\n",
            "1.2461866800437498\n",
            "1.2482137489501777\n",
            "1.2498920264146518\n",
            "1.251423985604435\n",
            "1.253101525099381\n",
            "1.2550467359440407\n",
            "1.2565564871444117\n",
            "1.2582067185655579\n",
            "1.2597145151604168\n",
            "1.2614961768050328\n",
            "1.2631037875514506\n",
            "1.264646211396093\n",
            "1.2664142918708685\n",
            "1.268156606980297\n",
            "1.2701219172428941\n",
            "1.271557074526082\n",
            "1.273219242425221\n",
            "1.2747646591547506\n",
            "1.2762806196042034\n",
            "1.2781333130643802\n",
            "1.2798258572283303\n",
            "1.281308334196925\n",
            "1.282806934907918\n",
            "1.2847049539656286\n",
            "1.2863681500830004\n",
            "1.2880408451380327\n",
            "1.2895650962734466\n",
            "1.2915153611651466\n",
            "1.2930377003786815\n",
            "1.2950990273214666\n",
            "1.296567140332878\n",
            "1.2983893941125602\n",
            "1.3002840847615391\n",
            "1.3018000348449668\n",
            "1.3035648327959164\n",
            "1.3051677555074472\n",
            "1.3069707652187104\n",
            "1.308434433644385\n",
            "1.3099544167213732\n",
            "1.3117181324897824\n",
            "1.3130418504290569\n",
            "1.3147612572326075\n",
            "1.3163501896211862\n",
            "1.318083448178323\n",
            "1.3201828127931756\n",
            "1.321816043780588\n",
            "1.323428143930557\n",
            "1.32508465899226\n",
            "1.3268054280134722\n",
            "1.3288440468061307\n",
            "1.330542116061501\n",
            "1.33223424223073\n",
            "0.0015329529562264758\n",
            "0.0029496170980546177\n",
            "0.004677695081666912\n",
            "0.006294020911311859\n",
            "0.00818873366431507\n",
            "0.010039235319932708\n",
            "0.011487734592174325\n",
            "0.013269585721633014\n",
            "0.014892200527288724\n",
            "0.01665232294355817\n",
            "0.01820958880207423\n",
            "0.02012626380871629\n",
            "0.02198913365678714\n",
            "0.023618340492248535\n",
            "0.02536275243515249\n",
            "0.026775603251688925\n",
            "0.028343652672779836\n",
            "0.029959632917438323\n",
            "0.031778371395052546\n",
            "0.033225342288346546\n",
            "0.0348969347336713\n",
            "0.03676086130654416\n",
            "0.03857787460317392\n",
            "0.040031939821170114\n",
            "0.04172029672071452\n",
            "0.04356223314314547\n",
            "0.04505859052433687\n",
            "0.0467856963882056\n",
            "0.04837535759981941\n",
            "0.050264620719968206\n",
            "0.051967544628836\n",
            "0.053476348404994094\n",
            "0.055110616604690355\n",
            "0.056846101418175664\n",
            "0.05849464058571154\n",
            "0.0604664634560685\n",
            "0.06237022925520797\n",
            "0.06397670080594699\n",
            "0.06567799115119993\n",
            "0.06737136398739826\n",
            "0.06930985719041752\n",
            "0.07135849139269661\n",
            "0.07323684015542345\n",
            "0.07486128563161397\n",
            "0.07653328951667338\n",
            "0.07831596047677042\n",
            "0.07978113593957613\n",
            "0.0812374707073202\n",
            "0.08288347660123235\n",
            "0.08469826211709805\n",
            "0.08638347262311774\n",
            "0.08850198115229302\n",
            "0.0899948412195191\n",
            "0.09155057000991938\n",
            "0.09329998859054292\n",
            "0.09507702622572174\n",
            "0.09653652994833944\n",
            "0.09825118027074868\n",
            "0.1000390130540599\n",
            "0.10145979601403941\n",
            "0.10313864391478127\n",
            "0.10488715577308479\n",
            "0.10635354345107018\n",
            "0.10821562319460427\n",
            "0.1096525149577109\n",
            "0.11124112477997684\n",
            "0.1131132882269447\n",
            "0.11448918690766825\n",
            "0.11595621423038376\n",
            "0.1175735713270924\n",
            "0.11928071176914302\n",
            "0.12105770397674093\n",
            "0.12254271818243939\n",
            "0.1242888572880679\n",
            "0.12601133030088965\n",
            "0.12766041581892906\n",
            "0.12946418529886114\n",
            "0.13129274085964387\n",
            "0.13294709963566811\n",
            "0.13488904243844854\n",
            "0.13646286177208355\n",
            "0.13800611962442813\n",
            "0.13967034685642213\n",
            "0.14146797903968245\n",
            "0.1432216711666273\n",
            "0.1449907976952965\n",
            "0.14667405877881648\n",
            "0.14829283281970207\n",
            "0.15016843976876926\n",
            "0.1518266278764476\n",
            "0.1534835028526423\n",
            "0.15513076212095178\n",
            "0.15697639555577428\n",
            "0.15859749753152014\n",
            "0.16012152732180818\n",
            "0.16193207801150544\n",
            "0.16321806788749402\n",
            "0.16478840027318892\n",
            "0.16652380276823897\n",
            "0.1682480300783806\n",
            "0.16992548069990504\n",
            "0.1719785122310414\n",
            "0.17346723396759814\n",
            "0.17499857363493546\n",
            "0.17665726693389971\n",
            "0.17826101603105549\n",
            "0.18015284946812388\n",
            "0.1821454318283159\n",
            "0.18435533973567017\n",
            "0.18619895469197226\n",
            "0.18793494606871738\n",
            "0.18955196855623094\n",
            "0.1912800466922848\n",
            "0.19311207624347618\n",
            "0.19495970437593776\n",
            "0.19664574126758233\n",
            "0.19858220288210818\n",
            "0.20015614691292843\n",
            "0.20211927055397913\n",
            "0.20367265418362435\n",
            "0.20566190813508484\n",
            "0.20725825131701692\n",
            "0.20891949282887648\n",
            "0.21068274700428213\n",
            "0.21230149375813087\n",
            "0.21379132389717395\n",
            "0.21555567656636543\n",
            "0.21722817695354257\n",
            "0.21888937212317192\n",
            "0.22054394172585529\n",
            "0.22213630389679423\n",
            "0.22415670988809727\n",
            "0.22568443440415364\n",
            "0.22748141093632143\n",
            "0.2288953367706455\n",
            "0.23082009209391405\n",
            "0.23246068509338458\n",
            "0.23392619958619024\n",
            "0.2354111573885164\n",
            "0.23714072198209252\n",
            "0.2386380985874654\n",
            "0.2401575856196606\n",
            "0.24172069578219557\n",
            "0.24330205944797878\n",
            "0.24481087846829153\n",
            "0.2463804312679164\n",
            "0.24809412852577542\n",
            "0.2498069291224565\n",
            "0.25140604308194214\n",
            "0.25307079715192166\n",
            "0.25465792219352235\n",
            "0.2563975473194171\n",
            "0.2581241502786231\n",
            "0.25990874657545554\n",
            "0.26154557426872155\n",
            "0.26310330126291653\n",
            "0.2645976372691981\n",
            "0.26598660811743774\n",
            "0.2675969280550242\n",
            "0.2690369537114487\n",
            "0.27063376549869544\n",
            "0.272280038317756\n",
            "0.2743611151300123\n",
            "0.2757906730827468\n",
            "0.27734990833360523\n",
            "0.279102954413275\n",
            "0.28061631055134334\n",
            "0.28238184037415875\n",
            "0.28376219263467034\n",
            "0.28533746275450567\n",
            "0.28698034557844976\n",
            "0.28884627096488347\n",
            "0.29077517452752194\n",
            "0.29231441219139587\n",
            "0.2940428898767437\n",
            "0.2957667171802667\n",
            "0.29763043048741566\n",
            "0.29920078970282277\n",
            "0.30111246066325154\n",
            "0.30275043685113073\n",
            "0.3046245605439481\n",
            "0.3065745498213317\n",
            "0.3085023335483678\n",
            "0.3105575856955155\n",
            "0.31241955994949927\n",
            "0.3142727256735877\n",
            "0.31590238678485844\n",
            "0.3173564361489337\n",
            "0.3190539715540074\n",
            "0.3210128402466054\n",
            "0.32244785941775195\n",
            "0.324137508564288\n",
            "0.3259231871961023\n",
            "0.3275450707396583\n",
            "0.3291639283185115\n",
            "0.33070032401462957\n",
            "0.3326219472738788\n",
            "0.3344460221202782\n",
            "0.3360765338553797\n",
            "0.33766892666706955\n",
            "0.33944297248445204\n",
            "0.3412379880085626\n",
            "0.342884273632713\n",
            "0.3447377625328805\n",
            "0.34618386328982576\n",
            "0.34776857152314444\n",
            "0.34968351586090635\n",
            "0.35133076628760607\n",
            "0.3531117600858059\n",
            "0.35504900128640177\n",
            "0.3566974653002551\n",
            "0.35854557423335515\n",
            "0.36032119965004494\n",
            "0.3619464902621706\n",
            "0.36371603402335323\n",
            "0.36568718005324263\n",
            "0.3669982974791466\n",
            "0.3690631255469359\n",
            "0.3709673585794161\n",
            "0.3726675444856629\n",
            "0.3742860459610629\n",
            "0.3760342527838314\n",
            "0.37798999124170873\n",
            "0.3796689115521853\n",
            "0.3811709867109118\n",
            "0.3827937882574623\n",
            "0.38463280832066254\n",
            "0.38631226324364354\n",
            "0.38798957194208794\n",
            "0.3900574526518507\n",
            "0.3917488406991105\n",
            "0.3934145408213291\n",
            "0.3952903526518351\n",
            "0.3970969483980437\n",
            "0.39874929707983264\n",
            "0.40054434476910955\n",
            "0.4024962779810971\n",
            "0.40397712047142753\n",
            "0.40579165689780583\n",
            "0.40752319774359386\n",
            "0.4091947754020886\n",
            "0.4108298815729673\n",
            "0.4126165753130413\n",
            "0.41409046860302196\n",
            "0.41578380348127514\n",
            "0.41737400174445816\n",
            "0.41897359558993286\n",
            "0.4208914697017816\n",
            "0.4225384475629958\n",
            "0.4247550857646386\n",
            "0.4262885083932706\n",
            "0.42828883204008916\n",
            "0.430317730245078\n",
            "0.43207200104013427\n",
            "0.43370225210019087\n",
            "0.43533785080970705\n",
            "0.436718551704036\n",
            "0.4383086307579294\n",
            "0.43983333281543857\n",
            "0.4416867247627824\n",
            "0.44322587202881913\n",
            "0.4449437934419383\n",
            "0.44659404361339483\n",
            "0.4482335507717279\n",
            "0.44978209926039364\n",
            "0.4515594558032882\n",
            "0.45307901692207514\n",
            "0.4546153673430538\n",
            "0.45607546787432696\n",
            "0.4575323951823632\n",
            "0.45894659434438057\n",
            "0.4605839083261807\n",
            "0.46228615234575005\n",
            "0.46372775485753404\n",
            "0.4652251177431677\n",
            "0.4668277744441996\n",
            "0.46857326795987764\n",
            "0.47040940138994886\n",
            "0.4721094478121804\n",
            "0.4736783059356767\n",
            "0.47536382528827015\n",
            "0.47715547200663927\n",
            "0.4789052451663005\n",
            "0.48049146622952904\n",
            "0.4821947941084957\n",
            "0.48367226032344884\n",
            "0.4851462487369547\n",
            "0.4868637774606495\n",
            "0.4885385646234693\n",
            "0.4902824898205145\n",
            "0.49191658179778275\n",
            "0.49346247109610714\n",
            "0.4952693534324236\n",
            "0.4972339231339867\n",
            "0.49904793821027515\n",
            "0.5006857137850789\n",
            "0.5021423761497068\n",
            "0.5038498995249229\n",
            "0.5057076741667355\n",
            "0.5072124437298007\n",
            "0.5088946419908568\n",
            "0.5106870350630387\n",
            "0.5126518410489992\n",
            "0.5145202576351897\n",
            "0.5163564611883724\n",
            "0.5183723049090646\n",
            "0.5198645045995103\n",
            "0.5213519968949926\n",
            "0.5227142919969681\n",
            "0.5242207870459008\n",
            "0.5259336199601898\n",
            "0.5276781497404094\n",
            "0.5291403301841463\n",
            "0.5308751448645921\n",
            "0.5324739960148511\n",
            "0.5341212103129043\n",
            "0.5359195249769694\n",
            "0.5376162695152985\n",
            "0.5394049155742616\n",
            "0.5412824579212062\n",
            "0.5429665382255984\n",
            "0.5447035456252525\n",
            "0.5463690001641393\n",
            "0.5480991919022387\n",
            "0.5500589493290543\n",
            "0.5521145462989807\n",
            "0.5538554949223843\n",
            "0.5557042971596389\n",
            "0.5572843653771579\n",
            "0.5588466889413116\n",
            "0.5607803944126725\n",
            "0.5627331584310897\n",
            "0.5643385945988433\n",
            "0.56599991172171\n",
            "0.5675472876299983\n",
            "0.5690340512548872\n",
            "0.5705570555708902\n",
            "0.5721529016409383\n",
            "0.5740179541470755\n",
            "0.5755279876691911\n",
            "0.5774568442798331\n",
            "0.5791490149619939\n",
            "0.58077155293711\n",
            "0.582375636948344\n",
            "0.5842463942744848\n",
            "0.5861064091972683\n",
            "0.5879194119092449\n",
            "0.5895172182251426\n",
            "0.5909891660561037\n",
            "0.5928168668771339\n",
            "0.5942758372067796\n",
            "0.5957262185223572\n",
            "0.5974859520602409\n",
            "0.599092813404015\n",
            "0.6009311948895759\n",
            "0.6028637716837246\n",
            "0.6048001817730077\n",
            "0.60627095885289\n",
            "0.6083435553419011\n",
            "0.6101460560508396\n",
            "0.6117579322641767\n",
            "0.6134656054894333\n",
            "0.6154395143699158\n",
            "0.617151075921705\n",
            "0.6185525735016064\n",
            "0.6201007750333117\n",
            "0.6220138481510874\n",
            "0.6235780113798273\n",
            "0.625305539354339\n",
            "0.6269455375268941\n",
            "0.6286193793996826\n",
            "0.6302037579019356\n",
            "0.6318107040032096\n",
            "0.6337500598729419\n",
            "0.6354062182214254\n",
            "0.6371240067055158\n",
            "0.6388265053024682\n",
            "0.6405430663272244\n",
            "0.6421203848041231\n",
            "0.6436083937240074\n",
            "0.6450066490246512\n",
            "0.6465223477319684\n",
            "0.6480542276521473\n",
            "0.6499434758330245\n",
            "0.651393148600293\n",
            "0.6528247140557565\n",
            "0.6543483532908018\n",
            "0.6558740267058467\n",
            "0.6576470843971233\n",
            "0.6593210300826051\n",
            "0.6610536749100746\n",
            "0.6629676442317036\n",
            "0.6647069480108179\n",
            "0.6662648669289201\n",
            "0.6677973282611583\n",
            "0.6696035584525379\n",
            "0.6713591085370544\n",
            "0.6727755391384329\n",
            "0.6744650606913944\n",
            "0.6760951955909924\n",
            "0.6780775238181014\n",
            "0.6795931296885166\n",
            "0.6811483849954727\n",
            "0.6827300582700373\n",
            "0.6843920080253231\n",
            "0.6862452923489348\n",
            "0.6880228181019463\n",
            "0.6896478273069767\n",
            "0.6913318227014273\n",
            "0.6930964244601062\n",
            "0.6946983220022352\n",
            "0.6962144114172367\n",
            "0.697894819709651\n",
            "0.6993363847208145\n",
            "0.7011985787955086\n",
            "0.7026638881019924\n",
            "0.704715263813048\n",
            "0.7063675707258532\n",
            "0.7079759470337187\n",
            "0.7096749262126816\n",
            "0.7113989431534886\n",
            "0.7127451145130655\n",
            "0.7142702816697337\n",
            "0.7158269490427374\n",
            "0.717533623013655\n",
            "0.7188001691227983\n",
            "0.7204998330691891\n",
            "0.721890285649263\n",
            "0.7237583997914249\n",
            "0.7254640252693839\n",
            "0.7273694656389144\n",
            "0.7287574915020057\n",
            "0.7303942342853302\n",
            "0.7319370787162001\n",
            "0.7335028000499891\n",
            "0.7354833758090769\n",
            "0.7372243564452052\n",
            "0.7391623956772982\n",
            "0.7406981657533085\n",
            "0.7421947214609522\n",
            "0.7439275903774955\n",
            "0.7455613478980101\n",
            "0.7472377518558746\n",
            "0.748700969359454\n",
            "0.7504121254167289\n",
            "0.7520825376595988\n",
            "0.753883413036766\n",
            "0.7556719943080716\n",
            "0.7575934875346816\n",
            "0.7593112854701479\n",
            "0.7610923838432487\n",
            "0.7627106566563286\n",
            "0.7643663947234678\n",
            "0.7664122704959586\n",
            "0.7681566737496944\n",
            "0.770143928582711\n",
            "0.7720002589933098\n",
            "0.7736475542378243\n",
            "0.7755525260020399\n",
            "0.7774346223877519\n",
            "0.7793104535783343\n",
            "0.7809691185231709\n",
            "0.782602357285102\n",
            "0.784166059378163\n",
            "0.7860514235008708\n",
            "0.7875381945953954\n",
            "0.7894430380038289\n",
            "0.7909213774039617\n",
            "0.7927617438309028\n",
            "0.7945660903020892\n",
            "0.7961423078461376\n",
            "0.7978446085739623\n",
            "0.7995088647698503\n",
            "0.8009855635940571\n",
            "0.8025067855634958\n",
            "0.8042231654876943\n",
            "0.8057834922200273\n",
            "0.8073187505497652\n",
            "0.8090469076505402\n",
            "0.8105391445367233\n",
            "0.8120901035835676\n",
            "0.8137731546026361\n",
            "0.8154894884894875\n",
            "0.8169595743993969\n",
            "0.818590140403689\n",
            "0.820230180650111\n",
            "0.8222258589456758\n",
            "0.8236004640074337\n",
            "0.8253142568461426\n",
            "0.8269187086988288\n",
            "0.8286732459617088\n",
            "0.8304677323612106\n",
            "0.8321865879361282\n",
            "0.8337934492799022\n",
            "0.8352837260726773\n",
            "0.8371708923593506\n",
            "0.8387687255049605\n",
            "0.8406098699935561\n",
            "0.8422447028367416\n",
            "0.8439034153433407\n",
            "0.8459702407002754\n",
            "0.8479763226740805\n",
            "0.8496079704035884\n",
            "0.8513194816496671\n",
            "0.8529168074698095\n",
            "0.8546159986949637\n",
            "0.856210462425066\n",
            "0.8578827416195589\n",
            "0.8593341002378927\n",
            "0.8610907392123776\n",
            "0.8627912438738986\n",
            "0.8644286429180819\n",
            "0.8661535117022522\n",
            "0.8679442602350279\n",
            "0.8695926905592994\n",
            "0.871387386565928\n",
            "0.872988510924532\n",
            "0.8746729069353675\n",
            "0.8765160671585356\n",
            "0.8786462455454385\n",
            "0.8806928484641072\n",
            "0.8824996751592592\n",
            "0.8842509382825983\n",
            "0.8858256536676451\n",
            "0.8874608796575795\n",
            "0.8893719993893753\n",
            "0.8911282994863018\n",
            "0.8928226448995683\n",
            "0.8945752857896068\n",
            "0.8965548081776066\n",
            "0.8982308119764109\n",
            "0.8999386347468247\n",
            "0.9014691431504076\n",
            "0.9031138327115636\n",
            "0.9047279382300804\n",
            "0.9063456381678276\n",
            "0.9080077796945791\n",
            "0.909573627554852\n",
            "0.9113171771359261\n",
            "0.9130435477742149\n",
            "0.9145207813633677\n",
            "0.9161677159311826\n",
            "0.9175563422615266\n",
            "0.9190883692877981\n",
            "0.9205942317043119\n",
            "0.9220917022136776\n",
            "0.9235929772067253\n",
            "0.9256989759252504\n",
            "0.9275653004036535\n",
            "0.9293589593504396\n",
            "0.9311313737383888\n",
            "0.9328856462103021\n",
            "0.934817688239505\n",
            "0.9364516759467552\n",
            "0.9379925358935696\n",
            "0.9398727020644166\n",
            "0.941395606073882\n",
            "0.9430680599663873\n",
            "0.9447275613579909\n",
            "0.9466036847790184\n",
            "0.9482110003986017\n",
            "0.9500788316092528\n",
            "0.9519003022967092\n",
            "0.9535120137207344\n",
            "0.9553545798791949\n",
            "0.9567849235156612\n",
            "0.9585583723719467\n",
            "0.960308889141473\n",
            "0.9621073753022782\n",
            "0.9641024366669033\n",
            "0.9660352232206203\n",
            "0.9675971513514019\n",
            "0.9692264298343902\n",
            "0.9712933495526423\n",
            "0.972902453311569\n",
            "0.9741749015000775\n",
            "0.9756012234236578\n",
            "0.9773907967845498\n",
            "0.9788436205185893\n",
            "0.9804290836424474\n",
            "0.9823980197272337\n",
            "0.9840259143458608\n",
            "0.9856660099285642\n",
            "0.9874226149085843\n",
            "0.9888842654655047\n",
            "0.9904026884557037\n",
            "0.9920647682436287\n",
            "0.9937437525795548\n",
            "0.9955694733373345\n",
            "0.9975004558977874\n",
            "0.9992096950026119\n",
            "1.0006727719550852\n",
            "1.0021540739042374\n",
            "1.0040777872895341\n",
            "1.0058377209831686\n",
            "1.0074665264400375\n",
            "1.0090959933407777\n",
            "1.010879438399049\n",
            "1.0124091441978884\n",
            "1.0140400502992712\n",
            "1.0157513389806918\n",
            "1.0175140449762954\n",
            "1.0194742496666092\n",
            "1.0213152261646203\n",
            "1.023167108330885\n",
            "1.0248205111459698\n",
            "1.0267554681624294\n",
            "1.0284693218253154\n",
            "1.030105188069746\n",
            "1.0317669472730984\n",
            "1.0336752636048494\n",
            "1.035345092149037\n",
            "1.0370594269174445\n",
            "1.038572584271736\n",
            "1.0403433696693167\n",
            "1.0422440166668514\n",
            "1.0437807204473355\n",
            "1.045867863061178\n",
            "1.0475045114831851\n",
            "1.049127824776008\n",
            "1.0505271085997676\n",
            "1.0523652865758637\n",
            "1.0540823677311772\n",
            "1.0557779509698033\n",
            "1.0576518510308717\n",
            "1.0593095586427947\n",
            "1.0608423925421733\n",
            "1.0624235357774798\n",
            "1.0643229034855544\n",
            "1.0659169990693211\n",
            "1.0677188316269604\n",
            "1.0695244107404938\n",
            "1.0713716245368314\n",
            "1.0731438773367412\n",
            "1.0748692382022242\n",
            "1.0764763134214885\n",
            "1.0784273545455445\n",
            "1.0797986321132202\n",
            "1.081395887505368\n",
            "1.0826472851931286\n",
            "1.0843464099537685\n",
            "1.0859075650534666\n",
            "1.0873085795461064\n",
            "1.0891526797238518\n",
            "1.0911481220399022\n",
            "1.0929287332098196\n",
            "1.0947370131302367\n",
            "1.096413534620534\n",
            "1.097940154697584\n",
            "1.0995555820367526\n",
            "1.1009912621944458\n",
            "1.102527703165703\n",
            "1.104125030510261\n",
            "1.1056967174915402\n",
            "1.107053628662968\n",
            "1.1087076988671443\n",
            "1.1104840267344813\n",
            "1.1120824714755768\n",
            "1.1137830868096608\n",
            "1.1155469909958218\n",
            "1.1171357098137935\n",
            "1.11897633203765\n",
            "1.1207904312616723\n",
            "1.1226770199473253\n",
            "1.1241190415209212\n",
            "1.125594864568442\n",
            "1.1272794358870561\n",
            "1.1286521061607029\n",
            "1.130306604878067\n",
            "1.131950532079048\n",
            "1.1337626065744464\n",
            "1.135746665013111\n",
            "1.1373769206464137\n",
            "1.1392018679157851\n",
            "1.1405636035572841\n",
            "1.1421545161615552\n",
            "1.1440988529063856\n",
            "1.1458888114870662\n",
            "1.1477506947334466\n",
            "1.1499234413551858\n",
            "1.1516686569699242\n",
            "1.1535330637336692\n",
            "1.1553555603527352\n",
            "1.1572877781470414\n",
            "1.1588911188532933\n",
            "1.1605158664686295\n",
            "1.1622949172468746\n",
            "1.1638390034665842\n",
            "1.1656331203477768\n",
            "1.1672045719593078\n",
            "1.1689176131087495\n",
            "1.170780267861798\n",
            "1.1724167324392998\n",
            "1.173952699927113\n",
            "1.1757593104601516\n",
            "1.1771023371030607\n",
            "1.1789287189998285\n",
            "1.180475622949088\n",
            "1.182138571196505\n",
            "1.1838984692188175\n",
            "1.1855929327742827\n",
            "1.187282851589915\n",
            "1.1890536199140427\n",
            "1.1907773426426647\n",
            "1.1926281909503595\n",
            "1.1947332455983857\n",
            "1.1965134165171163\n",
            "1.1980110586756636\n",
            "1.1999716484333243\n",
            "1.2016766193272816\n",
            "1.2033614232716963\n",
            "1.2049669255990811\n",
            "1.2068229242968742\n",
            "1.2082593166614737\n",
            "1.2097393066986748\n",
            "1.2115572883040093\n",
            "1.2133232416094417\n",
            "1.2149177013760637\n",
            "1.2164816299972632\n",
            "1.2180929170240222\n",
            "1.219955359730879\n",
            "1.221466245401241\n",
            "1.223016450167312\n",
            "1.2246412715643569\n",
            "1.2264556278048269\n",
            "1.2281506317655753\n",
            "1.2299248960316944\n",
            "1.231513443048043\n",
            "1.2331092339342513\n",
            "1.2348359216509572\n",
            "1.2363936323339066\n",
            "1.238037484686088\n",
            "1.2397472401104315\n",
            "1.2415622683132397\n",
            "1.2434065970008636\n",
            "1.2447891230778316\n",
            "1.246618975306411\n",
            "1.248431987927088\n",
            "1.2503027060757512\n",
            "1.2519319703816758\n",
            "1.2536438118161448\n",
            "1.2552730488350323\n",
            "1.25692657710951\n",
            "1.2585985517257925\n",
            "1.2604470257563969\n",
            "1.2621171125365644\n",
            "1.2638959017251155\n",
            "1.2654813130188476\n",
            "1.2672975450525503\n",
            "1.2689716849485626\n",
            "1.2705456728825484\n",
            "1.2720712621498595\n",
            "1.2737023740473306\n",
            "1.275473664026431\n",
            "1.2772908777836949\n",
            "1.279016308924731\n",
            "1.281029922578036\n",
            "1.2828197802424126\n",
            "1.2844258415729493\n",
            "1.2860699860031342\n",
            "1.2880299385551297\n",
            "1.2898083095965178\n",
            "1.2916990812782132\n",
            "1.2931714379574026\n",
            "1.2949614706246748\n",
            "1.2965461764189288\n",
            "1.2982234854222563\n",
            "1.300164019665145\n",
            "1.3019691036485346\n",
            "1.3039377278379163\n",
            "1.3054875297009791\n",
            "1.3071428820910052\n",
            "1.308960037920481\n",
            "1.3107695416416354\n",
            "1.3125503951936122\n",
            "1.3139841085504693\n",
            "1.315593314445232\n",
            "1.3174878061579927\n",
            "1.3192588911032128\n",
            "1.3209074488686174\n",
            "1.3225655101449287\n",
            "1.3242435540689532\n",
            "0.0014788622746382224\n",
            "0.003158428479948312\n",
            "0.00480352948083902\n",
            "0.006492081200680159\n",
            "0.008241010901263304\n",
            "0.010001906805940906\n",
            "0.011480568772386712\n",
            "0.013051503607074319\n",
            "0.014628870560385077\n",
            "0.016793719947795428\n",
            "0.018437128999958866\n",
            "0.020226164394632325\n",
            "0.021922601610803238\n",
            "0.02353467737012507\n",
            "0.025349522033310912\n",
            "0.02724327105085563\n",
            "0.02912060577241356\n",
            "0.03091935291314674\n",
            "0.032559238431398824\n",
            "0.034262560212703616\n",
            "0.035790848914924484\n",
            "0.0370944413687567\n",
            "0.03880090466545671\n",
            "0.040676549115144386\n",
            "0.04247681716518939\n",
            "0.04423615953806416\n",
            "0.046164463395657745\n",
            "0.04799875197813029\n",
            "0.04975195095667144\n",
            "0.05134889841689478\n",
            "0.0529415494645648\n",
            "0.05443563180811265\n",
            "0.056029584401708736\n",
            "0.058032111133760805\n",
            "0.059842523406533635\n",
            "0.061832085289918556\n",
            "0.06328364894213274\n",
            "0.0653502485331367\n",
            "0.06718480480296532\n",
            "0.06929667389301387\n",
            "0.0709654600419047\n",
            "0.07243268721548797\n",
            "0.07382507565076393\n",
            "0.07531664362343986\n",
            "0.07691785853232264\n",
            "0.07866261377358985\n",
            "0.08078523913917639\n",
            "0.08267473076920376\n",
            "0.084377341410693\n",
            "0.0858481099538486\n",
            "0.08753396223878007\n",
            "0.08925106183952078\n",
            "0.09101157755498081\n",
            "0.09258544414549533\n",
            "0.09438815674818385\n",
            "0.09605312499853656\n",
            "0.09775033448358326\n",
            "0.09967642062155486\n",
            "0.1011961322001484\n",
            "0.10302178405434884\n",
            "0.10491522453020295\n",
            "0.10665240296927254\n",
            "0.10829227552999315\n",
            "0.11007412543991947\n",
            "0.11191234945336266\n",
            "0.11341872041487633\n",
            "0.11502612826159543\n",
            "0.11654091170986595\n",
            "0.11829845390051527\n",
            "0.11984827680051174\n",
            "0.12141990036610752\n",
            "0.12311921064810985\n",
            "0.12469627881598899\n",
            "0.12624436784583284\n",
            "0.12827707800413946\n",
            "0.12985411690324164\n",
            "0.13176236570338765\n",
            "0.13341899433404283\n",
            "0.1352188594810798\n",
            "0.13702283719616473\n",
            "0.1386505013231731\n",
            "0.1403643354735411\n",
            "0.14229840817658798\n",
            "0.14422344033370543\n",
            "0.14584493972456364\n",
            "0.14739067383739343\n",
            "0.1487376511554279\n",
            "0.15030854254427467\n",
            "0.15199925801943026\n",
            "0.15357360159954453\n",
            "0.15519343358476448\n",
            "0.15722785749093957\n",
            "0.15949844475597372\n",
            "0.1612121761607392\n",
            "0.16319760687820747\n",
            "0.16471080249532713\n",
            "0.16663096871827265\n",
            "0.1682887257212568\n",
            "0.16998771450403707\n",
            "0.1715080555137771\n",
            "0.17333173355483034\n",
            "0.17483729291754915\n",
            "0.17673192411432487\n",
            "0.1782860644638081\n",
            "0.18041115480920542\n",
            "0.18200285263988367\n",
            "0.18368798158967586\n",
            "0.1853396985536951\n",
            "0.18726057241030056\n",
            "0.18883730124329667\n",
            "0.19030013947230776\n",
            "0.1921951008574737\n",
            "0.19391404240942367\n",
            "0.19573513412719493\n",
            "0.19728901837487964\n",
            "0.19892022753005748\n",
            "0.20074253283498233\n",
            "0.2023315990672392\n",
            "0.20404071316999547\n",
            "0.20566178084639333\n",
            "0.2075370280334102\n",
            "0.20955020966737167\n",
            "0.21125823305086103\n",
            "0.2129257761913797\n",
            "0.21465541487154755\n",
            "0.21601855388992583\n",
            "0.21751789257044682\n",
            "0.21914791588283256\n",
            "0.2208170276468672\n",
            "0.22231583781254566\n",
            "0.22387601362774745\n",
            "0.22616953419907318\n",
            "0.22796051383323376\n",
            "0.22950031431129828\n",
            "0.23121773937474127\n",
            "0.23299617352692978\n",
            "0.23450579042629818\n",
            "0.23602896684880756\n",
            "0.2378665690531816\n",
            "0.23950221410492803\n",
            "0.2410416580222147\n",
            "0.24275585239195763\n",
            "0.24467239690863568\n",
            "0.24625889114711597\n",
            "0.24786860589176188\n",
            "0.24948780951292618\n",
            "0.2511243878118217\n",
            "0.25281064101802114\n",
            "0.254681384167098\n",
            "0.25624803219304976\n",
            "0.25810186332448976\n",
            "0.259688308476792\n",
            "0.26124904832571666\n",
            "0.2629764528225755\n",
            "0.26451967896707834\n",
            "0.26597269081398656\n",
            "0.26746832409783095\n",
            "0.26896674416559124\n",
            "0.2708992160799558\n",
            "0.2727159092493374\n",
            "0.2741762334123597\n",
            "0.27592773099079765\n",
            "0.2777027916115568\n",
            "0.27903438712020057\n",
            "0.28091253877600747\n",
            "0.28277282870334125\n",
            "0.2843292520174285\n",
            "0.286276051303005\n",
            "0.28786548339497403\n",
            "0.28935641111315363\n",
            "0.29098555453293157\n",
            "0.2929438137642258\n",
            "0.2944550082811614\n",
            "0.2962254788869482\n",
            "0.2979395737123611\n",
            "0.2994932177121682\n",
            "0.3011453371218708\n",
            "0.3029150109156928\n",
            "0.3045262326974698\n",
            "0.3064138569185496\n",
            "0.3081374602854404\n",
            "0.3098753347726124\n",
            "0.3115162466797987\n",
            "0.3132163911219448\n",
            "0.3150490110792467\n",
            "0.31673128373177767\n",
            "0.318283244014701\n",
            "0.3198138150717596\n",
            "0.32153703809699136\n",
            "0.3232288099920658\n",
            "0.32479753198526096\n",
            "0.3264044872330278\n",
            "0.3281542882894921\n",
            "0.32964418428328335\n",
            "0.3313522552285353\n",
            "0.33294307270928114\n",
            "0.3348702353894558\n",
            "0.3363978484707415\n",
            "0.3379574110136008\n",
            "0.3395775116008261\n",
            "0.3412099910514129\n",
            "0.3426838468407731\n",
            "0.3444203310610388\n",
            "0.346102865608147\n",
            "0.3479704004724312\n",
            "0.349629878235595\n",
            "0.35117864425834794\n",
            "0.35286347518491623\n",
            "0.3544625769490781\n",
            "0.35611032616451876\n",
            "0.3578108128379373\n",
            "0.3596364598140082\n",
            "0.36109979622199406\n",
            "0.3625036096938736\n",
            "0.36390261546425196\n",
            "0.36535789259254475\n",
            "0.3668956567564279\n",
            "0.3683808221841407\n",
            "0.3701183667878056\n",
            "0.37169495461237095\n",
            "0.37339788888726394\n",
            "0.37481662425238765\n",
            "0.3762693446310585\n",
            "0.3777879337825434\n",
            "0.3794076916811716\n",
            "0.3810083453002793\n",
            "0.38255671604210156\n",
            "0.38457802418247816\n",
            "0.38633116294660835\n",
            "0.3879441766787673\n",
            "0.38947058059370426\n",
            "0.3912033059103105\n",
            "0.392966820455878\n",
            "0.39488666197832895\n",
            "0.3966423766997159\n",
            "0.3985846073121366\n",
            "0.40043632026828463\n",
            "0.4023118188314121\n",
            "0.4039196347641518\n",
            "0.40558577696685594\n",
            "0.40746592332030196\n",
            "0.4091997370695519\n",
            "0.4107554320179288\n",
            "0.41266675876534503\n",
            "0.4144740662611354\n",
            "0.4162122023380016\n",
            "0.41815430749102933\n",
            "0.41977956715752096\n",
            "0.42141384267441145\n",
            "0.4230925398104636\n",
            "0.4250297749133976\n",
            "0.4267162973313685\n",
            "0.42822804597332653\n",
            "0.43014473256552616\n",
            "0.43181856925530204\n",
            "0.43352085108037497\n",
            "0.43514855179335454\n",
            "0.4371373078707234\n",
            "0.43859229901867447\n",
            "0.44037254143249044\n",
            "0.44226120972572386\n",
            "0.44434541128480526\n",
            "0.446110557259806\n",
            "0.4475255253369851\n",
            "0.4488477839533325\n",
            "0.4504200666761764\n",
            "0.4519711054499497\n",
            "0.45349132420156923\n",
            "0.4552197050865349\n",
            "0.45680264194908043\n",
            "0.45851361294231757\n",
            "0.4604520239793431\n",
            "0.46196916585078324\n",
            "0.4637031550602535\n",
            "0.4657306526323109\n",
            "0.46743555950081866\n",
            "0.46895705876143085\n",
            "0.47058452989744104\n",
            "0.47261060000685473\n",
            "0.47436980487745434\n",
            "0.47606127188943537\n",
            "0.47773629122073086\n",
            "0.4793058229834222\n",
            "0.48084739285051975\n",
            "0.482374247687552\n",
            "0.4839754089370103\n",
            "0.48547712997402376\n",
            "0.48708517304466814\n",
            "0.48873246676476717\n",
            "0.49031530987576144\n",
            "0.4923392890969201\n",
            "0.4941063741283953\n",
            "0.4960257455211161\n",
            "0.4979189125168354\n",
            "0.4997221226887325\n",
            "0.5013170667621486\n",
            "0.5030773446687957\n",
            "0.5047220055709409\n",
            "0.5062958061542657\n",
            "0.5078395082212775\n",
            "0.5095919209063206\n",
            "0.5112230546029327\n",
            "0.5127927278313795\n",
            "0.5143662082874562\n",
            "0.5159912804508453\n",
            "0.5176329759075818\n",
            "0.5191330714603825\n",
            "0.5208620881790396\n",
            "0.5225824855477609\n",
            "0.5244190263016449\n",
            "0.5260561561340567\n",
            "0.5276900349980425\n",
            "0.5294679570990755\n",
            "0.5310337851419473\n",
            "0.5331847097562707\n",
            "0.5347182070812606\n",
            "0.5363553230414915\n",
            "0.5382541474478933\n",
            "0.5399380109804061\n",
            "0.5415481717690177\n",
            "0.5431028356027725\n",
            "0.5445470701703026\n",
            "0.5461513961062712\n",
            "0.5475386208890344\n",
            "0.5492229832102881\n",
            "0.5510869826502203\n",
            "0.5526972338366691\n",
            "0.5544056046344435\n",
            "0.5557705650244222\n",
            "0.5573214876377369\n",
            "0.5589119598383794\n",
            "0.5607034971037179\n",
            "0.562304057428599\n",
            "0.5637553674180794\n",
            "0.5655631345251332\n",
            "0.5672312720352427\n",
            "0.5689562557603393\n",
            "0.5704509763766432\n",
            "0.5721229546515229\n",
            "0.5740807916197326\n",
            "0.5759699609883301\n",
            "0.5776128793311546\n",
            "0.5792340293259877\n",
            "0.5810496613497624\n",
            "0.5827154665042067\n",
            "0.5842212847126719\n",
            "0.5858243162674672\n",
            "0.5874983180514382\n",
            "0.5889960397844729\n",
            "0.590837907913091\n",
            "0.5925682853249943\n",
            "0.5942411069065103\n",
            "0.5957060454751525\n",
            "0.5974603031602357\n",
            "0.5994021953524226\n",
            "0.600870471476289\n",
            "0.6026602943839929\n",
            "0.604263103679013\n",
            "0.6059106829221291\n",
            "0.6072415157657145\n",
            "0.60879990313669\n",
            "0.6104852072418193\n",
            "0.6119250283216882\n",
            "0.6138708687499356\n",
            "0.6155408041556473\n",
            "0.6176129605459131\n",
            "0.6193373960912075\n",
            "0.6209328447461433\n",
            "0.6226890189263522\n",
            "0.6245808002284116\n",
            "0.6261161969750738\n",
            "0.6276581781294645\n",
            "0.6293650056090196\n",
            "0.6307674852173651\n",
            "0.6321904569330727\n",
            "0.6337278592007239\n",
            "0.6351532163217549\n",
            "0.6370602776022518\n",
            "0.6387440591212123\n",
            "0.6402852855375051\n",
            "0.641756358658871\n",
            "0.6434429402241622\n",
            "0.6449082746835011\n",
            "0.6465841624742884\n",
            "0.6485042498849541\n",
            "0.6500319274490142\n",
            "0.6517503328640443\n",
            "0.6537604760330962\n",
            "0.6552675616405809\n",
            "0.6569047801939728\n",
            "0.6585440274394686\n",
            "0.6603787070345086\n",
            "0.662250436785276\n",
            "0.6637453691428884\n",
            "0.6652606022937219\n",
            "0.6671356500871956\n",
            "0.6691491861477532\n",
            "0.6711144918371039\n",
            "0.6727242187770737\n",
            "0.6744391326709172\n",
            "0.6763450181697641\n",
            "0.6781772149493323\n",
            "0.6802075854347794\n",
            "0.6821513977806891\n",
            "0.683680764397087\n",
            "0.6855840614384703\n",
            "0.6873713608288095\n",
            "0.6889227111931042\n",
            "0.6905513291468706\n",
            "0.6925562025640931\n",
            "0.6939394486224865\n",
            "0.6956783017843885\n",
            "0.6975106595422301\n",
            "0.6994913584740875\n",
            "0.7012271806407158\n",
            "0.7033825327673227\n",
            "0.7052885114079546\n",
            "0.7069059227738539\n",
            "0.7086071292762561\n",
            "0.7101840568930292\n",
            "0.7120219935541567\n",
            "0.7140264460802688\n",
            "0.7156871116679647\n",
            "0.7180372292123487\n",
            "0.7199508201740586\n",
            "0.7215846142805445\n",
            "0.7235427526256922\n",
            "0.7253026462272\n",
            "0.7272470631562841\n",
            "0.7287565981945419\n",
            "0.7304053611462683\n",
            "0.7319970798614385\n",
            "0.7335974854581496\n",
            "0.7354600284715442\n",
            "0.7370906527085073\n",
            "0.7387117581904087\n",
            "0.7403472399772586\n",
            "0.7422351474347322\n",
            "0.7439536603210527\n",
            "0.7458555739553993\n",
            "0.7476715512592774\n",
            "0.7494228331329268\n",
            "0.7512556248918518\n",
            "0.7528085687276348\n",
            "0.754167415601823\n",
            "0.7557655505816955\n",
            "0.7572736191322736\n",
            "0.7589764578263168\n",
            "0.7609596389638799\n",
            "0.7629120060245095\n",
            "0.7646935949545077\n",
            "0.7663034336341311\n",
            "0.7682443974572984\n",
            "0.7697238778824087\n",
            "0.7714849179967895\n",
            "0.7730630791705587\n",
            "0.7744156841731742\n",
            "0.7760023354264476\n",
            "0.7775020959127285\n",
            "0.7790956451459918\n",
            "0.7807855703641692\n",
            "0.7825365786052421\n",
            "0.7843745273092518\n",
            "0.7858170339518495\n",
            "0.7875717367662494\n",
            "0.7893431566255477\n",
            "0.7909189552602256\n",
            "0.7928059689529107\n",
            "0.7940760248000055\n",
            "0.7958923673538296\n",
            "0.7974703574881834\n",
            "0.7993242209372313\n",
            "0.8009314760375206\n",
            "0.8023731922706985\n",
            "0.8040863971423615\n",
            "0.8058675711264696\n",
            "0.8075330532572763\n",
            "0.8092688926497994\n",
            "0.811388974375737\n",
            "0.8130286428172265\n",
            "0.8148519185650379\n",
            "0.816657530758387\n",
            "0.8181771534635588\n",
            "0.8198751850658671\n",
            "0.8218745586969664\n",
            "0.8236558509757147\n",
            "0.8253065220382817\n",
            "0.8270350099372132\n",
            "0.8284471535774143\n",
            "0.8298644297720527\n",
            "0.8317403952636377\n",
            "0.8336985066266316\n",
            "0.8355579020269691\n",
            "0.8373247085477385\n",
            "0.8387556678956122\n",
            "0.8403240286023416\n",
            "0.8420053599283214\n",
            "0.8438578256408272\n",
            "0.8454024995226994\n",
            "0.8469937305773616\n",
            "0.8484693049927197\n",
            "0.850155916741437\n",
            "0.8516869466475514\n",
            "0.8533757072123115\n",
            "0.85516683349524\n",
            "0.8569149820853377\n",
            "0.8586772455431312\n",
            "0.8601237703924594\n",
            "0.8614184812968954\n",
            "0.8632116097470989\n",
            "0.8649062307746819\n",
            "0.8669695068350838\n",
            "0.8687287450903822\n",
            "0.8702572600158585\n",
            "0.8718237499599262\n",
            "0.8737504050097502\n",
            "0.875453338674877\n",
            "0.8769865247142284\n",
            "0.878696270992079\n",
            "0.8802529872988191\n",
            "0.8814401288928888\n",
            "0.883292908939864\n",
            "0.8847564249239919\n",
            "0.8865755367309541\n",
            "0.8882664871185332\n",
            "0.8901275505342752\n",
            "0.8919086533281809\n",
            "0.8938182003205389\n",
            "0.8956938682462249\n",
            "0.8973984583411985\n",
            "0.899024022433459\n",
            "0.9006213106005393\n",
            "0.9020633873579752\n",
            "0.9037496837051323\n",
            "0.9055777888011445\n",
            "0.9075127974952883\n",
            "0.9091213427083876\n",
            "0.9109558922707882\n",
            "0.9125648576127904\n",
            "0.9141812157600432\n",
            "0.9159896834884458\n",
            "0.917620054062675\n",
            "0.9191709629562504\n",
            "0.9207736519748903\n",
            "0.9225740326792383\n",
            "0.9243125952875523\n",
            "0.9257923952301444\n",
            "0.9276461173658785\n",
            "0.9292083054094973\n",
            "0.9311215074928215\n",
            "0.9324705304239717\n",
            "0.934023024099867\n",
            "0.9360430273406036\n",
            "0.9376901055083555\n",
            "0.9392050278308751\n",
            "0.940583220451994\n",
            "0.9420993318185782\n",
            "0.9438938371208317\n",
            "0.9455433691401616\n",
            "0.9470392858128414\n",
            "0.9487273192314236\n",
            "0.9502309090493585\n",
            "0.9518179060400599\n",
            "0.9533123954025375\n",
            "0.9550335683938488\n",
            "0.9566133680093624\n",
            "0.9584354745305103\n",
            "0.9598222285737772\n",
            "0.9614693796085885\n",
            "0.9632174766734433\n",
            "0.9650572690055194\n",
            "0.9668986920048209\n",
            "0.9686651599529149\n",
            "0.9703522799417491\n",
            "0.9717603693990147\n",
            "0.973474656148335\n",
            "0.9748646145129143\n",
            "0.9768337370337122\n",
            "0.9784457571518695\n",
            "0.9800020415917077\n",
            "0.9817071374877334\n",
            "0.9835138640287892\n",
            "0.9853900240357879\n",
            "0.9870052484752577\n",
            "0.9884770935606164\n",
            "0.9901062564929123\n",
            "0.991806230200526\n",
            "0.9933824821963639\n",
            "0.9951560322738364\n",
            "0.9969327726479992\n",
            "0.9990701372818569\n",
            "1.0005518820737025\n",
            "1.0023053552945862\n",
            "1.0041566380606892\n",
            "1.005800757642902\n",
            "1.007486164493634\n",
            "1.0091718701297974\n",
            "1.0109748217608312\n",
            "1.0126423731331935\n",
            "1.0145916768809413\n",
            "1.0165884157123468\n",
            "1.0180968171952631\n",
            "1.0200739502144591\n",
            "1.0220592528810282\n",
            "1.0236627207235303\n",
            "1.0252460054576855\n",
            "1.0268266584409778\n",
            "1.0282916206380595\n",
            "1.030066322258976\n",
            "1.0314341156226594\n",
            "1.0331114345346875\n",
            "1.034726904252606\n",
            "1.0364090333051998\n",
            "1.0380553157280779\n",
            "1.0397389600496463\n",
            "1.0410745310813874\n",
            "1.0427477993166354\n",
            "1.0443703926280332\n",
            "1.0458923301580922\n",
            "1.0475189477738822\n",
            "1.049319115364948\n",
            "1.0510157413037537\n",
            "1.0527412468362647\n",
            "1.0543940656477837\n",
            "1.0561082909631607\n",
            "1.0579189918077816\n",
            "1.0595879427459844\n",
            "1.0611636313178656\n",
            "1.0632064681681221\n",
            "1.0649027806871079\n",
            "1.0661383151550732\n",
            "1.0681683274028857\n",
            "1.0701409548597263\n",
            "1.0718930968085822\n",
            "1.073466277869461\n",
            "1.0749309935685618\n",
            "1.0765443219400732\n",
            "1.0783554128826123\n",
            "1.080280958462859\n",
            "1.0818648357373064\n",
            "1.083520601243924\n",
            "1.0849921621782395\n",
            "1.0865756905139865\n",
            "1.0885912305711176\n",
            "1.090119834522457\n",
            "1.091731179781887\n",
            "1.0935439366056485\n",
            "1.0955599059381753\n",
            "1.09699218603961\n",
            "1.0984009912099375\n",
            "1.1002236039894622\n",
            "1.1021868969930713\n",
            "1.1040088449750105\n",
            "1.1054666137603848\n",
            "1.1073918886044447\n",
            "1.1094939273489102\n",
            "1.1112203876228284\n",
            "1.1130943765573185\n",
            "1.1148253980347567\n",
            "1.1161273448820919\n",
            "1.1178818318392614\n",
            "1.1195408400824614\n",
            "1.1215357313223202\n",
            "1.123233050107956\n",
            "1.1249685684585815\n",
            "1.1263426401273673\n",
            "1.1279392904790162\n",
            "1.129647504719322\n",
            "1.1313123883646163\n",
            "1.1330833417527817\n",
            "1.1347709449813188\n",
            "1.1364086162861047\n",
            "1.1381009400195783\n",
            "1.1397792848632158\n",
            "1.1414122960299178\n",
            "1.1429827099718401\n",
            "1.1446787005342791\n",
            "1.14638404826374\n",
            "1.1477806590249777\n",
            "1.1497317736258592\n",
            "1.1515168802969902\n",
            "1.153512322765482\n",
            "1.1552183253838277\n",
            "1.157036998311577\n",
            "1.1587208645880376\n",
            "1.1602604990572576\n",
            "1.1616963456811198\n",
            "1.1631769976195168\n",
            "1.1647851766680208\n",
            "1.1663773824339327\n",
            "1.1679982845588108\n",
            "1.169820939869527\n",
            "1.1714726287843016\n",
            "1.1734087321611926\n",
            "1.1748582674261858\n",
            "1.1764829033018682\n",
            "1.1779239815671732\n",
            "1.179366299487136\n",
            "1.1811108611276389\n",
            "1.1827149760845068\n",
            "1.184480697983671\n",
            "1.1859714511562796\n",
            "1.1876210324142291\n",
            "1.1893867024832674\n",
            "1.191348305977214\n",
            "1.1930296211443898\n",
            "1.1948546713118053\n",
            "1.1965784272726845\n",
            "1.1984233716717156\n",
            "1.2003069162521216\n",
            "1.202257188765899\n",
            "1.2040799454502438\n",
            "1.2060318429909094\n",
            "1.208022576082698\n",
            "1.2100255009920702\n",
            "1.2116752532894348\n",
            "1.2133993293775622\n",
            "1.2153132764427252\n",
            "1.2167343819690177\n",
            "1.2187835466678796\n",
            "1.220838929000108\n",
            "1.2224273479655576\n",
            "1.2238507160292866\n",
            "1.2258637095503795\n",
            "1.227507430345506\n",
            "1.2291721981352248\n",
            "1.2312952248794038\n",
            "1.232930460778039\n",
            "1.2348887387596432\n",
            "1.2366933469729655\n",
            "1.2387574948465732\n",
            "1.2401305526266317\n",
            "1.2417498225598689\n",
            "1.243636347220072\n",
            "1.2455058703794504\n",
            "1.2473835453505406\n",
            "1.248916411567527\n",
            "1.250416405746699\n",
            "1.251873558820666\n",
            "1.2535876676707012\n",
            "1.2556275085872397\n",
            "1.2571001966743518\n",
            "1.259059065671833\n",
            "1.2607106274503577\n",
            "1.2623699064297444\n",
            "1.264001615440754\n",
            "1.265644854825476\n",
            "1.267132985241273\n",
            "1.268820568347526\n",
            "1.2707013269062237\n",
            "1.272327947418403\n",
            "1.2741819619370238\n",
            "1.2757206020300345\n",
            "1.277185137543227\n",
            "1.2788988776371608\n",
            "1.2806205462159403\n",
            "1.2822133518393388\n",
            "1.2840059228108058\n",
            "1.2857782983261605\n",
            "1.287787919382915\n",
            "1.2895393431796442\n",
            "1.2916451322910425\n",
            "1.2933197691464973\n",
            "1.2949639105278512\n",
            "1.2966155006605036\n",
            "1.2982565976317277\n",
            "1.299789020243813\n",
            "1.3013112707363674\n",
            "1.3028963097678425\n",
            "1.3043696390241004\n",
            "1.305952788085279\n",
            "1.3076456886575656\n",
            "1.3095262685547704\n",
            "1.311317697891494\n",
            "1.3127496877442235\n",
            "1.3143149676072934\n",
            "1.3158186129139513\n",
            "1.3176874334702406\n",
            "1.3192416402842382\n",
            "1.3205623545152756\n",
            "1.3220445747723055\n",
            "1.3232474683038413\n",
            "0.0017236541299258962\n",
            "0.0032213574175334646\n",
            "0.004780159124632931\n",
            "0.006437794936587439\n",
            "0.008093575382476572\n",
            "0.009773999223928622\n",
            "0.011174948014261778\n",
            "0.0126740633679168\n",
            "0.01431540546514799\n",
            "0.01609946974098225\n",
            "0.01798341493777302\n",
            "0.01948069032195889\n",
            "0.020835926313229534\n",
            "0.02260184303269057\n",
            "0.02462054381285177\n",
            "0.026426120487320452\n",
            "0.027918606767873933\n",
            "0.029432787919593285\n",
            "0.0312775059429276\n",
            "0.033290289247127444\n",
            "0.03495044476540802\n",
            "0.03677423957668607\n",
            "0.03849635313234061\n",
            "0.04048509853880119\n",
            "0.04218985418529462\n",
            "0.04388889815191479\n",
            "0.045827643493252336\n",
            "0.047249602692206495\n",
            "0.04867773211520651\n",
            "0.05017809566024624\n",
            "0.05183779507341897\n",
            "0.05379820052924973\n",
            "0.05547206801221804\n",
            "0.057362037698935976\n",
            "0.059407071689205705\n",
            "0.060947934075084795\n",
            "0.06275716126727326\n",
            "0.06450798688337321\n",
            "0.06617089793505267\n",
            "0.06816059762559583\n",
            "0.06967541186705879\n",
            "0.07130021405646868\n",
            "0.07328775349785299\n",
            "0.0750077365304503\n",
            "0.07660228151189702\n",
            "0.07823297236581593\n",
            "0.08007291439549087\n",
            "0.08184925872651512\n",
            "0.08340491586938843\n",
            "0.08501675671628674\n",
            "0.08663870215111072\n",
            "0.0886657440753849\n",
            "0.09019251400247559\n",
            "0.09176596336047668\n",
            "0.09334961730805809\n",
            "0.09486579773066293\n",
            "0.09684603994764636\n",
            "0.09849297817405837\n",
            "0.10026011777960736\n",
            "0.101910040659063\n",
            "0.10366957678514369\n",
            "0.10516329311653781\n",
            "0.10680505747685347\n",
            "0.10888759224006282\n",
            "0.1109981927115594\n",
            "0.113016002775763\n",
            "0.11471335799492838\n",
            "0.11611069056688977\n",
            "0.11804192861937501\n",
            "0.1195208628464233\n",
            "0.12117901680719517\n",
            "0.12296327072031357\n",
            "0.12439227820662281\n",
            "0.12612390960268963\n",
            "0.1277897325927949\n",
            "0.1295871595897333\n",
            "0.13146590073700146\n",
            "0.1333588918151758\n",
            "0.13503016977358961\n",
            "0.13679221508752964\n",
            "0.1388151798101947\n",
            "0.1401577431832433\n",
            "0.14189618444808608\n",
            "0.14375478059739408\n",
            "0.14533821594379748\n",
            "0.14710906414729555\n",
            "0.14897704261648076\n",
            "0.1507671441873321\n",
            "0.15232698249694943\n",
            "0.15399211645126343\n",
            "0.1556439565880524\n",
            "0.15775152377765198\n",
            "0.15949866198517781\n",
            "0.16124588266358048\n",
            "0.1632410833597793\n",
            "0.1647692123032592\n",
            "0.16635687317689665\n",
            "0.16812667456429328\n",
            "0.1700191490180657\n",
            "0.17159954559467638\n",
            "0.17313826831100543\n",
            "0.17445721025662045\n",
            "0.17579448421288024\n",
            "0.1773262241917193\n",
            "0.17933910048526266\n",
            "0.18116070776034499\n",
            "0.1827294023140617\n",
            "0.1844320137177587\n",
            "0.1864956647843656\n",
            "0.1880444907166464\n",
            "0.18982354210465766\n",
            "0.19138165172713492\n",
            "0.1929895027214304\n",
            "0.1947304159783951\n",
            "0.1964644608290299\n",
            "0.1984843803793573\n",
            "0.20013943124000375\n",
            "0.2016781438951907\n",
            "0.20346630976328153\n",
            "0.2051612493936973\n",
            "0.20687652899481146\n",
            "0.2088918013645865\n",
            "0.21050842415036447\n",
            "0.21217175563582985\n",
            "0.21398819224608828\n",
            "0.2154099139411126\n",
            "0.2172139212298576\n",
            "0.2185872235261571\n",
            "0.22050475540673337\n",
            "0.22220809548102377\n",
            "0.2240542113933417\n",
            "0.22550342897015155\n",
            "0.2268507660502363\n",
            "0.22839686648010293\n",
            "0.22990650731279416\n",
            "0.23160070211381253\n",
            "0.23332574087030747\n",
            "0.23508276140598386\n",
            "0.23704194870141462\n",
            "0.2385700538640132\n",
            "0.24004677433491972\n",
            "0.24182528943356957\n",
            "0.24381160598886592\n",
            "0.245394272267666\n",
            "0.24732627984507918\n",
            "0.24874911192433\n",
            "0.2503527485196243\n",
            "0.25188699555214106\n",
            "0.25384759841977483\n",
            "0.2555547085259577\n",
            "0.2574972452410042\n",
            "0.25967950421526\n",
            "0.26126026832843985\n",
            "0.26279742272613604\n",
            "0.2642731158934591\n",
            "0.2656342730192882\n",
            "0.2672842292834426\n",
            "0.26911965889089245\n",
            "0.2707932393264283\n",
            "0.27238378073553293\n",
            "0.27382112677444886\n",
            "0.27549331953458467\n",
            "0.2771837904934993\n",
            "0.2791476745129851\n",
            "0.2807260603855943\n",
            "0.28219393589307584\n",
            "0.28431489293837486\n",
            "0.28574283775466175\n",
            "0.2875660630443212\n",
            "0.28941479439625656\n",
            "0.2906416259763186\n",
            "0.2922195243408613\n",
            "0.293804808345902\n",
            "0.29544598626358737\n",
            "0.29694086038852896\n",
            "0.29891571227241964\n",
            "0.3006372988376471\n",
            "0.30234337904874015\n",
            "0.30419111389028447\n",
            "0.30567133091294857\n",
            "0.30727004745732184\n",
            "0.30859849779197324\n",
            "0.30994979941936407\n",
            "0.31150457560253875\n",
            "0.31312680259689957\n",
            "0.3149862708642964\n",
            "0.31692884233601565\n",
            "0.31840689758510543\n",
            "0.3201972237023551\n",
            "0.3219112685269407\n",
            "0.323779873530883\n",
            "0.3251867955907836\n",
            "0.3267464837454774\n",
            "0.3287383285934663\n",
            "0.33050291144939337\n",
            "0.3320965094639517\n",
            "0.33349869912847535\n",
            "0.335216804690983\n",
            "0.3368067206324214\n",
            "0.3384935677508869\n",
            "0.34009930925905857\n",
            "0.34175441149250624\n",
            "0.3431757959868292\n",
            "0.34483395574037984\n",
            "0.34683287601031915\n",
            "0.3484352759997863\n",
            "0.35012914274659607\n",
            "0.3522467515657625\n",
            "0.35391145319584993\n",
            "0.355614330610046\n",
            "0.35721816111098775\n",
            "0.3590593784666427\n",
            "0.3607185202486375\n",
            "0.36259877239651694\n",
            "0.36430365258775405\n",
            "0.36580595793321613\n",
            "0.36748225579176413\n",
            "0.36908204384776944\n",
            "0.3711578763659348\n",
            "0.37254767024608526\n",
            "0.37384038523334984\n",
            "0.37551146776170075\n",
            "0.37722919587893866\n",
            "0.3791843712177423\n",
            "0.38096244896159454\n",
            "0.3829116644456868\n",
            "0.38453760019043826\n",
            "0.3865934451827613\n",
            "0.3881479745630718\n",
            "0.38977053159338126\n",
            "0.39130635853008844\n",
            "0.39287817752574716\n",
            "0.39456764252289483\n",
            "0.39612511676900525\n",
            "0.39790985102543747\n",
            "0.3995035658102206\n",
            "0.40121296878970797\n",
            "0.4030220786019055\n",
            "0.4046088251311456\n",
            "0.4062544301037898\n",
            "0.40809362547476885\n",
            "0.40994176611571054\n",
            "0.41148330549449874\n",
            "0.41325184115973274\n",
            "0.414797227705836\n",
            "0.4161897001364042\n",
            "0.4176691409267123\n",
            "0.4192927501085774\n",
            "0.4209793990530321\n",
            "0.4227151387487836\n",
            "0.42435742202012433\n",
            "0.4260659920590003\n",
            "0.427840873560942\n",
            "0.4294854394920037\n",
            "0.4312173150994284\n",
            "0.4327795488755112\n",
            "0.43434511929216896\n",
            "0.4359575846920843\n",
            "0.43749916843136255\n",
            "0.439540480103944\n",
            "0.4411067389466269\n",
            "0.4428242033399889\n",
            "0.4443094166343474\n",
            "0.4459329954803447\n",
            "0.44749802411974543\n",
            "0.44933884512737887\n",
            "0.4509794616028476\n",
            "0.45291322500199616\n",
            "0.4546938087324352\n",
            "0.4564596550239017\n",
            "0.45825347433919494\n",
            "0.4600380087447593\n",
            "0.46178036653782095\n",
            "0.46335203293949134\n",
            "0.4650514073993849\n",
            "0.46659019109233263\n",
            "0.46835141230727095\n",
            "0.47026145610662984\n",
            "0.47172987933658883\n",
            "0.47331931752621975\n",
            "0.4748979335855645\n",
            "0.47654770966381066\n",
            "0.47794870068045225\n",
            "0.4798032691716538\n",
            "0.4812995860033938\n",
            "0.4832339492600287\n",
            "0.48471653232793976\n",
            "0.4864267659614153\n",
            "0.4881881667524957\n",
            "0.48993882346336187\n",
            "0.4918298934731642\n",
            "0.4935787254587159\n",
            "0.49524540029218433\n",
            "0.4970275931955908\n",
            "0.4985033830108545\n",
            "0.5001386821727314\n",
            "0.501647172986394\n",
            "0.5029653511998599\n",
            "0.5046044646016777\n",
            "0.5062384160278398\n",
            "0.5079514157131809\n",
            "0.5097320418223701\n",
            "0.5114525263876561\n",
            "0.5131768936391377\n",
            "0.5147637439810712\n",
            "0.5165772727688255\n",
            "0.5184885973820601\n",
            "0.5203286880422431\n",
            "0.5218437320131171\n",
            "0.5234642269666238\n",
            "0.5253239074326537\n",
            "0.5271718727658167\n",
            "0.5287225653448373\n",
            "0.5302868028126104\n",
            "0.5320279070788332\n",
            "0.5338915641350515\n",
            "0.5358592088874954\n",
            "0.5373446660883286\n",
            "0.5389029232742232\n",
            "0.54053172065169\n",
            "0.5423873781853015\n",
            "0.5441626252420723\n",
            "0.545728438193231\n",
            "0.547643098989716\n",
            "0.549542679048865\n",
            "0.551307757034936\n",
            "0.5532705087186126\n",
            "0.5553720399851689\n",
            "0.5570601109043717\n",
            "0.5589127606138244\n",
            "0.5604647774525615\n",
            "0.5621320272955443\n",
            "0.5640881970105573\n",
            "0.5657775402069092\n",
            "0.5672906446639839\n",
            "0.5691412917488371\n",
            "0.5708837332323079\n",
            "0.572776734828949\n",
            "0.5744425660508978\n",
            "0.5759574630681206\n",
            "0.5777085582008752\n",
            "0.5791665929967486\n",
            "0.5808995686223745\n",
            "0.5823634974182109\n",
            "0.5840667256004061\n",
            "0.5855849144403892\n",
            "0.5872579919712623\n",
            "0.5893054788984606\n",
            "0.5910374662455391\n",
            "0.5927842341725479\n",
            "0.5942560028846916\n",
            "0.5959581786104481\n",
            "0.5976092516613738\n",
            "0.5997839542605993\n",
            "0.6014866322812522\n",
            "0.6032557803041795\n",
            "0.604739421621308\n",
            "0.6061209158214462\n",
            "0.6080812397210494\n",
            "0.6098842907439718\n",
            "0.6114698384728883\n",
            "0.6131579706735928\n",
            "0.6149836467659991\n",
            "0.6164442896842957\n",
            "0.6178618372248872\n",
            "0.6199377087680885\n",
            "0.6218540208114077\n",
            "0.6233655237175925\n",
            "0.6250881174641192\n",
            "0.6267570348651818\n",
            "0.6283562779426575\n",
            "0.6297687909487263\n",
            "0.6314436195756469\n",
            "0.6328286598710453\n",
            "0.6345232930939521\n",
            "0.6364339734892102\n",
            "0.6383141927097155\n",
            "0.6397780759255295\n",
            "0.6415942445435487\n",
            "0.6432559116722067\n",
            "0.6448552026163281\n",
            "0.6467012996258943\n",
            "0.6485497472841112\n",
            "0.6500755432621598\n",
            "0.6515621571894497\n",
            "0.6530486589197613\n",
            "0.654783147070414\n",
            "0.6564667429155706\n",
            "0.6579221067831035\n",
            "0.6594563635718792\n",
            "0.6612182072056528\n",
            "0.6629619240151037\n",
            "0.6647702359482456\n",
            "0.6665270078517592\n",
            "0.6682749079621356\n",
            "0.6703593157746298\n",
            "0.6722468735311952\n",
            "0.6738682762741128\n",
            "0.6753085671788286\n",
            "0.6767131306631181\n",
            "0.6785674143935103\n",
            "0.6801117143362684\n",
            "0.6818669135003443\n",
            "0.6832660651572829\n",
            "0.6850041551968021\n",
            "0.6867292980708735\n",
            "0.6885371726492177\n",
            "0.6903396719861823\n",
            "0.6920862902155922\n",
            "0.6938953837165442\n",
            "0.6956436230093622\n",
            "0.6974951792556001\n",
            "0.6993828348796386\n",
            "0.7011236008780691\n",
            "0.7028106165968854\n",
            "0.704336990328396\n",
            "0.7061684874012647\n",
            "0.7076452423239608\n",
            "0.7093683932443409\n",
            "0.7110517529575416\n",
            "0.7127489000939957\n",
            "0.7144204615936864\n",
            "0.7161410001232801\n",
            "0.7175864578817811\n",
            "0.7190771380348888\n",
            "0.7206201632614331\n",
            "0.7222569311976128\n",
            "0.7237532843104408\n",
            "0.7252710237527442\n",
            "0.7267643371811303\n",
            "0.7282814202101334\n",
            "0.7300270323253348\n",
            "0.7321872464226334\n",
            "0.7340197015906234\n",
            "0.7357606416773004\n",
            "0.7374691205561313\n",
            "0.7389934729127323\n",
            "0.7407782213462283\n",
            "0.7425049212582581\n",
            "0.7440353316419265\n",
            "0.745859010597629\n",
            "0.7474250895592868\n",
            "0.7490960871776962\n",
            "0.7507126620968284\n",
            "0.7526830599436065\n",
            "0.7542805930842524\n",
            "0.7561229747884414\n",
            "0.7578015406723217\n",
            "0.7594072253197965\n",
            "0.761388759326447\n",
            "0.7632547065120219\n",
            "0.7648968138658178\n",
            "0.7665170851875754\n",
            "0.7682068332686753\n",
            "0.7698860461144801\n",
            "0.7718203329979001\n",
            "0.773130070522923\n",
            "0.774575116994131\n",
            "0.7761056824108524\n",
            "0.7778137620452725\n",
            "0.7791800398350982\n",
            "0.7807407472139735\n",
            "0.7820553314655333\n",
            "0.7838280556147056\n",
            "0.7851240237045776\n",
            "0.7865745612727407\n",
            "0.788132421653289\n",
            "0.7898053536024849\n",
            "0.7916982736428986\n",
            "0.7933320775056434\n",
            "0.7951761757016487\n",
            "0.7971233911526477\n",
            "0.7986614850475965\n",
            "0.8003693370867873\n",
            "0.80198923340234\n",
            "0.8036001789600343\n",
            "0.8053342417987717\n",
            "0.8070412660803636\n",
            "0.8088983379666458\n",
            "0.8108729742981894\n",
            "0.8124690574148427\n",
            "0.8141224840108086\n",
            "0.8156250320432131\n",
            "0.8171465703288613\n",
            "0.8189277789172005\n",
            "0.820874312802044\n",
            "0.8224892439439778\n",
            "0.8242900220634383\n",
            "0.8258264195888548\n",
            "0.8274280660597565\n",
            "0.8291508412117239\n",
            "0.8308745630256965\n",
            "0.8325683425759416\n",
            "0.834144343500552\n",
            "0.8358181111343072\n",
            "0.8376622260988825\n",
            "0.839561601886359\n",
            "0.8413145013172608\n",
            "0.8432945554213755\n",
            "0.8447355550268422\n",
            "0.8462159305887149\n",
            "0.8477063002183919\n",
            "0.8491182961427343\n",
            "0.85069048740065\n",
            "0.8523878520711914\n",
            "0.853884430645067\n",
            "0.8556497996420507\n",
            "0.8572086915945458\n",
            "0.858755754387897\n",
            "0.8606728216266388\n",
            "0.8624866671879273\n",
            "0.8641571565662198\n",
            "0.8661051717255731\n",
            "0.8679319450922329\n",
            "0.8693415745140036\n",
            "0.8713828564604835\n",
            "0.8731181028553897\n",
            "0.8743849378412641\n",
            "0.8759738546808052\n",
            "0.8777715199438813\n",
            "0.8794469376049383\n",
            "0.8810588531481945\n",
            "0.8826348580362852\n",
            "0.8841674550415\n",
            "0.885717710265723\n",
            "0.8874696266010899\n",
            "0.8893891470816434\n",
            "0.8908982467468437\n",
            "0.8924103690230328\n",
            "0.8938254186564394\n",
            "0.8954701586757474\n",
            "0.8974509057791337\n",
            "0.8991041252070375\n",
            "0.900497071425933\n",
            "0.9021354360348733\n",
            "0.9038985495067313\n",
            "0.9056999424229497\n",
            "0.9072201723028022\n",
            "0.9089908464180539\n",
            "0.9106285543088108\n",
            "0.9125688129373829\n",
            "0.9143171982692025\n",
            "0.9160403340978696\n",
            "0.917971575808952\n",
            "0.9196952546344084\n",
            "0.9212567179709139\n",
            "0.9228750209674201\n",
            "0.9246934630986675\n",
            "0.9263159311031137\n",
            "0.9279709627561252\n",
            "0.9295842077421106\n",
            "0.9316123240744062\n",
            "0.9332793271145248\n",
            "0.9351109519334095\n",
            "0.9372326044170448\n",
            "0.9385952076033863\n",
            "0.940216160643741\n",
            "0.9418399298892302\n",
            "0.9434649575396877\n",
            "0.9451817557634905\n",
            "0.947138463444722\n",
            "0.9487447017598944\n",
            "0.9504400371285655\n",
            "0.9525111299341597\n",
            "0.9542479690383462\n",
            "0.9558690574467944\n",
            "0.9573102631532323\n",
            "0.9589371412916257\n",
            "0.9606607373413223\n",
            "0.9624186161229068\n",
            "0.9639067197089914\n",
            "0.9656979203833949\n",
            "0.96771815411575\n",
            "0.9694384222140398\n",
            "0.9710224340943729\n",
            "0.9723604391602909\n",
            "0.9739686840635431\n",
            "0.9755489025884272\n",
            "0.977391580638983\n",
            "0.9791683319889372\n",
            "0.980809898175242\n",
            "0.9827093244208704\n",
            "0.9844070523596176\n",
            "0.9864714889575148\n",
            "0.988209543325712\n",
            "0.9899441860520931\n",
            "0.9917408974884111\n",
            "0.9934082183691547\n",
            "0.9951223900251072\n",
            "0.9968159607304331\n",
            "0.9983829208042311\n",
            "1.0000082944969997\n",
            "1.001727886059705\n",
            "1.0031696945200186\n",
            "1.0044704247313692\n",
            "1.006185083285622\n",
            "1.0077813789057914\n",
            "1.0096719925056028\n",
            "1.0111097675150313\n",
            "1.012774858175946\n",
            "1.0143202695700213\n",
            "1.0158506046475657\n",
            "1.0170586899570797\n",
            "1.0187157972541916\n",
            "1.0204068590765414\n",
            "1.0220795931566098\n",
            "1.0236938909039168\n",
            "1.025549002696791\n",
            "1.0272976662344335\n",
            "1.028894989615511\n",
            "1.0303226220790687\n",
            "1.032270023813638\n",
            "1.0340610307348355\n",
            "1.0358325896208243\n",
            "1.0377439718569637\n",
            "1.0391397470860835\n",
            "1.040684082547722\n",
            "1.0422501127280848\n",
            "1.0440643006730872\n",
            "1.0457370271310782\n",
            "1.0474874356671062\n",
            "1.0491826525886956\n",
            "1.050868817378798\n",
            "1.0524710717865877\n",
            "1.054009004093497\n",
            "1.0556001292012842\n",
            "1.0567918516638335\n",
            "1.0584745523722277\n",
            "1.0604344980643534\n",
            "1.062044544521805\n",
            "1.0638068974627863\n",
            "1.065207237706465\n",
            "1.0669256336701192\n",
            "1.0685327242859795\n",
            "1.0703581604353911\n",
            "1.0718322525091488\n",
            "1.073592592002181\n",
            "1.0751944983859196\n",
            "1.0768172060284773\n",
            "1.078691935981326\n",
            "1.0801069060402453\n",
            "1.0816535932176254\n",
            "1.0835173783247427\n",
            "1.0850143558381464\n",
            "1.0866468514475371\n",
            "1.0884742678126411\n",
            "1.0901497452307845\n",
            "1.0919162235449038\n",
            "1.0935954403541888\n",
            "1.0955800963637163\n",
            "1.097208685658472\n",
            "1.0988856024296998\n",
            "1.1001254886465\n",
            "1.1020405746024589\n",
            "1.1037212158255565\n",
            "1.1052021122801945\n",
            "1.107258914605431\n",
            "1.109096096376019\n",
            "1.110833493263825\n",
            "1.112601534561123\n",
            "1.1140718823656097\n",
            "1.1159102030269934\n",
            "1.1175805414881548\n",
            "1.1192502467071308\n",
            "1.1209708379814998\n",
            "1.1226673550770412\n",
            "1.1241599148344201\n",
            "1.1256749542320477\n",
            "1.1274409433612433\n",
            "1.128993209289468\n",
            "1.1307052266414819\n",
            "1.1323754428445225\n",
            "1.1337656053283331\n",
            "1.1356834367565487\n",
            "1.1372711297953526\n",
            "1.1388980038178242\n",
            "1.1407253017358463\n",
            "1.1419411334387786\n",
            "1.1437429737709368\n",
            "1.1455955541194858\n",
            "1.14706813991832\n",
            "1.1482983900000676\n",
            "1.1500738343162\n",
            "1.1521572623868732\n",
            "1.1538414783642421\n",
            "1.1556515113624466\n",
            "1.1575593773818686\n",
            "1.1592063732311855\n",
            "1.1607435808309814\n",
            "1.1625664195288783\n",
            "1.1645284196757295\n",
            "1.1663218324294176\n",
            "1.1682138202897727\n",
            "1.1698309991823133\n",
            "1.1719547106939203\n",
            "1.1736401516915587\n",
            "1.1751968079363293\n",
            "1.176796336231939\n",
            "1.1784135215270244\n",
            "1.18053995305315\n",
            "1.182122170086712\n",
            "1.1835554608756014\n",
            "1.1853131800508865\n",
            "1.1867844900664162\n",
            "1.1885081341352\n",
            "1.190159066787461\n",
            "1.1919380035394294\n",
            "1.1939615842783848\n",
            "1.1956472582067066\n",
            "1.197196662349774\n",
            "1.1992365003699232\n",
            "1.2007405694640811\n",
            "1.202299082446891\n",
            "1.2042542733347323\n",
            "1.2061752011556455\n",
            "1.207951112552677\n",
            "1.2094990462445847\n",
            "1.211319808688615\n",
            "1.2130470394021104\n",
            "1.214964253442062\n",
            "1.2166380415029843\n",
            "1.2183694582613533\n",
            "1.2198850830345203\n",
            "1.2216389358348554\n",
            "1.2231808723238728\n",
            "1.2247744816191055\n",
            "1.2263698215832186\n",
            "1.2281575536026674\n",
            "1.2299122442217434\n",
            "1.2318055758543331\n",
            "1.2334039971194304\n",
            "1.2353110021489966\n",
            "1.236984575572221\n",
            "1.2387235345285568\n",
            "1.2400892231318041\n",
            "1.241727771890133\n",
            "1.2436031715186966\n",
            "1.2453729364725634\n",
            "1.2471211818630432\n",
            "1.2489413875905448\n",
            "1.250597491212513\n",
            "1.2525684933375825\n",
            "1.254359869472206\n",
            "1.2557226115327966\n",
            "1.2573689369441907\n",
            "1.2588643345534039\n",
            "1.2604805067219698\n",
            "1.2621188380986528\n",
            "1.2638164171020088\n",
            "1.2655418645542906\n",
            "1.2670698387696004\n",
            "1.2689120506539064\n",
            "1.2706801242688124\n",
            "1.2723010558149088\n",
            "1.2738535452224409\n",
            "1.2755018702096037\n",
            "1.2772954642162908\n",
            "1.279308083340945\n",
            "1.2810648188109288\n",
            "1.2826852965385407\n",
            "1.2842541828637233\n",
            "1.2858852699132222\n",
            "1.2873950830048613\n",
            "1.289048241608588\n",
            "1.290682863015348\n",
            "1.2921227179372403\n",
            "1.2938698253515737\n",
            "1.2953844969077488\n",
            "1.2971146891031728\n",
            "1.2992616935306802\n",
            "1.3013023440642735\n",
            "1.3031998109787017\n",
            "1.304828469634361\n",
            "1.3066458458943135\n",
            "1.3084649673050932\n",
            "1.3101989046844376\n",
            "1.311706675821558\n",
            "1.312988107085533\n",
            "1.314900104880638\n",
            "1.316589400515227\n",
            "1.3182996516794805\n",
            "1.3203531672125277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Display input image, input label, predicted labels"
      ],
      "metadata": {
        "id": "wr4rjE4HIq95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "FHUPh54WNdYj",
        "outputId": "3d8a46ff-f703-48ec-fae0-47950daa56f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3491268268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will here try other models"
      ],
      "metadata": {
        "id": "jvgdj2vkNNrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained InceptionV3\n",
        "model = models.inception_v3(pretrained=True)\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBS8PEmDNNt-",
        "outputId": "2744bb58-f298-4bda-b660-843b33222f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception3(\n",
            "  (Conv2d_1a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2b_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Conv2d_3b_1x1): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_4a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Mixed_5b): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5c): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5d): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6a): InceptionB(\n",
            "    (branch3x3): BasicConv2d(\n",
            "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6b): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6c): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6d): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6e): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (AuxLogits): InceptionAux(\n",
            "    (conv0): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): BasicConv2d(\n",
            "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            "  (Mixed_7a): InceptionD(\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7b): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7c): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6I6FCj5NNwV",
        "outputId": "8221e845-f902-4328-a199-bdcc5df4d8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=2048, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Replace the fc of the Inception-v3 model\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "# Print the modified model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZNynWAYNNya",
        "outputId": "9f7fb59c-fcaa-4c03-eeda-5cb9c55aea90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception3(\n",
            "  (Conv2d_1a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2b_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Conv2d_3b_1x1): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_4a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Mixed_5b): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5c): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5d): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6a): InceptionB(\n",
            "    (branch3x3): BasicConv2d(\n",
            "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6b): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6c): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6d): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6e): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (AuxLogits): InceptionAux(\n",
            "    (conv0): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): BasicConv2d(\n",
            "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            "  (Mixed_7a): InceptionD(\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7b): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7c): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "V83bdFx1NN4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "uONjee_dNN6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the classifier for CIFAR-10 (10 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "lO2p_PmSNN9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(299), # Resize to the expected input size for InceptionV3\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])"
      ],
      "metadata": {
        "id": "qUYsI-JqNN_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaGPMWSJNOBh",
        "outputId": "fb6a8dd2-2233-4968-8bdf-2ae4c5728ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 46.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSTMG1p8NOD2",
        "outputId": "f66adc2f-802b-44f1-957e-7bf0f19bf586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (AuxLogits): InceptionAux(\n",
              "    (conv0): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): BasicConv2d(\n",
              "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQUUPkM7hGbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "# Training loop\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        # Handle InceptionOutputs during training\n",
        "        if isinstance(outputs, torchvision.models.inception.InceptionOutputs):\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            _, predicted = outputs.logits.max(1) # Apply max to logits\n",
        "        else:\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        print(running_loss/len(train_loader))\n",
        "        #print(f'Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%’)\n",
        "#### Step 4: E"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "xfWko4v2NOIk",
        "outputId": "c8913ff8-68a6-4dc1-f932-0524754ac371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.002935546133524317\n",
            "0.00593525369453918\n",
            "0.00888528726289949\n",
            "0.011840572747428094\n",
            "0.014828911827653266\n",
            "0.017801920776172063\n",
            "0.02078558843763893\n",
            "0.023727386808761245\n",
            "0.026701218636749346\n",
            "0.029608250274072826\n",
            "0.032525922941124955\n",
            "0.03546811003819146\n",
            "0.038395174324055155\n",
            "0.04134667803869223\n",
            "0.04426623152954804\n",
            "0.04712261842644733\n",
            "0.0501079178222305\n",
            "0.05300830117881755\n",
            "0.05583459428509178\n",
            "0.058705959478607565\n",
            "0.0615400758850605\n",
            "0.06444608860308557\n",
            "0.06732588442390228\n",
            "0.07014041392089766\n",
            "0.07302908610809794\n",
            "0.07585168037268207\n",
            "0.07865994818070356\n",
            "0.08153720554488395\n",
            "0.08429707316181544\n",
            "0.08715392713961394\n",
            "0.08998817403603088\n",
            "0.09290165791426168\n",
            "0.09573044435447439\n",
            "0.09852242042951266\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2014545466.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3326\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3328\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3329\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tostring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Xom4eI-YNOKZ",
        "outputId": "f20d2845-58a4-4c23-bf1a-3e8876608dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3491268268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained InceptionV3\n",
        "model = models.inception_v3(pretrained=True)\n",
        "\n",
        "# Disable auxiliary outputs for faster training\n",
        "model.aux_logits = False\n",
        "\n",
        "# Replace the fc of the Inception-v3 model\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# Freeze all layers except the classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD-oztOXhHXm",
        "outputId": "fa63cfde-3e14-4eee-d192-e4fe17a1d2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception3(\n",
            "  (Conv2d_1a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_2b_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Conv2d_3b_1x1): BasicConv2d(\n",
            "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (Conv2d_4a_3x3): BasicConv2d(\n",
            "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (Mixed_5b): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5c): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_5d): InceptionA(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch5x5_2): BasicConv2d(\n",
            "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6a): InceptionB(\n",
            "    (branch3x3): BasicConv2d(\n",
            "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6b): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6c): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6d): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_6e): InceptionC(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7dbl_5): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (AuxLogits): InceptionAux(\n",
            "    (conv0): BasicConv2d(\n",
            "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): BasicConv2d(\n",
            "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            "  (Mixed_7a): InceptionD(\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_2): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_3): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch7x7x3_4): BasicConv2d(\n",
            "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7b): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (Mixed_7c): InceptionE(\n",
            "    (branch1x1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3_2b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_1): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_2): BasicConv2d(\n",
            "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3a): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch3x3dbl_3b): BasicConv2d(\n",
            "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch_pool): BasicConv2d(\n",
            "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(299), # Resize to the expected input size for InceptionV3\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcW7lb_ghXYV",
        "outputId": "382dd11c-4716-42e0-fd00-314fa5e311c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception3(\n",
              "  (Conv2d_1a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_2b_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Conv2d_3b_1x1): BasicConv2d(\n",
              "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (Conv2d_4a_3x3): BasicConv2d(\n",
              "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (Mixed_5b): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5c): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_5d): InceptionA(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch5x5_2): BasicConv2d(\n",
              "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6a): InceptionB(\n",
              "    (branch3x3): BasicConv2d(\n",
              "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6b): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6c): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6d): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_6e): InceptionC(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7dbl_5): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (AuxLogits): InceptionAux(\n",
              "    (conv0): BasicConv2d(\n",
              "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv1): BasicConv2d(\n",
              "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              "  (Mixed_7a): InceptionD(\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_2): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_3): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch7x7x3_4): BasicConv2d(\n",
              "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7b): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (Mixed_7c): InceptionE(\n",
              "    (branch1x1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3_2b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_1): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_2): BasicConv2d(\n",
              "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3a): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch3x3dbl_3b): BasicConv2d(\n",
              "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (branch_pool): BasicConv2d(\n",
              "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
        "# Training loop\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        print(running_loss/len(train_loader))\n",
        "\n",
        "    #print(f'Epoch [{epoch+1}/10], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6X1HTMLhong",
        "outputId": "20cc1b05-257a-4561-f153-41e37807b64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0.545130645413228\n",
            "0.5466578331825983\n",
            "0.5476945523182144\n",
            "0.5484837003604836\n",
            "0.5497261975960963\n",
            "0.5503922008416232\n",
            "0.5515641013298498\n",
            "0.5525377394674379\n",
            "0.5534476874505773\n",
            "0.5543604557547728\n",
            "0.5555135470903133\n",
            "0.556447773752615\n",
            "0.5573904287555943\n",
            "0.5584874382180631\n",
            "0.559948279767695\n",
            "0.5610619977383358\n",
            "0.5621174202322046\n",
            "0.5632699044692852\n",
            "0.564190902482823\n",
            "0.5650936692495785\n",
            "0.566762620645106\n",
            "0.567723981674065\n",
            "0.5689276493037753\n",
            "0.5702082534199175\n",
            "0.5718658802378208\n",
            "0.5730804261725272\n",
            "0.5742122140305731\n",
            "0.5753753445566158\n",
            "0.5764935699189105\n",
            "0.5776842992247828\n",
            "0.5791365598397487\n",
            "0.5804710375607166\n",
            "0.5815917772938833\n",
            "0.583165951206556\n",
            "0.5843378713597422\n",
            "0.5855182907389253\n",
            "0.5867246581846491\n",
            "0.5878301792971009\n",
            "0.5890036327073641\n",
            "0.5901918920028545\n",
            "0.59156514856669\n",
            "0.5929868030349922\n",
            "0.5940962563009213\n",
            "0.5949415388085958\n",
            "0.5960402629335823\n",
            "0.5973134506160341\n",
            "0.598485201330441\n",
            "0.5996157422166346\n",
            "0.6007619127059531\n",
            "0.6018063033099674\n",
            "0.6032426179293782\n",
            "0.6041554862352283\n",
            "0.6052809100398018\n",
            "0.6063506645543496\n",
            "0.6072663737608649\n",
            "0.6084322512835798\n",
            "0.6097868572339378\n",
            "0.611206263036984\n",
            "0.612706890153458\n",
            "0.6136718376746872\n",
            "0.6147179725148794\n",
            "0.6159999600380582\n",
            "0.6173596438163381\n",
            "0.6182603201521631\n",
            "0.6191395369103497\n",
            "0.6201064917056457\n",
            "0.6211664606924252\n",
            "0.6226459202330435\n",
            "0.6240680494043224\n",
            "0.6253281204063265\n",
            "0.6266481669433891\n",
            "0.6279892905060288\n",
            "0.6293320947367212\n",
            "0.6303745042866148\n",
            "0.6311981612078065\n",
            "0.6322785612491085\n",
            "0.6331317766624338\n",
            "0.6340150053010267\n",
            "0.6351458798817662\n",
            "0.6361412772208529\n",
            "0.6371448535062468\n",
            "0.6380195435889237\n",
            "0.6390940520693275\n",
            "0.6403339322646866\n",
            "0.641310762680705\n",
            "0.6425470066116289\n",
            "0.6435600284039212\n",
            "0.6449108060897158\n",
            "0.6461891097104763\n",
            "0.6471784251653935\n",
            "0.6483628302050368\n",
            "0.6489602776668261\n",
            "0.6503928545719523\n",
            "0.6516040863512117\n",
            "0.6528150338269866\n",
            "0.6540257035542631\n",
            "0.6551983369814466\n",
            "0.6565928460310793\n",
            "0.6577335664302187\n",
            "0.6593260647314588\n",
            "0.6599961197208566\n",
            "0.6609450104215261\n",
            "0.661833221688295\n",
            "0.6631314297542548\n",
            "0.6641955998014001\n",
            "0.6650861416707563\n",
            "0.66648134859779\n",
            "0.6679696104182001\n",
            "0.6691726275798305\n",
            "0.6701495942023709\n",
            "0.6713480417761961\n",
            "0.6726257937109988\n",
            "0.6738766258406212\n",
            "0.6752474479129552\n",
            "0.6764933078566475\n",
            "0.6778464409167809\n",
            "0.679229875392926\n",
            "0.68065830482089\n",
            "0.6818902806933883\n",
            "0.6828061778603307\n",
            "0.6836791924579674\n",
            "0.6845418767017477\n",
            "0.6863618885236018\n",
            "0.6877460858172468\n",
            "0.6887564385104972\n",
            "0.6895728243510132\n",
            "0.6907075464801715\n",
            "0.6917443152355112\n",
            "0.6925888718927607\n",
            "0.6938094414789658\n",
            "0.6949557128083675\n",
            "0.6959373510021079\n",
            "0.6971212308806227\n",
            "0.6982198028689455\n",
            "0.6991800124306813\n",
            "0.7007137637232881\n",
            "0.701898221355265\n",
            "0.7032381900207466\n",
            "0.7042015292836578\n",
            "0.7057476460247698\n",
            "0.7068457061905995\n",
            "0.7076298799889776\n",
            "0.7087717856211431\n",
            "0.7097088177414501\n",
            "0.7112182347899507\n",
            "0.7123978251157819\n",
            "0.7133936157540592\n",
            "0.7149888869670346\n",
            "0.7160898697421983\n",
            "0.7169584737485631\n",
            "0.7179711530046999\n",
            "0.7192165978500605\n",
            "0.7202760220869727\n",
            "0.7213037804035884\n",
            "0.7223837260166397\n",
            "0.7236570663693006\n",
            "0.7249306339360869\n",
            "0.7258962515141348\n",
            "0.7268164972286395\n",
            "0.7280777148197374\n",
            "0.7292142079385651\n",
            "0.7306859528698275\n",
            "0.7316254017221958\n",
            "0.7327576707619841\n",
            "0.7337685300184943\n",
            "0.7346708928913717\n",
            "0.7355839220230537\n",
            "0.7367226792037335\n",
            "0.7376452552921632\n",
            "0.738998187434338\n",
            "0.74056479902676\n",
            "0.7417339468779771\n",
            "0.7431471464807725\n",
            "0.7442576585294646\n",
            "0.745208311606856\n",
            "0.7462048123940788\n",
            "0.7473011168906146\n",
            "0.7486560987618268\n",
            "0.7496546928001486\n",
            "0.7506802135797412\n",
            "0.7522222991565914\n",
            "0.7533231637133356\n",
            "0.754500394015361\n",
            "0.7556740017726903\n",
            "0.756991028137829\n",
            "0.7579583988698853\n",
            "0.7592563849809529\n",
            "0.760403867306002\n",
            "0.7614782454488832\n",
            "0.7623425771284591\n",
            "0.7636157713658974\n",
            "0.7649621122404743\n",
            "0.7660500063082142\n",
            "0.7670823320784532\n",
            "0.7684156799026768\n",
            "0.7694684243796731\n",
            "0.7706742707039694\n",
            "0.7715792007687147\n",
            "0.77277733789533\n",
            "0.7744550348623939\n",
            "0.7753950777032491\n",
            "0.776333832870359\n",
            "0.7774501654421887\n",
            "0.7784784647738537\n",
            "0.7798246742438173\n",
            "0.7809825584940289\n",
            "0.782380823093607\n",
            "0.7834592452058402\n",
            "0.7848084217980694\n",
            "0.7855844860110441\n",
            "0.7867362247327404\n",
            "0.7881861441504315\n",
            "0.7894465731614081\n",
            "0.7904232420656078\n",
            "0.7913287719878395\n",
            "0.7925984727985719\n",
            "0.7935838678380107\n",
            "0.7944960699743017\n",
            "0.7953993750121587\n",
            "0.7965230111347135\n",
            "0.7973511975515833\n",
            "0.798560389815389\n",
            "0.7995343890870014\n",
            "0.800429747956793\n",
            "0.801728537129929\n",
            "0.8029237519520933\n",
            "0.8041937278435968\n",
            "0.805267296826748\n",
            "0.806356231460486\n",
            "0.807470403020949\n",
            "0.8085079540300857\n",
            "0.8098727002396913\n",
            "0.8114965222299556\n",
            "0.8127593251369188\n",
            "0.8137873097530106\n",
            "0.8149426791750257\n",
            "0.8160589717309493\n",
            "0.8170056316020239\n",
            "0.8180257833903403\n",
            "0.8192027135349601\n",
            "0.8204347113590411\n",
            "0.8213677436418241\n",
            "0.8222056783907249\n",
            "0.8233712681800204\n",
            "0.8245243326858487\n",
            "0.8257442258889108\n",
            "0.8266914840930563\n",
            "0.8280097485884376\n",
            "0.8293461950919817\n",
            "0.8304871357691562\n",
            "0.8317546882973913\n",
            "0.8326431035309496\n",
            "0.8342585247724562\n",
            "0.835277607640647\n",
            "0.8363241801405197\n",
            "0.8373961942961149\n",
            "0.8387026006684584\n",
            "0.8401006076418226\n",
            "0.841367940044464\n",
            "0.8425312514424019\n",
            "0.8437913652804806\n",
            "0.8447879970149921\n",
            "0.8458517689610381\n",
            "0.8470111704619644\n",
            "0.8483266922290368\n",
            "0.8496795955597592\n",
            "0.8506607380898102\n",
            "0.8516460480668661\n",
            "0.8529791282037334\n",
            "0.8540327596618696\n",
            "0.8553972831543755\n",
            "0.8567975256830225\n",
            "0.8578548546108749\n",
            "0.8589162838352306\n",
            "0.8600852398387612\n",
            "0.8617588246951018\n",
            "0.8630017987678728\n",
            "0.8641467944664114\n",
            "0.8653481779882061\n",
            "0.8666650720340822\n",
            "0.867663378491426\n",
            "0.8688316341022702\n",
            "0.8702802236199074\n",
            "0.8715576681563312\n",
            "0.8731473659539162\n",
            "0.8744539322374422\n",
            "0.875680583631596\n",
            "0.8769384518532497\n",
            "0.8780505097354464\n",
            "0.8791813909473931\n",
            "0.8804447221405366\n",
            "0.8813931686265389\n",
            "0.882495510036988\n",
            "0.8834388034651651\n",
            "0.8842542322014298\n",
            "0.8857669063160182\n",
            "0.8870692748166716\n",
            "0.8880826376207039\n",
            "0.889106681089267\n",
            "0.8903440550693771\n",
            "0.8917473523741792\n",
            "0.8929199098854723\n",
            "0.8940414612174339\n",
            "0.895273355457484\n",
            "0.8965138967156105\n",
            "0.8975093339181617\n",
            "0.8986638155206085\n",
            "0.8999103955219468\n",
            "0.0012607083601110121\n",
            "0.0025185713987521197\n",
            "0.0032706593003724235\n",
            "0.004538548800646497\n",
            "0.005647056471661228\n",
            "0.007155941804046826\n",
            "0.00829084152760713\n",
            "0.009373368204707074\n",
            "0.010489343910875833\n",
            "0.011408535187201732\n",
            "0.012664208174361598\n",
            "0.01381553278859619\n",
            "0.015095492076995733\n",
            "0.016335041550419215\n",
            "0.01736066660002979\n",
            "0.01847059334940313\n",
            "0.01979492989647419\n",
            "0.02080179061121343\n",
            "0.02227968411982212\n",
            "0.023355430425585383\n",
            "0.024556794587303612\n",
            "0.02550221754766791\n",
            "0.02669639652952209\n",
            "0.0274611966841666\n",
            "0.02846161872529618\n",
            "0.029578288116723375\n",
            "0.030646926911590655\n",
            "0.03182318029196366\n",
            "0.03313970512441357\n",
            "0.03408743589735397\n",
            "0.03509408807205727\n",
            "0.036792087204316086\n",
            "0.038077854058321786\n",
            "0.0392397216061497\n",
            "0.04062043896416569\n",
            "0.041647348181366006\n",
            "0.04251005590114447\n",
            "0.0435405234089288\n",
            "0.044913802229230054\n",
            "0.045760680799898895\n",
            "0.04699327489909004\n",
            "0.047888834686840284\n",
            "0.04897793837825356\n",
            "0.04983219123252518\n",
            "0.05085579002909648\n",
            "0.05173110817094593\n",
            "0.05297675492513515\n",
            "0.054566857912351406\n",
            "0.05583269417743244\n",
            "0.05705726329627854\n",
            "0.05824182466472811\n",
            "0.05927579657500967\n",
            "0.06058236072435404\n",
            "0.06150852566789788\n",
            "0.06239850937253069\n",
            "0.06347840650917014\n",
            "0.06485748451079248\n",
            "0.06602898514484201\n",
            "0.06693186830071841\n",
            "0.06801698488347671\n",
            "0.06928088201586242\n",
            "0.07039819082335742\n",
            "0.07144579329454076\n",
            "0.07289706364921901\n",
            "0.07395014204942357\n",
            "0.07526381713959872\n",
            "0.0762533638483423\n",
            "0.07731257946899785\n",
            "0.07835946683688542\n",
            "0.07934120838599437\n",
            "0.0807681210205683\n",
            "0.08192196244473958\n",
            "0.08307234199760515\n",
            "0.08419691693142552\n",
            "0.08539878178740401\n",
            "0.0866153041267639\n",
            "0.08759136082571181\n",
            "0.08851100142349673\n",
            "0.08972162031151755\n",
            "0.09063972559426446\n",
            "0.0916545364405493\n",
            "0.09290749299556703\n",
            "0.09426329591695\n",
            "0.09515117417516002\n",
            "0.09617354154891675\n",
            "0.09741685251750605\n",
            "0.09886615866285456\n",
            "0.10000989405090546\n",
            "0.10102614828997561\n",
            "0.10200705872777173\n",
            "0.10329301811545097\n",
            "0.10446484306889117\n",
            "0.10533476150249277\n",
            "0.10629507342872717\n",
            "0.10723115370401641\n",
            "0.1085836266922524\n",
            "0.10977430477776491\n",
            "0.11067631528200701\n",
            "0.11150644014558524\n",
            "0.11291501436696942\n",
            "0.11375584085579113\n",
            "0.11503709681198725\n",
            "0.11638297319717114\n",
            "0.11747399826183953\n",
            "0.1183673191405928\n",
            "0.12008316978774107\n",
            "0.12122346761891299\n",
            "0.12257406710053953\n",
            "0.1235430717010937\n",
            "0.1244533271588328\n",
            "0.12554231773861838\n",
            "0.12643822318757586\n",
            "0.12780429365689797\n",
            "0.1290112720883411\n",
            "0.12997580618809557\n",
            "0.13090574596544055\n",
            "0.1319875344443504\n",
            "0.13306521248939396\n",
            "0.13415278025600305\n",
            "0.13570233150516325\n",
            "0.1370846429444335\n",
            "0.13800913347002794\n",
            "0.1391267266572284\n",
            "0.14053300327962012\n",
            "0.14158058433276613\n",
            "0.143121432998906\n",
            "0.14415225645770197\n",
            "0.14532946961005325\n",
            "0.14669013938025746\n",
            "0.14813898042644685\n",
            "0.14932641318387083\n",
            "0.15068332648948027\n",
            "0.15179903168812434\n",
            "0.15291362360615254\n",
            "0.1541121526599845\n",
            "0.15496304090065724\n",
            "0.15612019213569134\n",
            "0.15685990483254728\n",
            "0.15808248657094853\n",
            "0.15916515692420627\n",
            "0.16082897042984243\n",
            "0.16202363539534761\n",
            "0.16285845431525384\n",
            "0.16375915816677805\n",
            "0.1655334600859591\n",
            "0.16669111719826604\n",
            "0.16775318096056008\n",
            "0.1691847336109337\n",
            "0.17028410965219484\n",
            "0.1717536918952337\n",
            "0.1728761693095917\n",
            "0.1738663471263388\n",
            "0.17490923945861095\n",
            "0.17595058778667694\n",
            "0.17717000918315196\n",
            "0.17876178552122676\n",
            "0.17987370605358993\n",
            "0.18081010691345195\n",
            "0.1820987535407171\n",
            "0.1831928122683864\n",
            "0.18450493032060317\n",
            "0.18566694390743285\n",
            "0.18722088882685317\n",
            "0.1884285856390853\n",
            "0.18947421818438087\n",
            "0.19072266567088758\n",
            "0.19163893349945088\n",
            "0.19321246990157515\n",
            "0.1943796837268888\n",
            "0.19561145662346766\n",
            "0.19695914309957754\n",
            "0.19809878406012454\n",
            "0.19892846958716506\n",
            "0.19992250082133067\n",
            "0.2012309950330983\n",
            "0.20223105487311283\n",
            "0.20357859149918228\n",
            "0.20452017186547788\n",
            "0.20550643483086314\n",
            "0.20639847878300016\n",
            "0.20781513164415383\n",
            "0.20885560091804056\n",
            "0.20999422844718485\n",
            "0.21120102692138204\n",
            "0.21226384633642328\n",
            "0.21320081869964405\n",
            "0.21449983226673683\n",
            "0.21559004809545435\n",
            "0.21679103595521443\n",
            "0.21801367996598753\n",
            "0.21941565956605974\n",
            "0.22054920751420434\n",
            "0.22185545504245613\n",
            "0.22295294347626474\n",
            "0.22384401538487894\n",
            "0.22473898857755734\n",
            "0.22586345169550318\n",
            "0.22680841961785045\n",
            "0.22783420434998125\n",
            "0.22893844990779066\n",
            "0.230076974843774\n",
            "0.23129607862828638\n",
            "0.2327443910071917\n",
            "0.2340017384885217\n",
            "0.23503958995994703\n",
            "0.2360381644857509\n",
            "0.23723239285866624\n",
            "0.23909143032625202\n",
            "0.24043992550476737\n",
            "0.24143558702505458\n",
            "0.24305028599851272\n",
            "0.24406656058852935\n",
            "0.24508372947687992\n",
            "0.24572558308501377\n",
            "0.24676927649761404\n",
            "0.24790294281661968\n",
            "0.24902200462568141\n",
            "0.2500691298023819\n",
            "0.25087039107861725\n",
            "0.2517216986859851\n",
            "0.2530456739465904\n",
            "0.2539060539601709\n",
            "0.2548774477770871\n",
            "0.255990632964522\n",
            "0.2573472160817412\n",
            "0.25836872879196615\n",
            "0.25973357850938195\n",
            "0.261104495400358\n",
            "0.26223070199227394\n",
            "0.26312833178378736\n",
            "0.26426540600979115\n",
            "0.2652836042596861\n",
            "0.2660877908129826\n",
            "0.2671839403526862\n",
            "0.26817067939302197\n",
            "0.26899018517845424\n",
            "0.27039616027146657\n",
            "0.27152666754429905\n",
            "0.27262833988879953\n",
            "0.2741759127515661\n",
            "0.27523902272019546\n",
            "0.27656610169069235\n",
            "0.27790680405733836\n",
            "0.2788667850329748\n",
            "0.28004133533638764\n",
            "0.2807084770916063\n",
            "0.2816671223744102\n",
            "0.2826861595863576\n",
            "0.2837913800078585\n",
            "0.2852191579006517\n",
            "0.28615741999557864\n",
            "0.2876021315527084\n",
            "0.288868715619797\n",
            "0.28999868385932026\n",
            "0.29106564015683617\n",
            "0.2924084369178928\n",
            "0.29369539144398915\n",
            "0.29471821720947694\n",
            "0.2960002006167341\n",
            "0.2968114732628893\n",
            "0.2977531926558756\n",
            "0.29864802034309756\n",
            "0.2994083423748651\n",
            "0.30077410887574296\n",
            "0.30197413475312235\n",
            "0.3030852319486916\n",
            "0.3039402073758947\n",
            "0.30518322657136354\n",
            "0.30651112506761574\n",
            "0.3076745495771813\n",
            "0.30880889136468054\n",
            "0.3097837673276282\n",
            "0.31088929644326113\n",
            "0.3116524984007296\n",
            "0.31259682164777575\n",
            "0.31352464660354284\n",
            "0.31457156331642816\n",
            "0.31550111147143955\n",
            "0.3167397950006568\n",
            "0.3176861188905623\n",
            "0.3190708628395939\n",
            "0.32005729699683616\n",
            "0.3212672303552213\n",
            "0.32286950122669833\n",
            "0.32344074139509665\n",
            "0.32438378870639656\n",
            "0.3257034985000825\n",
            "0.32700153415465294\n",
            "0.3278393496942642\n",
            "0.32909581956960965\n",
            "0.33012990268599957\n",
            "0.3308992024577792\n",
            "0.3316737107761071\n",
            "0.33264875785469095\n",
            "0.3338117495064845\n",
            "0.3352410042529826\n",
            "0.3366189694313137\n",
            "0.337931818135864\n",
            "0.3389936286927489\n",
            "0.34053641031770143\n",
            "0.34182061906665795\n",
            "0.3431829123393349\n",
            "0.3439003485243034\n",
            "0.34496116485742045\n",
            "0.34629572474438214\n",
            "0.34720239165189015\n",
            "0.34798228184280494\n",
            "0.3492595493945929\n",
            "0.35027432174938716\n",
            "0.351496947695837\n",
            "0.3524354139862158\n",
            "0.3535971173544979\n",
            "0.3545164533740724\n",
            "0.35610020488424377\n",
            "0.3572189909265474\n",
            "0.35823310862111923\n",
            "0.35949951395049423\n",
            "0.360236042753205\n",
            "0.36148666939162233\n",
            "0.3624619702853815\n",
            "0.363644651287352\n",
            "0.36482743229097725\n",
            "0.3660570257307623\n",
            "0.3672204666277942\n",
            "0.3683861423179012\n",
            "0.3692508898580166\n",
            "0.3702616853177395\n",
            "0.37145779321870537\n",
            "0.37251324894483134\n",
            "0.37371037073452457\n",
            "0.3745727808884038\n",
            "0.3756720266683632\n",
            "0.37671692833266296\n",
            "0.3779292956490041\n",
            "0.3790314987949703\n",
            "0.3803114704311351\n",
            "0.3811410591577935\n",
            "0.3823115665589452\n",
            "0.38330878595561935\n",
            "0.3843245535250515\n",
            "0.38530131877230867\n",
            "0.3863274865900464\n",
            "0.38756094053578194\n",
            "0.38880845256473706\n",
            "0.3904000548145655\n",
            "0.39166513855195106\n",
            "0.3929313647625087\n",
            "0.39464163696369553\n",
            "0.3957141505940186\n",
            "0.3968185716120483\n",
            "0.397911964627483\n",
            "0.39889625911517523\n",
            "0.39996586774316284\n",
            "0.40134182130284324\n",
            "0.4026311121480849\n",
            "0.4038508050429547\n",
            "0.40485974102069044\n",
            "0.4062325378208209\n",
            "0.4074813909542835\n",
            "0.4085386275025585\n",
            "0.4097902904386106\n",
            "0.4109620489275364\n",
            "0.4121808234383078\n",
            "0.41304491654686304\n",
            "0.4142115930157244\n",
            "0.4153304562696715\n",
            "0.41633224571147537\n",
            "0.41738232825418264\n",
            "0.418479139085316\n",
            "0.419952156141286\n",
            "0.42088678486816716\n",
            "0.4221628565922418\n",
            "0.42324889537013705\n",
            "0.4244333457611406\n",
            "0.4257170110559829\n",
            "0.4266419916049294\n",
            "0.42781085393312945\n",
            "0.42883593430909356\n",
            "0.42968164654948826\n",
            "0.4307847497865672\n",
            "0.43216277243536144\n",
            "0.4329541482583946\n",
            "0.4340713334357952\n",
            "0.4350892701722167\n",
            "0.43635267842456205\n",
            "0.43733651139547147\n",
            "0.4388914256906875\n",
            "0.4397707007577657\n",
            "0.440815846404761\n",
            "0.44205907444514886\n",
            "0.4430590380945474\n",
            "0.44385303004318494\n",
            "0.44528877879957407\n",
            "0.44652153913627196\n",
            "0.4476069923099654\n",
            "0.4483871905852462\n",
            "0.44961795736761656\n",
            "0.45098170401800014\n",
            "0.45214501183356165\n",
            "0.45338930452571197\n",
            "0.4548614287315427\n",
            "0.45620088656540114\n",
            "0.4573390204888171\n",
            "0.45854739002559497\n",
            "0.4597039340859484\n",
            "0.4605807197825683\n",
            "0.46151553159174713\n",
            "0.4623688382413381\n",
            "0.4633783211031228\n",
            "0.4644328566920727\n",
            "0.4655714329246365\n",
            "0.4665182423408684\n",
            "0.46707148697522594\n",
            "0.4685072317681349\n",
            "0.4697052977045479\n",
            "0.47124202587567937\n",
            "0.4725126732721963\n",
            "0.47342599509164807\n",
            "0.4746615392396517\n",
            "0.47585298890805305\n",
            "0.47665974303432135\n",
            "0.47808966456014484\n",
            "0.47914483514435763\n",
            "0.48028047870644525\n",
            "0.48126072312712365\n",
            "0.48230744219954363\n",
            "0.48329667076277916\n",
            "0.48468270924542567\n",
            "0.4862405155092249\n",
            "0.4877739209119621\n",
            "0.4890960446175407\n",
            "0.4902391187903826\n",
            "0.49137416973595727\n",
            "0.4922917304212785\n",
            "0.49344395207779485\n",
            "0.49465083134601184\n",
            "0.495451659840696\n",
            "0.49644670999416\n",
            "0.49763711292268065\n",
            "0.498802542648352\n",
            "0.4999166459150022\n",
            "0.5010373636965861\n",
            "0.5021018682004851\n",
            "0.5033908386135955\n",
            "0.5044518458797499\n",
            "0.5053870671469233\n",
            "0.5065163403292141\n",
            "0.5080258043297111\n",
            "0.5094534788671357\n",
            "0.5103060012049687\n",
            "0.5119423529757258\n",
            "0.5131652328135717\n",
            "0.51399024554988\n",
            "0.5150286903238053\n",
            "0.5160919334501257\n",
            "0.5173714107183545\n",
            "0.5183059750386821\n",
            "0.5194528424145316\n",
            "0.5205194272882189\n",
            "0.522077751853277\n",
            "0.5235981078022887\n",
            "0.5246415409209478\n",
            "0.5261851737794974\n",
            "0.5277395040711479\n",
            "0.5290709841815407\n",
            "0.530096309683512\n",
            "0.5313694680971868\n",
            "0.5327250100386417\n",
            "0.5339445504919648\n",
            "0.5349505400032644\n",
            "0.5365147744222065\n",
            "0.5375981915866017\n",
            "0.538485503372024\n",
            "0.5396480698643438\n",
            "0.5410555904097569\n",
            "0.5423234303665283\n",
            "0.5433604217627469\n",
            "0.5444528670490855\n",
            "0.5456608572350744\n",
            "0.5467980351975507\n",
            "0.5481496532935926\n",
            "0.5494319296935025\n",
            "0.5506432979841671\n",
            "0.5517838007729986\n",
            "0.5528647175911442\n",
            "0.5545088658323678\n",
            "0.5558413276663217\n",
            "0.5567726011547591\n",
            "0.5578152795353204\n",
            "0.5594237430397507\n",
            "0.5607270768002781\n",
            "0.5623766297040997\n",
            "0.5635948105312675\n",
            "0.5650385565236401\n",
            "0.5662443865366908\n",
            "0.5673654871752195\n",
            "0.5686111987932868\n",
            "0.5699992601752586\n",
            "0.5710894212012401\n",
            "0.5722522083526987\n",
            "0.5733866658433319\n",
            "0.574445102716346\n",
            "0.5756294377090986\n",
            "0.5768940367585863\n",
            "0.5778020174835649\n",
            "0.5789547458557827\n",
            "0.5801571434187462\n",
            "0.5810461104144831\n",
            "0.5819621135283004\n",
            "0.5833786532778265\n",
            "0.5847217072077724\n",
            "0.5857573456852637\n",
            "0.5865875675778864\n",
            "0.587925417763193\n",
            "0.5889147009005022\n",
            "0.5898716325116584\n",
            "0.5911412128173482\n",
            "0.592362930364621\n",
            "0.5934016972475344\n",
            "0.5944389862858731\n",
            "0.5954588039985398\n",
            "0.5965683040640238\n",
            "0.5978463265825721\n",
            "0.5991588329034083\n",
            "0.6006265419447209\n",
            "0.6018922200516972\n",
            "0.6032043131035002\n",
            "0.6043790374189386\n",
            "0.6052991528721416\n",
            "0.6063529965289108\n",
            "0.6075704409871869\n",
            "0.6086961608332442\n",
            "0.6100050257828534\n",
            "0.6113457153444095\n",
            "0.6125836951272262\n",
            "0.613726509387231\n",
            "0.6148414270728445\n",
            "0.6157878356440293\n",
            "0.6167377108122076\n",
            "0.617687422563048\n",
            "0.6187739261352193\n",
            "0.6202018669880259\n",
            "0.6214685678253393\n",
            "0.622445275754575\n",
            "0.6235421027445123\n",
            "0.625101703092875\n",
            "0.6264159102040483\n",
            "0.6274936378688154\n",
            "0.6284311071152577\n",
            "0.6293500576863813\n",
            "0.6303276717281707\n",
            "0.6314458443456904\n",
            "0.6324791860626177\n",
            "0.6335828624036915\n",
            "0.6343269475814327\n",
            "0.6351656850112979\n",
            "0.6362622908085508\n",
            "0.6374441874225426\n",
            "0.6384172481877725\n",
            "0.6398537804937119\n",
            "0.640948365602042\n",
            "0.6423455612814944\n",
            "0.6432677108384764\n",
            "0.6443042738739487\n",
            "0.6451864377083376\n",
            "0.6461376614506592\n",
            "0.6475823816969571\n",
            "0.6487793711673878\n",
            "0.6501147850318942\n",
            "0.6510204041324308\n",
            "0.6520733735369294\n",
            "0.653184871196442\n",
            "0.654231302687884\n",
            "0.6552622043491935\n",
            "0.6565364260426567\n",
            "0.6577298647683599\n",
            "0.6589651179816717\n",
            "0.6602949730651763\n",
            "0.6615021866377052\n",
            "0.6623678204729734\n",
            "0.6635056716173201\n",
            "0.6645953090065886\n",
            "0.665869550517453\n",
            "0.667016048756097\n",
            "0.6683131875970479\n",
            "0.6690690194630562\n",
            "0.6703104523517897\n",
            "0.6714809323896838\n",
            "0.6728795346472879\n",
            "0.6740413300140434\n",
            "0.6755578825464639\n",
            "0.6770032299372851\n",
            "0.6782170253260361\n",
            "0.6792745414521079\n",
            "0.6801911518930475\n",
            "0.6814278411819502\n",
            "0.6828799240500726\n",
            "0.6837714170022389\n",
            "0.6845054504130502\n",
            "0.6855845582073606\n",
            "0.6865195436474613\n",
            "0.6876381508758306\n",
            "0.6885351690718585\n",
            "0.6894749799729003\n",
            "0.6904212941065468\n",
            "0.6919654666081719\n",
            "0.6930978964737919\n",
            "0.6942868764366945\n",
            "0.6951164578461586\n",
            "0.6962937143681299\n",
            "0.6973802097846785\n",
            "0.6983787506209005\n",
            "0.6996869388824839\n",
            "0.7011222742173983\n",
            "0.702372578518165\n",
            "0.7032515317811381\n",
            "0.7044830587132812\n",
            "0.706097484054163\n",
            "0.707102133528046\n",
            "0.7085124750042815\n",
            "0.7094989899555435\n",
            "0.7105303007699645\n",
            "0.7117180733195961\n",
            "0.712656179695483\n",
            "0.7140175430747249\n",
            "0.7151656907309046\n",
            "0.7164910481790142\n",
            "0.7174980640792481\n",
            "0.7187438664000357\n",
            "0.7201017509107395\n",
            "0.721363742828674\n",
            "0.7224908877745309\n",
            "0.7234339493391154\n",
            "0.7242800334606634\n",
            "0.7256751688926116\n",
            "0.7272938423220764\n",
            "0.7284906134962121\n",
            "0.7297538760144388\n",
            "0.7308735136714433\n",
            "0.732183842822109\n",
            "0.7334368653080957\n",
            "0.7346971456123434\n",
            "0.7358592708626062\n",
            "0.7368385458312681\n",
            "0.7379347087096071\n",
            "0.7388807835100252\n",
            "0.739681412687387\n",
            "0.7405066223781737\n",
            "0.7415893447139988\n",
            "0.742725346285059\n",
            "0.7438267703785006\n",
            "0.7447405880140832\n",
            "0.7458786441923102\n",
            "0.7471162405084161\n",
            "0.7480246502039073\n",
            "0.749344398412863\n",
            "0.7505309136246171\n",
            "0.7513227889223781\n",
            "0.7524965405083068\n",
            "0.7533566132759499\n",
            "0.754542312239442\n",
            "0.7555648982905976\n",
            "0.7565538956762274\n",
            "0.757509265295075\n",
            "0.7585558238465463\n",
            "0.7596726298255994\n",
            "0.760673942430245\n",
            "0.7619021436976045\n",
            "0.7631054088816314\n",
            "0.7642069630839331\n",
            "0.7656790398804428\n",
            "0.7668274776328861\n",
            "0.768258607410409\n",
            "0.7694560470407271\n",
            "0.7704221826913716\n",
            "0.7715742081556174\n",
            "0.7724562609363395\n",
            "0.7736569706664975\n",
            "0.7745751515602517\n",
            "0.7753630479812013\n",
            "0.7764090883076343\n",
            "0.7777985704448217\n",
            "0.7787516840049983\n",
            "0.7799381790944683\n",
            "0.781438937158231\n",
            "0.7825933772203563\n",
            "0.7836698499863105\n",
            "0.7845513713558007\n",
            "0.7855935195446624\n",
            "0.7866483505653299\n",
            "0.787823606368221\n",
            "0.789023339405389\n",
            "0.7901661513787707\n",
            "0.791164751286092\n",
            "0.7924160001909032\n",
            "0.7935632015661815\n",
            "0.7945166770988108\n",
            "0.796509969074403\n",
            "0.7974960140483763\n",
            "0.7989477591060311\n",
            "0.7999562667992414\n",
            "0.8012087843226045\n",
            "0.8024090733903143\n",
            "0.8033292573659926\n",
            "0.8047893879282505\n",
            "0.8058118040833022\n",
            "0.8071485888546385\n",
            "0.808225830383313\n",
            "0.80947222890299\n",
            "0.8104444598526601\n",
            "0.8113769303121225\n",
            "0.8132838396083973\n",
            "0.8146197082060377\n",
            "0.8157042891092008\n",
            "0.8171173598988891\n",
            "0.8181423648925084\n",
            "0.819507233512676\n",
            "0.8205811650780461\n",
            "0.8217291221060716\n",
            "0.8225998967276205\n",
            "0.8239310024415746\n",
            "0.8249893809294762\n",
            "0.8261587640742207\n",
            "0.8270325297513581\n",
            "0.828708890873148\n",
            "0.8301452968050452\n",
            "0.8313613110567297\n",
            "0.8324328212405715\n",
            "0.8334757438324907\n",
            "0.8343340414183219\n",
            "0.835121318164384\n",
            "0.8363716839753148\n",
            "0.8375968358019734\n",
            "0.8382810763157237\n",
            "0.8395591842015381\n",
            "0.8407217078577832\n",
            "0.8422902539334334\n",
            "0.8433591426562166\n",
            "0.8444794061619912\n",
            "0.8455221750928313\n",
            "0.8468930585991086\n",
            "0.8477455335276206\n",
            "0.8488787040304955\n",
            "0.8498890144974375\n",
            "0.8511486967734974\n",
            "0.8525985358544933\n",
            "0.8534793066018073\n",
            "0.8543074693902374\n",
            "0.8554714551133573\n",
            "0.8567424902449483\n",
            "0.8576933646674656\n",
            "0.858797286058326\n",
            "0.8599280129994273\n",
            "0.8612912469888891\n",
            "0.8621964449696529\n",
            "0.8634728084668479\n",
            "0.8647355892697869\n",
            "0.8657100351189103\n",
            "0.8667129231307208\n",
            "0.867596427459851\n",
            "0.8682645514721761\n",
            "0.8693794168703392\n",
            "0.8705363093358477\n",
            "0.8718227428167372\n",
            "0.8730831439309108\n",
            "0.8742667409541357\n",
            "0.8758787313461913\n",
            "0.8769302405893346\n",
            "0.8778454993310791\n",
            "0.8791864309698114\n",
            "0.8802664779564914\n",
            "0.8811131353344759\n",
            "0.8824744019133356\n",
            "0.8838582049931407\n",
            "0.8857423633032138\n",
            "0.8870402910291691\n",
            "0.8882503474078824\n",
            "0.889637173556001\n",
            "0.8908730195382671\n",
            "0.8922285152899335\n",
            "0.8934305789296889\n",
            "0.8952070034449667\n",
            "0.0013200468419457946\n",
            "0.002746089949937123\n",
            "0.003900990690416692\n",
            "0.005543013286712529\n",
            "0.006658430675716352\n",
            "0.007614009139482933\n",
            "0.008800708195742439\n",
            "0.009660099397230027\n",
            "0.010507088106916383\n",
            "0.011612920428785827\n",
            "0.012829031328411053\n",
            "0.014153228239025302\n",
            "0.014927425195493966\n",
            "0.015880154374310427\n",
            "0.017093285041696885\n",
            "0.01807056897131683\n",
            "0.019275563071146036\n",
            "0.020533230298620357\n",
            "0.021578769397247784\n",
            "0.022501577654153185\n",
            "0.02354026465769619\n",
            "0.0245735947128452\n",
            "0.025526791048781645\n",
            "0.026561818921657473\n",
            "0.027811966207631104\n",
            "0.028905535369273035\n",
            "0.030041999600427534\n",
            "0.030859223945671335\n",
            "0.03198815116187191\n",
            "0.0329667098077057\n",
            "0.03439430995365543\n",
            "0.03601470673480607\n",
            "0.037242311727055506\n",
            "0.03824423646073207\n",
            "0.03919579191585941\n",
            "0.040157957226419086\n",
            "0.04106553024648096\n",
            "0.04201054329152607\n",
            "0.043308724375332106\n",
            "0.0443382356935145\n",
            "0.04551577064997095\n",
            "0.04669976081994488\n",
            "0.048124929523224114\n",
            "0.04915443748769248\n",
            "0.050330970826966075\n",
            "0.05129332082045963\n",
            "0.05236387024145297\n",
            "0.05352818950667711\n",
            "0.05429539823776011\n",
            "0.0556680092116451\n",
            "0.056529311649024944\n",
            "0.05769008702939124\n",
            "0.0587018334957035\n",
            "0.05948058860686124\n",
            "0.06063369038464773\n",
            "0.06183215167821216\n",
            "0.06275363369366092\n",
            "0.06351995795888973\n",
            "0.06456786531316655\n",
            "0.065523144153073\n",
            "0.06653352199917864\n",
            "0.06762817707817878\n",
            "0.06915648498803453\n",
            "0.07054268795510997\n",
            "0.07182769077208341\n",
            "0.07300135516144736\n",
            "0.07414187830122536\n",
            "0.0752733647061126\n",
            "0.07597275920536208\n",
            "0.07752711739381561\n",
            "0.07853686702830712\n",
            "0.07975822923433445\n",
            "0.08092303250146948\n",
            "0.08182169264539733\n",
            "0.08323265013792326\n",
            "0.08409223707435685\n",
            "0.08541693704207536\n",
            "0.08660611586497567\n",
            "0.08779343467234346\n",
            "0.08920372843437488\n",
            "0.09057257257763991\n",
            "0.09188068720995618\n",
            "0.09359709823223027\n",
            "0.094942675679541\n",
            "0.0958246692367222\n",
            "0.0973068832436486\n",
            "0.09813605321337805\n",
            "0.09882770650222174\n",
            "0.1000344815766415\n",
            "0.10118933284983915\n",
            "0.1018941064015069\n",
            "0.10290389064022952\n",
            "0.10393628668602166\n",
            "0.1056195916727071\n",
            "0.10663951799997588\n",
            "0.1075797983447609\n",
            "0.10892046991821445\n",
            "0.11015850412266334\n",
            "0.11123618018596679\n",
            "0.1124097275764436\n",
            "0.11345015317582718\n",
            "0.11453506289540655\n",
            "0.11587311140716533\n",
            "0.11664324953123127\n",
            "0.1177097738856245\n",
            "0.11873673996352174\n",
            "0.1199575691576809\n",
            "0.12109612572528518\n",
            "0.12198673436403884\n",
            "0.12316796404626364\n",
            "0.12436168417906213\n",
            "0.12552192670000178\n",
            "0.12650342579083065\n",
            "0.12775432164101955\n",
            "0.1290772273717329\n",
            "0.13024547894287597\n",
            "0.13159647080904383\n",
            "0.1325879555833919\n",
            "0.1337907966750357\n",
            "0.13492557231117697\n",
            "0.13602078464025122\n",
            "0.13727750406240868\n",
            "0.13844293028192448\n",
            "0.13965619967111845\n",
            "0.14064268199988947\n",
            "0.14165347387723606\n",
            "0.14257842393787315\n",
            "0.14399049913181977\n",
            "0.1452576157534519\n",
            "0.14617332633194105\n",
            "0.14714609898264755\n",
            "0.14804360491540425\n",
            "0.1492097751258889\n",
            "0.1503410292860797\n",
            "0.15131047733909334\n",
            "0.15237711709173743\n",
            "0.1531097863793678\n",
            "0.15390384601205206\n",
            "0.154782245378665\n",
            "0.15589077629701562\n",
            "0.15779104280044964\n",
            "0.15880047329856306\n",
            "0.15979355177306154\n",
            "0.16088599378190688\n",
            "0.1621761709223013\n",
            "0.1630458086347946\n",
            "0.1639158858362671\n",
            "0.16527319091665166\n",
            "0.16640043929409798\n",
            "0.16763621035134396\n",
            "0.1686099105326416\n",
            "0.16947876637244164\n",
            "0.17072568890993553\n",
            "0.17175924793228775\n",
            "0.17302161089294707\n",
            "0.17408319949493994\n",
            "0.17523250967035514\n",
            "0.17659153642556857\n",
            "0.17785762848756503\n",
            "0.1791101120164632\n",
            "0.18005655465833367\n",
            "0.1812390060833348\n",
            "0.1822435986203001\n",
            "0.18301416388558\n",
            "0.18414980820987537\n",
            "0.18549230664282504\n",
            "0.18658217330417975\n",
            "0.18815478895936172\n",
            "0.18961067432942597\n",
            "0.19103839856279475\n",
            "0.19236797025746397\n",
            "0.1934365541733744\n",
            "0.194465088417463\n",
            "0.19576776972817034\n",
            "0.19675516175187152\n",
            "0.19788720060492415\n",
            "0.19917459797371379\n",
            "0.20086236218052447\n",
            "0.2018925123050085\n",
            "0.20283346545056005\n",
            "0.20400561822954652\n",
            "0.20508226798013654\n",
            "0.20617722939042485\n",
            "0.20790142224877692\n",
            "0.20912353774470746\n",
            "0.21034376807225025\n",
            "0.21165769194702969\n",
            "0.2127497982795891\n",
            "0.21413675400302234\n",
            "0.21527712157620188\n",
            "0.21630567335106832\n",
            "0.21751067545407873\n",
            "0.21866060179822586\n",
            "0.2201128410713752\n",
            "0.22113142012025389\n",
            "0.22248973985157355\n",
            "0.22358321983491064\n",
            "0.22467956312782014\n",
            "0.22583490930249928\n",
            "0.2268110390209481\n",
            "0.22804131608484957\n",
            "0.22904505235764683\n",
            "0.23004938825926818\n",
            "0.23072827037642984\n",
            "0.23215607540381838\n",
            "0.23328343155743825\n",
            "0.23492159280935518\n",
            "0.23602617114706112\n",
            "0.2374014056399655\n",
            "0.2383879361402653\n",
            "0.23936713549792005\n",
            "0.24034841743576557\n",
            "0.24156488192355846\n",
            "0.24274039298981961\n",
            "0.24397752656961036\n",
            "0.24498158899109687\n",
            "0.2462316538824145\n",
            "0.24748491028995465\n",
            "0.24892465377707615\n",
            "0.24980241144099807\n",
            "0.25106056061242243\n",
            "0.2520059756458263\n",
            "0.25336813812365616\n",
            "0.25406332180628083\n",
            "0.2554006210678374\n",
            "0.25625898298400135\n",
            "0.25758223292772725\n",
            "0.258540161887703\n",
            "0.2597424022072112\n",
            "0.26075544610352774\n",
            "0.2617368741565958\n",
            "0.2627803043026449\n",
            "0.26404449458012497\n",
            "0.2649938624228358\n",
            "0.26649670710649026\n",
            "0.2676455877016268\n",
            "0.26876137491382296\n",
            "0.26982909951673445\n",
            "0.271391267438069\n",
            "0.2724997306723729\n",
            "0.27383467867551253\n",
            "0.2749037514714634\n",
            "0.276360476184684\n",
            "0.2773729265498383\n",
            "0.27859553953875665\n",
            "0.2795541058568394\n",
            "0.2806185655429235\n",
            "0.28166679020427987\n",
            "0.2828806811738807\n",
            "0.2843981732035537\n",
            "0.2856483140869824\n",
            "0.2868378881145926\n",
            "0.28793466944828666\n",
            "0.2889981195140068\n",
            "0.29033857035210064\n",
            "0.2917016741564816\n",
            "0.2931518030288579\n",
            "0.29422593718904366\n",
            "0.2952748430354516\n",
            "0.29679017908432903\n",
            "0.29782491556518825\n",
            "0.2989751153132495\n",
            "0.299799165076307\n",
            "0.3009628456114503\n",
            "0.30198093166436685\n",
            "0.3032239215910587\n",
            "0.3045087620577849\n",
            "0.30552493085336807\n",
            "0.30668995051127873\n",
            "0.30811232648542164\n",
            "0.3094442543166373\n",
            "0.3107302624856115\n",
            "0.31186435548850644\n",
            "0.31319940318841766\n",
            "0.3144691469114455\n",
            "0.315158802834923\n",
            "0.316072058647185\n",
            "0.3174317738284235\n",
            "0.3186529381653232\n",
            "0.31968970882618214\n",
            "0.3209165939132271\n",
            "0.32202341619050107\n",
            "0.32311783277470135\n",
            "0.3242622337225453\n",
            "0.3253121411861361\n",
            "0.3267121853883309\n",
            "0.32788053703734943\n",
            "0.3290045207266308\n",
            "0.33053274753758366\n",
            "0.3315020424630636\n",
            "0.33288672177687934\n",
            "0.333635243537176\n",
            "0.33537811170453613\n",
            "0.33701298814600383\n",
            "0.338083460279133\n",
            "0.33932777012095733\n",
            "0.34063467208076925\n",
            "0.3415370127734016\n",
            "0.34273489097804977\n",
            "0.3436452827185316\n",
            "0.34495764726872946\n",
            "0.3461414417037574\n",
            "0.3474688429356841\n",
            "0.34857286684348454\n",
            "0.3499365323949653\n",
            "0.3512435718570524\n",
            "0.35267000872155896\n",
            "0.35346550191454873\n",
            "0.3549390487048937\n",
            "0.3562610717990514\n",
            "0.35762035496094646\n",
            "0.35882512131310484\n",
            "0.3597376848883031\n",
            "0.3608989432034895\n",
            "0.3615710480743662\n",
            "0.36265750766715127\n",
            "0.36346060693111565\n",
            "0.3646223034395281\n",
            "0.3660616641459258\n",
            "0.3671973106044028\n",
            "0.36864620759663985\n",
            "0.36979253769225784\n",
            "0.37088146782897013\n",
            "0.37198721348782027\n",
            "0.3729416376641949\n",
            "0.37410710527159063\n",
            "0.3754841362881234\n",
            "0.376554407808177\n",
            "0.37761492832847265\n",
            "0.3788591484584467\n",
            "0.38021248792443435\n",
            "0.38137115115094977\n",
            "0.38236569763754336\n",
            "0.3832676492993484\n",
            "0.3846173717847566\n",
            "0.3856077558549164\n",
            "0.38665114423198166\n",
            "0.387937475881918\n",
            "0.38921014960769496\n",
            "0.39025374454305606\n",
            "0.39105601551587627\n",
            "0.3925965152433156\n",
            "0.3936726796962416\n",
            "0.3948825808132396\n",
            "0.3962608780092595\n",
            "0.39742357194271233\n",
            "0.3984243077085451\n",
            "0.4000471275480812\n",
            "0.4009688685617179\n",
            "0.4020378284747034\n",
            "0.4032589323685297\n",
            "0.40416436259398986\n",
            "0.4051500687666256\n",
            "0.4061570021197619\n",
            "0.4072934689424227\n",
            "0.40826499172488745\n",
            "0.4093917926863941\n",
            "0.4105036123786741\n",
            "0.41172756083176265\n",
            "0.41268615687594695\n",
            "0.41358270570445244\n",
            "0.4148644180706395\n",
            "0.4161608137590501\n",
            "0.4176150289795283\n",
            "0.4188681975807375\n",
            "0.4200280669247708\n",
            "0.42077565856297\n",
            "0.4218883626448834\n",
            "0.42278185212398733\n",
            "0.4237163752850974\n",
            "0.4245846055047896\n",
            "0.42571999487059803\n",
            "0.4268347271872908\n",
            "0.4281750868653397\n",
            "0.429013055761147\n",
            "0.4303229621914037\n",
            "0.4312477647648443\n",
            "0.4325620837681129\n",
            "0.43363500418870343\n",
            "0.4351170239850993\n",
            "0.43592839487983137\n",
            "0.43727131389900853\n",
            "0.4385135725636007\n",
            "0.4398976558309687\n",
            "0.44098816533832597\n",
            "0.44220245783896095\n",
            "0.4433543903900839\n",
            "0.4445038548363444\n",
            "0.44566046757161465\n",
            "0.4470928787727795\n",
            "0.44806148992170153\n",
            "0.44924556881265565\n",
            "0.45030609886054795\n",
            "0.45164716068436117\n",
            "0.4529770820799386\n",
            "0.4540280706589789\n",
            "0.4550156584938469\n",
            "0.4566722610569976\n",
            "0.457776971409083\n",
            "0.45881524278074887\n",
            "0.4602011213522128\n",
            "0.46160153827398936\n",
            "0.46275228688783965\n",
            "0.4637863184789867\n",
            "0.4650043355077124\n",
            "0.46602414110127616\n",
            "0.4673314331590062\n",
            "0.4686127741775854\n",
            "0.46967246061395806\n",
            "0.4705577202312782\n",
            "0.4718099907231148\n",
            "0.47304578708565753\n",
            "0.47428695404011273\n",
            "0.47520697825705\n",
            "0.4759854399182303\n",
            "0.4769054911935421\n",
            "0.47819489606506077\n",
            "0.4793108413591409\n",
            "0.480545313843071\n",
            "0.4817087219651703\n",
            "0.48281911800584526\n",
            "0.48420365204286697\n",
            "0.48551664129852334\n",
            "0.48690890046336766\n",
            "0.48804656318996265\n",
            "0.48929553271254617\n",
            "0.4909165241681706\n",
            "0.49208795086806995\n",
            "0.4933463350281386\n",
            "0.49465924653860616\n",
            "0.49601917025988057\n",
            "0.496970099790017\n",
            "0.49819508385475336\n",
            "0.49995089126060077\n",
            "0.501003639167532\n",
            "0.5021920934357607\n",
            "0.5036191853415936\n",
            "0.5047461271590894\n",
            "0.505874871750317\n",
            "0.5069757677100198\n",
            "0.5084729580318227\n",
            "0.5098883814519019\n",
            "0.5113381423303843\n",
            "0.5123488254406873\n",
            "0.513623080976174\n",
            "0.5151902689500842\n",
            "0.5165241859910433\n",
            "0.5177755565442088\n",
            "0.5187666620439886\n",
            "0.5201793210890592\n",
            "0.5212334706960127\n",
            "0.5222842036305791\n",
            "0.5231943533701056\n",
            "0.5247403705668876\n",
            "0.5259301707415325\n",
            "0.5268459415344326\n",
            "0.5279035590341329\n",
            "0.5290660637876262\n",
            "0.53017265740258\n",
            "0.5312623751285436\n",
            "0.532438277588476\n",
            "0.5336222281998686\n",
            "0.5349682769964418\n",
            "0.5360221771328041\n",
            "0.537082139945701\n",
            "0.5381857972315816\n",
            "0.5392688267371234\n",
            "0.5403247332329031\n",
            "0.5409219022983175\n",
            "0.5423995235463237\n",
            "0.5434785490221989\n",
            "0.5447181744115127\n",
            "0.5458091573260934\n",
            "0.5469143265272345\n",
            "0.5481931252781388\n",
            "0.5496254514169205\n",
            "0.5506204654417379\n",
            "0.5517898482053786\n",
            "0.5524914229236295\n",
            "0.5538546610290133\n",
            "0.5550539939833419\n",
            "0.556211984027987\n",
            "0.5570679184649606\n",
            "0.5583082597197779\n",
            "0.5592693013074758\n",
            "0.5601295618831045\n",
            "0.5614175292308373\n",
            "0.563165823097729\n",
            "0.564255078239819\n",
            "0.5655846999353155\n",
            "0.5667259454955835\n",
            "0.567712090013887\n",
            "0.5688700594789232\n",
            "0.5697991932978106\n",
            "0.5709988550685555\n",
            "0.5720446248112432\n",
            "0.5730043664536513\n",
            "0.5741785392355736\n",
            "0.5753522859433727\n",
            "0.5764347072833639\n",
            "0.5773275394345183\n",
            "0.5783579844571746\n",
            "0.5793951697590406\n",
            "0.5802977397237592\n",
            "0.5818501298918444\n",
            "0.582743146954595\n",
            "0.5838681698379005\n",
            "0.5847422486299749\n",
            "0.5859135472027542\n",
            "0.5871054530524842\n",
            "0.5883440079972567\n",
            "0.5893005418884175\n",
            "0.5904467042221133\n",
            "0.5919886724189725\n",
            "0.5933495278629806\n",
            "0.5944167296676075\n",
            "0.595531681042803\n",
            "0.5968959359333034\n",
            "0.598238356316181\n",
            "0.5999035510946723\n",
            "0.6010826062744535\n",
            "0.6023037456109396\n",
            "0.603588670072958\n",
            "0.6046522658727967\n",
            "0.6056552159283167\n",
            "0.6067204047423189\n",
            "0.6080730695020208\n",
            "0.6096989595143082\n",
            "0.6106012636209692\n",
            "0.6116713009145863\n",
            "0.6126089296911074\n",
            "0.6136249815640242\n",
            "0.614636145017641\n",
            "0.6156178393098705\n",
            "0.6165913461190661\n",
            "0.6180599038024692\n",
            "0.6191999581082702\n",
            "0.6204375887999449\n",
            "0.6215629669482751\n",
            "0.6225236014408224\n",
            "0.6239920027191986\n",
            "0.6253229719979684\n",
            "0.6262797890493023\n",
            "0.6271806506016065\n",
            "0.6281390917651793\n",
            "0.6290885126575485\n",
            "0.6302057633848142\n",
            "0.6314544633145223\n",
            "0.6324374365151081\n",
            "0.633216265560416\n",
            "0.6345716660361156\n",
            "0.6360290366441698\n",
            "0.637034884994597\n",
            "0.6380777830434272\n",
            "0.639382551431351\n",
            "0.6406818177465283\n",
            "0.641905973977445\n",
            "0.6430522291480428\n",
            "0.6447907768170852\n",
            "0.6457935647510201\n",
            "0.6471245706157611\n",
            "0.6482726147641307\n",
            "0.6494743166600957\n",
            "0.6505203636939568\n",
            "0.651431908273636\n",
            "0.6525711108885153\n",
            "0.6534742328440747\n",
            "0.6543892070536723\n",
            "0.655562577916838\n",
            "0.6572484881295573\n",
            "0.6585113314335304\n",
            "0.6594587965008548\n",
            "0.6606028615437505\n",
            "0.6616006049582416\n",
            "0.6627988736419117\n",
            "0.663907753422742\n",
            "0.6651708505418904\n",
            "0.6663690997892634\n",
            "0.6673302713333799\n",
            "0.6681812431882409\n",
            "0.6693316914160233\n",
            "0.6705165685671369\n",
            "0.6720309799055919\n",
            "0.6732450102067664\n",
            "0.6748002124640643\n",
            "0.6757972441289736\n",
            "0.676704054865081\n",
            "0.6781783836805607\n",
            "0.6792302718171683\n",
            "0.6803089053658269\n",
            "0.6816342617468456\n",
            "0.6825953000570502\n",
            "0.6837572121940305\n",
            "0.6849660477446168\n",
            "0.6859249497389854\n",
            "0.6869161024956447\n",
            "0.6881212913395499\n",
            "0.6892920361684106\n",
            "0.6907043985622313\n",
            "0.6919683530126386\n",
            "0.6932887039754702\n",
            "0.6942336917152185\n",
            "0.6958361351505264\n",
            "0.6969226964980441\n",
            "0.6978833789334578\n",
            "0.6989100334208335\n",
            "0.6999239863260932\n",
            "0.7009512024081271\n",
            "0.7023061697973925\n",
            "0.7033737192449667\n",
            "0.7047757633278132\n",
            "0.7055773143954289\n",
            "0.7065499599098854\n",
            "0.7077221001124443\n",
            "0.7093414762974395\n",
            "0.7102057566042141\n",
            "0.7113362769870197\n",
            "0.712208595841437\n",
            "0.7131634942634636\n",
            "0.7141612170983458\n",
            "0.7153757381850802\n",
            "0.7165416266835863\n",
            "0.7177202061695211\n",
            "0.718872684918706\n",
            "0.7199238514732522\n",
            "0.7210184983966296\n",
            "0.7222013998290767\n",
            "0.7232127537584061\n",
            "0.724457535940363\n",
            "0.7255311501224327\n",
            "0.726965508154591\n",
            "0.7279843704398635\n",
            "0.7294024484770377\n",
            "0.730506854860679\n",
            "0.7315393009835192\n",
            "0.7325006080481707\n",
            "0.7334759665648346\n",
            "0.7343230153364904\n",
            "0.7357756713848285\n",
            "0.7370009084263116\n",
            "0.7380739273241413\n",
            "0.7392123337368222\n",
            "0.7403650345171199\n",
            "0.741885281317984\n",
            "0.7430786026636963\n",
            "0.744270634353923\n",
            "0.7453922609157879\n",
            "0.7467347258115973\n",
            "0.7476725632805958\n",
            "0.7493162029768194\n",
            "0.7504580570837421\n",
            "0.7514486198153947\n",
            "0.7524511830504897\n",
            "0.7533916012024331\n",
            "0.754511206465609\n",
            "0.7560981828767015\n",
            "0.7575276665904028\n",
            "0.7584853969190432\n",
            "0.7596077174710496\n",
            "0.7606294403219467\n",
            "0.7617198069915747\n",
            "0.7625680376425423\n",
            "0.7639027184156506\n",
            "0.7648122331599141\n",
            "0.7663018651249464\n",
            "0.7673001683810178\n",
            "0.7686802610335752\n",
            "0.7697209293199012\n",
            "0.7709810853842884\n",
            "0.7721008943093707\n",
            "0.7731326732717817\n",
            "0.7741221481805567\n",
            "0.7752444922085613\n",
            "0.7764640395979747\n",
            "0.7774436128185228\n",
            "0.7786853298582994\n",
            "0.7797822608133717\n",
            "0.7806320428619604\n",
            "0.7820543750853794\n",
            "0.7831877574820043\n",
            "0.7844159470875854\n",
            "0.7854246576804944\n",
            "0.7862617329639547\n",
            "0.7873000371867739\n",
            "0.7881590779251455\n",
            "0.789650220585906\n",
            "0.7909311365974528\n",
            "0.7920887671849307\n",
            "0.7928873148491925\n",
            "0.7941431024723955\n",
            "0.7950996313329852\n",
            "0.7962468731814943\n",
            "0.7972285086312867\n",
            "0.7979878297318583\n",
            "0.7989328292096057\n",
            "0.8003258120144725\n",
            "0.8012701403683103\n",
            "0.802255254000654\n",
            "0.8032243086020355\n",
            "0.8045126065954833\n",
            "0.8057107183405811\n",
            "0.807070454863636\n",
            "0.8081169811737202\n",
            "0.8091376590378144\n",
            "0.810005764331659\n",
            "0.8111349126643232\n",
            "0.8122750431908976\n",
            "0.8134891761614539\n",
            "0.8144753418691323\n",
            "0.8156848710287562\n",
            "0.8168765045416629\n",
            "0.8179560273580844\n",
            "0.819105588985831\n",
            "0.8206451687285358\n",
            "0.8214752638660123\n",
            "0.8224405988936534\n",
            "0.8233950357226765\n",
            "0.824523077405932\n",
            "0.8257405684351007\n",
            "0.8269324295432366\n",
            "0.8280625405442684\n",
            "0.8293837342039704\n",
            "0.8304195672730961\n",
            "0.8314954227270068\n",
            "0.832492859352885\n",
            "0.833328443834239\n",
            "0.8343066138303493\n",
            "0.835534224462936\n",
            "0.836897514246004\n",
            "0.8380544664685988\n",
            "0.839458060119768\n",
            "0.8408155042268431\n",
            "0.8414850710984081\n",
            "0.8428597139656696\n",
            "0.843903061259738\n",
            "0.8451907100046382\n",
            "0.8461405369250671\n",
            "0.8471410825962911\n",
            "0.8480522684047899\n",
            "0.8492716823316291\n",
            "0.850242697255081\n",
            "0.8512188063939209\n",
            "0.852309200960352\n",
            "0.8534608534763536\n",
            "0.8544739633798599\n",
            "0.8555549329427807\n",
            "0.8566935462948612\n",
            "0.8577388744143879\n",
            "0.8591723490477828\n",
            "0.8603487688943249\n",
            "0.861787464498254\n",
            "0.8631152213763094\n",
            "0.8642983555869983\n",
            "0.865518775628046\n",
            "0.8668059543956577\n",
            "0.8680695789625578\n",
            "0.8687682344251887\n",
            "0.869853737752151\n",
            "0.8711075373089222\n",
            "0.872245439216304\n",
            "0.8735775089705996\n",
            "0.8747975360173399\n",
            "0.8761176651777209\n",
            "0.877495798194195\n",
            "0.8786385924081364\n",
            "0.8799618487544072\n",
            "0.8812883487519096\n",
            "0.882740459516835\n",
            "0.8838486174869415\n",
            "0.8850467060609242\n",
            "0.8859714099284633\n",
            "0.887069682978913\n",
            "0.888196763975541\n",
            "0.8894629064194687\n",
            "0.890657748903155\n",
            "0.8916208229177748\n",
            "0.8926641316441319\n",
            "0.8941549892392\n",
            "0.8953268604586496\n",
            "0.8965948232070869\n",
            "0.8979574651135813\n",
            "0.8991433143082177\n",
            "0.001090918965351856\n",
            "0.00238248057987379\n",
            "0.003421577041411339\n",
            "0.0042283384086530835\n",
            "0.005424277099502056\n",
            "0.006239227474193134\n",
            "0.007535480629757542\n",
            "0.008900431873243483\n",
            "0.009966294936206944\n",
            "0.0109977944732627\n",
            "0.012170360978607022\n",
            "0.013143797817132662\n",
            "0.014167107279648256\n",
            "0.01515588415858081\n",
            "0.015931308117059187\n",
            "0.01703582715500346\n",
            "0.018181080403535263\n",
            "0.01927372195836528\n",
            "0.020690804399797678\n",
            "0.021899275462645702\n",
            "0.023479899329602566\n",
            "0.024395913419211306\n",
            "0.026014784565362173\n",
            "0.02721471478567099\n",
            "0.028432997596233398\n",
            "0.029434449837335844\n",
            "0.030568302897236232\n",
            "0.03180546345918075\n",
            "0.03316817945226684\n",
            "0.03438659244791016\n",
            "0.035261350214633794\n",
            "0.03620728133889416\n",
            "0.03737438845512507\n",
            "0.03871803492536325\n",
            "0.040200243901718606\n",
            "0.04151482144585046\n",
            "0.0425979317454121\n",
            "0.04368116658971743\n",
            "0.04466825998042855\n",
            "0.04579079097799023\n",
            "0.046832063313945176\n",
            "0.04769464450723985\n",
            "0.04880958079072216\n",
            "0.04979979679407671\n",
            "0.05087923912136146\n",
            "0.05188686753173009\n",
            "0.052764674053167746\n",
            "0.05390957058848018\n",
            "0.05532835747884667\n",
            "0.056348601951623514\n",
            "0.05769703478154624\n",
            "0.05881168622799846\n",
            "0.060048351964682264\n",
            "0.061068900558344845\n",
            "0.06222807537869114\n",
            "0.0629867966217763\n",
            "0.06421543051824545\n",
            "0.06564732524744994\n",
            "0.06669063229695\n",
            "0.06766999911164384\n",
            "0.06859443971263174\n",
            "0.06957358655417362\n",
            "0.07081558164733145\n",
            "0.07171597040217856\n",
            "0.07251665849819817\n",
            "0.07353684763469355\n",
            "0.07464644099440416\n",
            "0.07583796543538418\n",
            "0.07671487674383862\n",
            "0.07803232034149073\n",
            "0.07923524923946547\n",
            "0.0802454232712231\n",
            "0.0811792605215936\n",
            "0.08230773163268633\n",
            "0.08330639693743128\n",
            "0.08464963776071358\n",
            "0.08569972213271938\n",
            "0.08681075880899454\n",
            "0.08792958829713904\n",
            "0.08922629847246058\n",
            "0.09040663050263739\n",
            "0.09194753892586359\n",
            "0.09330884590173316\n",
            "0.09460914455106496\n",
            "0.09542650975229795\n",
            "0.09647832258278147\n",
            "0.097503748543732\n",
            "0.09846083579770744\n",
            "0.09981351885039483\n",
            "0.1008596872277272\n",
            "0.10203400132296335\n",
            "0.10272749526726316\n",
            "0.10377320799681232\n",
            "0.10498145489436586\n",
            "0.1065254383684729\n",
            "0.10769984583415644\n",
            "0.1083843676787813\n",
            "0.1095432093381272\n",
            "0.11097420832080304\n",
            "0.11235555785391337\n",
            "0.11352544146425583\n",
            "0.11495868667312291\n",
            "0.11601601209481964\n",
            "0.11696117476124288\n",
            "0.1181213103444375\n",
            "0.11923924766842971\n",
            "0.12052421679582133\n",
            "0.12187763065328379\n",
            "0.12266647739483573\n",
            "0.12339983625180276\n",
            "0.12455276417000519\n",
            "0.12561190196925112\n",
            "0.12700411082838503\n",
            "0.12850563475847854\n",
            "0.12941135462287748\n",
            "0.13051166109111914\n",
            "0.13202424648472721\n",
            "0.13317994472315853\n",
            "0.13439737828186407\n",
            "0.13563484326957742\n",
            "0.13700136008774838\n",
            "0.1382211280600799\n",
            "0.13920818181598887\n",
            "0.14037495645720635\n",
            "0.14153302904894893\n",
            "0.142466597392431\n",
            "0.14368256339636606\n",
            "0.14510758270692947\n",
            "0.14623144398564877\n",
            "0.14714530018894265\n",
            "0.14891234497584954\n",
            "0.15004613340053413\n",
            "0.15148533922632026\n",
            "0.15279394365332621\n",
            "0.15399378439044709\n",
            "0.15512233599067649\n",
            "0.15628152232035955\n",
            "0.1570397069692002\n",
            "0.15821528099382015\n",
            "0.1592907989421464\n",
            "0.16059537327198117\n",
            "0.1615397823436181\n",
            "0.16276384177415268\n",
            "0.16387662748851434\n",
            "0.16495790041011313\n",
            "0.1660169903427134\n",
            "0.166890968523367\n",
            "0.16851684755986304\n",
            "0.16986034418006077\n",
            "0.17124574637169118\n",
            "0.17218801242006404\n",
            "0.1732530537468698\n",
            "0.17440576717981596\n",
            "0.17544411225696963\n",
            "0.17660720734035268\n",
            "0.1777110230130003\n",
            "0.17851956687924808\n",
            "0.17936743212782819\n",
            "0.18043457059299245\n",
            "0.18151096873880956\n",
            "0.18279579548579653\n",
            "0.18421197730257077\n",
            "0.1855096987751134\n",
            "0.18716163037683042\n",
            "0.18854596090438724\n",
            "0.1896362104989074\n",
            "0.1909867708030564\n",
            "0.19188806269784717\n",
            "0.19305034534400686\n",
            "0.1944900801419602\n",
            "0.1956686365330006\n",
            "0.19681118546849322\n",
            "0.1979808383585547\n",
            "0.19911265114079352\n",
            "0.20080454331224837\n",
            "0.20199860186528062\n",
            "0.20308455535213052\n",
            "0.20436089964169066\n",
            "0.20570592936652396\n",
            "0.20677129082057788\n",
            "0.2079836524965818\n",
            "0.20918460133130592\n",
            "0.21049467178866688\n",
            "0.2119090375692948\n",
            "0.21294458427697496\n",
            "0.2138631698268149\n",
            "0.2150028345682432\n",
            "0.2162452427779927\n",
            "0.21765130582977743\n",
            "0.2185114252445338\n",
            "0.21943709589636234\n",
            "0.22079026615223313\n",
            "0.22170780237068605\n",
            "0.22297082479347657\n",
            "0.22385058363380334\n",
            "0.22491850016062218\n",
            "0.22599501286626167\n",
            "0.22683416729997796\n",
            "0.22790809147193303\n",
            "0.2289125658666996\n",
            "0.23006334024317124\n",
            "0.23109026500940932\n",
            "0.23234129073979604\n",
            "0.23353298042741272\n",
            "0.23455853687832728\n",
            "0.23541489098687915\n",
            "0.23672486662559802\n",
            "0.23807082624386644\n",
            "0.23953325288070132\n",
            "0.2407585877134367\n",
            "0.2420156611048657\n",
            "0.24329597070393966\n",
            "0.2442431225801063\n",
            "0.24543023078947726\n",
            "0.24646490248267913\n",
            "0.24741758143200593\n",
            "0.2487564788907385\n",
            "0.24989200575882212\n",
            "0.25077308634357987\n",
            "0.2518085723795244\n",
            "0.253326312431594\n",
            "0.25418412243313804\n",
            "0.25510732612341563\n",
            "0.2562116411945704\n",
            "0.2575292256482117\n",
            "0.2587549447860864\n",
            "0.2601490332494916\n",
            "0.2613348621694023\n",
            "0.262291518166242\n",
            "0.26313905322643194\n",
            "0.2642626800500523\n",
            "0.26524625272702074\n",
            "0.26649545007349584\n",
            "0.2676465977030947\n",
            "0.2690307513984573\n",
            "0.27074423950651416\n",
            "0.2724177780206246\n",
            "0.2736638261534064\n",
            "0.27502332944089497\n",
            "0.27595695456885316\n",
            "0.27676275540190887\n",
            "0.2779771346417839\n",
            "0.2794625982451622\n",
            "0.2808215844509242\n",
            "0.28187566660249325\n",
            "0.28299200664395874\n",
            "0.2842613723881714\n",
            "0.28538876588997025\n",
            "0.2865116627472441\n",
            "0.2874454951194851\n",
            "0.28825066278657646\n",
            "0.29008389494913006\n",
            "0.2913374426724661\n",
            "0.2922539031109237\n",
            "0.29328290984758637\n",
            "0.29439229451481946\n",
            "0.2956958756879772\n",
            "0.29682776293791163\n",
            "0.29798750682255193\n",
            "0.2993549927116355\n",
            "0.3002445491988336\n",
            "0.30095533001453373\n",
            "0.3019880602121963\n",
            "0.3029412182090837\n",
            "0.3039806957744881\n",
            "0.3053474464379918\n",
            "0.3067153882797417\n",
            "0.3077956643860663\n",
            "0.3089545405734226\n",
            "0.31036273521535535\n",
            "0.311810454901527\n",
            "0.3129221217711563\n",
            "0.3140777716093966\n",
            "0.31517317959719604\n",
            "0.31626731302122324\n",
            "0.31705473938866346\n",
            "0.31808428919833637\n",
            "0.3192406154959403\n",
            "0.3203268185296022\n",
            "0.3215321710195078\n",
            "0.32244922094942663\n",
            "0.323760052699872\n",
            "0.32495199254406687\n",
            "0.32617948068987074\n",
            "0.3272754296164988\n",
            "0.3288079157967092\n",
            "0.3297149818724074\n",
            "0.3307648362863399\n",
            "0.3319758124211255\n",
            "0.3333769487145612\n",
            "0.33455989084890125\n",
            "0.3353758478713462\n",
            "0.33677586112790703\n",
            "0.3379219301673762\n",
            "0.33912292923159004\n",
            "0.3396668226822563\n",
            "0.3406749840282723\n",
            "0.3418702079207086\n",
            "0.3434305585863645\n",
            "0.344419380892878\n",
            "0.3453802064709041\n",
            "0.34636951720013337\n",
            "0.34728167634790813\n",
            "0.34836491378371975\n",
            "0.34947784058273296\n",
            "0.35058831825585623\n",
            "0.3516448756008197\n",
            "0.3524934993985364\n",
            "0.3535323388436261\n",
            "0.35452014360281514\n",
            "0.35560758994973224\n",
            "0.3569736848097018\n",
            "0.3577195025618424\n",
            "0.3588388587355309\n",
            "0.3603877445773395\n",
            "0.3614757632660439\n",
            "0.3627736990714012\n",
            "0.3641593294680271\n",
            "0.36547914429393874\n",
            "0.3667169156129403\n",
            "0.3682296255512921\n",
            "0.36920045701134235\n",
            "0.3703341797337203\n",
            "0.3716744064827404\n",
            "0.37269849354958595\n",
            "0.37365913292026276\n",
            "0.3749457279891919\n",
            "0.3759775692239747\n",
            "0.37711705667588413\n",
            "0.3780043741778644\n",
            "0.37901222225650194\n",
            "0.38014238638341274\n",
            "0.38166126029570696\n",
            "0.38284777505013645\n",
            "0.383874671081143\n",
            "0.38489417956613214\n",
            "0.38576814738076054\n",
            "0.38682486395091964\n",
            "0.3881899401964739\n",
            "0.38925088305607475\n",
            "0.39010177030587745\n",
            "0.3912691696525535\n",
            "0.3924456624423756\n",
            "0.39369478082412956\n",
            "0.39475064509360075\n",
            "0.39553247380744466\n",
            "0.39643041366506415\n",
            "0.3974338831651546\n",
            "0.3986394227008381\n",
            "0.3997601356042925\n",
            "0.400895459725119\n",
            "0.40173361307519784\n",
            "0.4028440239789236\n",
            "0.40365474059453704\n",
            "0.40459988855035106\n",
            "0.4059421545099419\n",
            "0.4070301802871782\n",
            "0.4079363874309813\n",
            "0.40904580487314696\n",
            "0.40986138041062126\n",
            "0.41116354052368026\n",
            "0.4123134237268697\n",
            "0.41346006579411304\n",
            "0.41455037712745957\n",
            "0.41544859732508355\n",
            "0.41658223506129916\n",
            "0.4175927138237087\n",
            "0.41894970914287033\n",
            "0.42039964289006676\n",
            "0.42157086417498185\n",
            "0.4227201956922136\n",
            "0.4238001681349771\n",
            "0.42487477265355533\n",
            "0.4258060530780831\n",
            "0.4268866264454239\n",
            "0.4278375673324556\n",
            "0.4288049932483517\n",
            "0.42990387842783234\n",
            "0.4309082706566052\n",
            "0.4326114713993219\n",
            "0.43379346168864413\n",
            "0.4348365428959927\n",
            "0.43591020380139656\n",
            "0.4370027274426902\n",
            "0.4383446103166741\n",
            "0.43977517331652627\n",
            "0.44065468543020964\n",
            "0.4418573829981372\n",
            "0.44272143182242313\n",
            "0.4439315350769121\n",
            "0.44515766161482045\n",
            "0.44683239267915104\n",
            "0.4477473697851381\n",
            "0.44856411988473\n",
            "0.4493418042464634\n",
            "0.4505706007218422\n",
            "0.45167999903259376\n",
            "0.45273842645423185\n",
            "0.45394455913997367\n",
            "0.45483204051661674\n",
            "0.4557624232128758\n",
            "0.4566576323850685\n",
            "0.45790020972871415\n",
            "0.45890119976704685\n",
            "0.4601283594775383\n",
            "0.46127768809838066\n",
            "0.46272643188686324\n",
            "0.46392809689197395\n",
            "0.4648973591187421\n",
            "0.4661095451058634\n",
            "0.46725561795637127\n",
            "0.4683517455444921\n",
            "0.4695280482397055\n",
            "0.47069686201527294\n",
            "0.4719684223079925\n",
            "0.4731633259207391\n",
            "0.47413951676824817\n",
            "0.47531389984328426\n",
            "0.4762144386768341\n",
            "0.47750598138860423\n",
            "0.4782325402092751\n",
            "0.4794553051061947\n",
            "0.48065186987447617\n",
            "0.4814916380378596\n",
            "0.4824097215214654\n",
            "0.48375312210348864\n",
            "0.4847318655846979\n",
            "0.4860086159785385\n",
            "0.4871711004573061\n",
            "0.4887144558722406\n",
            "0.4897108775423006\n",
            "0.49099767383407145\n",
            "0.4918669461441772\n",
            "0.4931678191932571\n",
            "0.4939743799474233\n",
            "0.4951976465294733\n",
            "0.49619322611242916\n",
            "0.4975287018682036\n",
            "0.4989385684890211\n",
            "0.5003641094545574\n",
            "0.5015104552516547\n",
            "0.503008460983291\n",
            "0.5042527565718307\n",
            "0.5054357070142351\n",
            "0.506899642060175\n",
            "0.5082730301810653\n",
            "0.5092812392412854\n",
            "0.510278836845437\n",
            "0.5121352373791472\n",
            "0.513713685600349\n",
            "0.515019731600876\n",
            "0.5159284021238537\n",
            "0.5172788025168202\n",
            "0.5186668861552578\n",
            "0.5197899327863513\n",
            "0.5209837938513597\n",
            "0.5224643049337675\n",
            "0.5236645688486221\n",
            "0.5248213478213991\n",
            "0.5259774608532791\n",
            "0.5273752135541433\n",
            "0.5285266365693964\n",
            "0.5296714065020042\n",
            "0.5309243176294409\n",
            "0.5317320517261924\n",
            "0.5329628313898735\n",
            "0.5339026883282625\n",
            "0.5355474472503223\n",
            "0.5367518454561453\n",
            "0.5383639177093116\n",
            "0.5393786349564867\n",
            "0.5404342004405264\n",
            "0.5414225828007359\n",
            "0.5425955354405181\n",
            "0.5441385123430921\n",
            "0.5450411643975835\n",
            "0.5464553225528249\n",
            "0.5475336731699727\n",
            "0.5485910761844167\n",
            "0.5494522869282061\n",
            "0.5507926866983819\n",
            "0.5521534500676958\n",
            "0.5536258716869842\n",
            "0.5547376279635807\n",
            "0.5555543112937752\n",
            "0.5567597292573251\n",
            "0.5580071133116017\n",
            "0.5591288721165084\n",
            "0.5601606803476963\n",
            "0.5611619740495901\n",
            "0.562565126534923\n",
            "0.5636411938825836\n",
            "0.5650757724976601\n",
            "0.5661294566243505\n",
            "0.5677969787279358\n",
            "0.5689994796462681\n",
            "0.5698910468374677\n",
            "0.571035691844228\n",
            "0.5720382334326234\n",
            "0.573286264601266\n",
            "0.574451940291373\n",
            "0.5758308656990071\n",
            "0.5771411178666918\n",
            "0.5784064423092796\n",
            "0.5795683764740635\n",
            "0.5807670692501166\n",
            "0.5815997642781728\n",
            "0.5828198852289058\n",
            "0.5840318424774863\n",
            "0.5851147273160002\n",
            "0.5861493444351284\n",
            "0.5870581462864986\n",
            "0.5882131142537003\n",
            "0.5897361257344561\n",
            "0.5907941359998016\n",
            "0.5918892427631046\n",
            "0.5931981677746834\n",
            "0.5941573653532111\n",
            "0.5959370390076162\n",
            "0.597366019054447\n",
            "0.5986246389653677\n",
            "0.5996674949403309\n",
            "0.6012520019508079\n",
            "0.6024741640938517\n",
            "0.6034906572850464\n",
            "0.6044948502727177\n",
            "0.6057556058897082\n",
            "0.6068136563234012\n",
            "0.6079270309956787\n",
            "0.6093290035834398\n",
            "0.6107328377873696\n",
            "0.6115629724834276\n",
            "0.6124359026284474\n",
            "0.6135583033647074\n",
            "0.615134446822164\n",
            "0.6161862555367258\n",
            "0.6174110407414644\n",
            "0.618458958233104\n",
            "0.6194006233568996\n",
            "0.6205804611715819\n",
            "0.6221922416516277\n",
            "0.6231885521918001\n",
            "0.624433021944807\n",
            "0.6253868461112537\n",
            "0.6264096734773777\n",
            "0.6275824453214855\n",
            "0.6287386794681744\n",
            "0.6298948469216866\n",
            "0.6308294401296874\n",
            "0.6318975525438938\n",
            "0.6330011209563526\n",
            "0.6343444165061501\n",
            "0.6360916731607579\n",
            "0.6369416991158214\n",
            "0.6380375155707454\n",
            "0.6395155997837291\n",
            "0.6405195893381562\n",
            "0.6412617903383796\n",
            "0.6426750675339223\n",
            "0.643790420059048\n",
            "0.6448264213474205\n",
            "0.6459515119147727\n",
            "0.6470414185158128\n",
            "0.6479308921510302\n",
            "0.6487925214230862\n",
            "0.6499523863463146\n",
            "0.6513578725592865\n",
            "0.652766113695891\n",
            "0.6538175582275976\n",
            "0.6547161654743088\n",
            "0.6558860262184192\n",
            "0.6569836585570479\n",
            "0.6581935281948665\n",
            "0.6592080760032625\n",
            "0.660362361260997\n",
            "0.6613100277036047\n",
            "0.6623486814748906\n",
            "0.6637193578131059\n",
            "0.6648822735490092\n",
            "0.6662132329190783\n",
            "0.6676230568562627\n",
            "0.6687043197929402\n",
            "0.6698224457633465\n",
            "0.6706971465169317\n",
            "0.6722278328197996\n",
            "0.6730987459344937\n",
            "0.6740178791305903\n",
            "0.6749809964386093\n",
            "0.6762083795521875\n",
            "0.6773076025421357\n",
            "0.6784077144187429\n",
            "0.6801241705637149\n",
            "0.6813733535044638\n",
            "0.6829561484439294\n",
            "0.6844115415802392\n",
            "0.685541036793643\n",
            "0.6864818233968047\n",
            "0.6874450367429982\n",
            "0.6885715207785291\n",
            "0.6900239673721821\n",
            "0.6912486427427863\n",
            "0.6922250617190701\n",
            "0.6931980241595022\n",
            "0.6941811692379319\n",
            "0.6952136909717794\n",
            "0.6964812209386655\n",
            "0.6975334766118423\n",
            "0.6984809616490093\n",
            "0.6993192354279101\n",
            "0.7005618216893862\n",
            "0.7016864735299669\n",
            "0.7027510098941491\n",
            "0.7039828074862585\n",
            "0.7050353665181133\n",
            "0.7064367174492467\n",
            "0.7074693166996207\n",
            "0.7085500810762195\n",
            "0.7096928262802036\n",
            "0.7110670606041198\n",
            "0.7123805469716601\n",
            "0.7137087537047199\n",
            "0.7144527701313234\n",
            "0.7156090167019983\n",
            "0.7166315465784439\n",
            "0.7178658151718051\n",
            "0.7192587677932456\n",
            "0.7203558409762809\n",
            "0.7214113675877262\n",
            "0.7226015766105993\n",
            "0.7235018141434321\n",
            "0.7242928063473129\n",
            "0.725630029083213\n",
            "0.7266621902165815\n",
            "0.7278953279985492\n",
            "0.729149143485462\n",
            "0.7300893363287991\n",
            "0.7311941125356328\n",
            "0.7323353801999251\n",
            "0.7331535030356453\n",
            "0.7343415958649667\n",
            "0.7355898132409586\n",
            "0.7366702832529307\n",
            "0.7374249140319922\n",
            "0.7382994160018004\n",
            "0.7394365371035798\n",
            "0.7401670800602954\n",
            "0.7415385789730969\n",
            "0.7425923747632205\n",
            "0.7434943351141937\n",
            "0.7444157397655575\n",
            "0.745277578416078\n",
            "0.7467855637335716\n",
            "0.7476579987484476\n",
            "0.7488876603296041\n",
            "0.7499993868038782\n",
            "0.7511323239949658\n",
            "0.7521486469089528\n",
            "0.7531975946005653\n",
            "0.7542364995955201\n",
            "0.7555533925743054\n",
            "0.7567846722462598\n",
            "0.7581427890778808\n",
            "0.7588694677176073\n",
            "0.7600406521878889\n",
            "0.7610783384888983\n",
            "0.7622003768716017\n",
            "0.7634508728676135\n",
            "0.764524221115405\n",
            "0.7657330455377583\n",
            "0.7667568313038867\n",
            "0.7679427505454139\n",
            "0.7693859052170268\n",
            "0.7703993985110231\n",
            "0.7713549537274539\n",
            "0.7722989862684704\n",
            "0.7733348063038438\n",
            "0.7744333049677827\n",
            "0.7756441807960306\n",
            "0.7767232832548868\n",
            "0.777832320812718\n",
            "0.7792010297403311\n",
            "0.7802631165212988\n",
            "0.7816800760948445\n",
            "0.7832638617519223\n",
            "0.7844639105717545\n",
            "0.7852894202675051\n",
            "0.7866328735943036\n",
            "0.7880517882306862\n",
            "0.789207285856042\n",
            "0.7909812382267564\n",
            "0.7921646887536549\n",
            "0.7933758701509832\n",
            "0.7945155800151094\n",
            "0.7957374529003183\n",
            "0.7968582613846226\n",
            "0.7978739960266806\n",
            "0.7989249172265572\n",
            "0.8002364479214944\n",
            "0.80161894320527\n",
            "0.8027778021667314\n",
            "0.8037894303384034\n",
            "0.8050162434730384\n",
            "0.806199130271097\n",
            "0.8076246608706081\n",
            "0.8087800501100243\n",
            "0.8100405311035683\n",
            "0.8110703365577151\n",
            "0.8121043599170187\n",
            "0.8133486712070377\n",
            "0.8144538431521267\n",
            "0.8156414506075632\n",
            "0.816722356602359\n",
            "0.8178641783916737\n",
            "0.818896067874206\n",
            "0.8197895497312326\n",
            "0.8209292759828251\n",
            "0.8222796666195326\n",
            "0.8237683851548168\n",
            "0.8252062179395915\n",
            "0.8263059308004501\n",
            "0.8272052554370802\n",
            "0.8279598027543948\n",
            "0.8290722952474414\n",
            "0.8302538830148595\n",
            "0.8313616039350514\n",
            "0.8327121715563948\n",
            "0.8339476601394546\n",
            "0.8354834462218272\n",
            "0.8369210645213456\n",
            "0.8382887017848851\n",
            "0.8391310861500938\n",
            "0.8402835598687077\n",
            "0.8416865332352231\n",
            "0.8435664567191278\n",
            "0.8445814033908308\n",
            "0.8463074514628066\n",
            "0.8476328110450979\n",
            "0.8490380327719862\n",
            "0.850102029797976\n",
            "0.8516671354203578\n",
            "0.8530199166454012\n",
            "0.8541083145324532\n",
            "0.8549953539048314\n",
            "0.8556994179935407\n",
            "0.857026932337095\n",
            "0.8578842907305568\n",
            "0.859556835173341\n",
            "0.8608969161882425\n",
            "0.8619857393872098\n",
            "0.8634681299214473\n",
            "0.8644394252611243\n",
            "0.865781418197905\n",
            "0.8665905628362884\n",
            "0.8679092864856086\n",
            "0.8692918971676351\n",
            "0.8702434757176567\n",
            "0.8716335423157343\n",
            "0.8731854626589723\n",
            "0.8744885881843469\n",
            "0.8757142257659941\n",
            "0.8767367445904276\n",
            "0.8775446427142833\n",
            "0.8788378764601315\n",
            "0.8798903528686679\n",
            "0.8808452700410048\n",
            "0.8822519355417823\n",
            "0.883351182236391\n",
            "0.8849233289813752\n",
            "0.8862764234737972\n",
            "0.8872051789327655\n",
            "0.888184386446043\n",
            "0.8890840537712702\n",
            "0.8900981024860422\n",
            "0.8911229069428066\n",
            "0.8920745778724056\n",
            "0.8930559145367664\n",
            "0.8942478434813906\n",
            "0.8954272869297916\n",
            "0.8967424478677227\n",
            "0.8980920057162605\n",
            "0.8991455646122203\n",
            "0.9005177075905568\n",
            "0.000882252707810658\n",
            "0.001993084883750857\n",
            "0.003401294160072151\n",
            "0.004384807384837314\n",
            "0.00577975363682603\n",
            "0.006966952320254977\n",
            "0.008119877037184928\n",
            "0.009437469722669753\n",
            "0.010370197503463081\n",
            "0.011320220235058718\n",
            "0.012609102079630508\n",
            "0.013831209191275985\n",
            "0.015068260528852262\n",
            "0.01623178016194297\n",
            "0.0172420715736916\n",
            "0.018471295114063545\n",
            "0.019576422851103954\n",
            "0.020934410869617903\n",
            "0.02264157478766673\n",
            "0.02365242718430736\n",
            "0.024849847759432196\n",
            "0.025938616384325736\n",
            "0.02703490098723975\n",
            "0.02825765384127722\n",
            "0.02932115184986378\n",
            "0.030512260125421197\n",
            "0.03160114460589026\n",
            "0.03239961475362558\n",
            "0.03352350377670639\n",
            "0.03529016608777254\n",
            "0.036627693096999925\n",
            "0.03763649492617458\n",
            "0.03861386490904767\n",
            "0.03980101176235072\n",
            "0.04082130631217566\n",
            "0.04235166486571817\n",
            "0.04360851287232031\n",
            "0.04460285623055285\n",
            "0.045390723077842345\n",
            "0.046596703550699725\n",
            "0.047604393090128594\n",
            "0.04875448849195105\n",
            "0.049716586956892475\n",
            "0.050861353154682445\n",
            "0.0521816428360122\n",
            "0.05347947726774094\n",
            "0.05459369235026562\n",
            "0.055981656169647454\n",
            "0.057160553343765574\n",
            "0.058024426631610414\n",
            "0.05902231669486941\n",
            "0.060240408953498394\n",
            "0.061639447651250893\n",
            "0.06239979666517214\n",
            "0.06360873511380247\n",
            "0.0650094421318425\n",
            "0.06598713163219755\n",
            "0.0669986751988111\n",
            "0.0678599179553254\n",
            "0.06919551581677878\n",
            "0.07041680660394146\n",
            "0.07122930266973003\n",
            "0.07251060832187038\n",
            "0.07378595960719506\n",
            "0.07472954526581728\n",
            "0.07583149657834826\n",
            "0.07698558701578613\n",
            "0.0782660803069239\n",
            "0.07923792001536435\n",
            "0.08030817598638022\n",
            "0.08149090515987953\n",
            "0.08276598532791332\n",
            "0.08390216319762228\n",
            "0.08479332016861957\n",
            "0.08590371255069743\n",
            "0.08684659446291912\n",
            "0.08794525547710526\n",
            "0.08931411013883703\n",
            "0.0901757828567339\n",
            "0.09120509318073693\n",
            "0.09245088795566803\n",
            "0.09345099352814658\n",
            "0.09444278570087365\n",
            "0.09540792628932182\n",
            "0.09644029878289498\n",
            "0.0974522672040993\n",
            "0.09909553799177985\n",
            "0.10083154964325068\n",
            "0.10217192243127261\n",
            "0.10335375372406162\n",
            "0.10424172573382287\n",
            "0.10552959003107017\n",
            "0.10657085070524679\n",
            "0.10762076723910964\n",
            "0.10888078206640375\n",
            "0.10994788402181757\n",
            "0.11091433903750252\n",
            "0.11277104262500773\n",
            "0.11403213307985564\n",
            "0.11551635489439416\n",
            "0.11674230444766677\n",
            "0.1182090778027654\n",
            "0.1193135223730141\n",
            "0.12036191929331826\n",
            "0.12139141551978752\n",
            "0.12249643936791384\n",
            "0.12341014892243973\n",
            "0.12449187573874393\n",
            "0.12558812108795966\n",
            "0.12684955339297613\n",
            "0.12760098533862083\n",
            "0.12874125527298969\n",
            "0.1296099678177358\n",
            "0.13062649698513548\n",
            "0.13213265239430205\n",
            "0.13325549681168383\n",
            "0.13417223050161395\n",
            "0.13511140907511993\n",
            "0.13616734712629977\n",
            "0.13783003286937315\n",
            "0.1387460500840336\n",
            "0.14033771315803917\n",
            "0.14215361043010527\n",
            "0.14320500915312706\n",
            "0.14420159485029138\n",
            "0.14520798238646954\n",
            "0.1466213707118998\n",
            "0.14763154352412505\n",
            "0.1487121604897482\n",
            "0.15012348932988198\n",
            "0.15153469118620733\n",
            "0.15255875431973\n",
            "0.15349686587862957\n",
            "0.1545619725266381\n",
            "0.15595374586027297\n",
            "0.15674972137831666\n",
            "0.15793448457937412\n",
            "0.15902319649601226\n",
            "0.16015006544644875\n",
            "0.16120668506378408\n",
            "0.1621395619324101\n",
            "0.16323739503655593\n",
            "0.1643898224891604\n",
            "0.16544334975349934\n",
            "0.1665607914138023\n",
            "0.16762617565786747\n",
            "0.16863567101985902\n",
            "0.16952881476153497\n",
            "0.17055936192002746\n",
            "0.17172050323632673\n",
            "0.1727257523390338\n",
            "0.17389919370641488\n",
            "0.17519093581172815\n",
            "0.1760430565423063\n",
            "0.17714459984503744\n",
            "0.1781515881533513\n",
            "0.179143880563014\n",
            "0.18036156732712866\n",
            "0.1818929080615568\n",
            "0.18293203714558537\n",
            "0.18373994632145327\n",
            "0.18499041388711662\n",
            "0.1858236086185631\n",
            "0.18689886459609126\n",
            "0.18816784550161922\n",
            "0.1893717491870646\n",
            "0.1903853219793276\n",
            "0.19174575592245896\n",
            "0.19248577708478473\n",
            "0.19353846340533107\n",
            "0.19492340126001012\n",
            "0.1962479289687808\n",
            "0.19768983438191817\n",
            "0.19896051943149712\n",
            "0.19977341382704733\n",
            "0.20104435048139918\n",
            "0.20238398133641314\n",
            "0.20368954501188624\n",
            "0.20502364726932457\n",
            "0.20607981184864288\n",
            "0.2072559604254525\n",
            "0.20814255009526791\n",
            "0.20900363591320983\n",
            "0.21030845018603916\n",
            "0.2115657355474389\n",
            "0.21253655725123022\n",
            "0.2137427006841011\n",
            "0.21470804417224795\n",
            "0.21606517577415232\n",
            "0.21709306503805664\n",
            "0.21774422962342382\n",
            "0.21906602375037834\n",
            "0.22020051310129482\n",
            "0.22143739881113056\n",
            "0.2225655845516478\n",
            "0.22402989445135113\n",
            "0.22499802015016757\n",
            "0.2260388571892858\n",
            "0.2271681495029908\n",
            "0.22809863151491755\n",
            "0.2295204602239077\n",
            "0.23061182142218664\n",
            "0.2316566987720597\n",
            "0.2330097312207722\n",
            "0.23410925840782693\n",
            "0.23520207145939703\n",
            "0.23638899994018436\n",
            "0.23757101972694591\n",
            "0.23862147232150788\n",
            "0.2400352265066503\n",
            "0.2412846408536672\n",
            "0.24232028512393727\n",
            "0.24371926375972036\n",
            "0.24492070871545835\n",
            "0.24620994574883404\n",
            "0.24709401982824516\n",
            "0.24814916731756362\n",
            "0.24969441651383326\n",
            "0.25076754173964183\n",
            "0.25149945415499264\n",
            "0.2524598560979604\n",
            "0.253713453745903\n",
            "0.25483216966509514\n",
            "0.25628775472531234\n",
            "0.257747406773555\n",
            "0.2589563358470302\n",
            "0.2598051878497424\n",
            "0.2607874368005397\n",
            "0.26210077194606557\n",
            "0.2628400146656329\n",
            "0.26406506443267586\n",
            "0.26503702777121074\n",
            "0.26589978617780347\n",
            "0.26694741768910146\n",
            "0.26793609593835327\n",
            "0.26920546351186453\n",
            "0.2705815842999217\n",
            "0.27184874673023857\n",
            "0.27302982877282533\n",
            "0.2741614513842346\n",
            "0.27530507701437185\n",
            "0.2765951454639435\n",
            "0.27774575055407746\n",
            "0.2790946923863247\n",
            "0.2805585373393105\n",
            "0.2818817424652217\n",
            "0.2831243900844203\n",
            "0.2840799262456577\n",
            "0.28531748742398705\n",
            "0.28675906127676026\n",
            "0.28796295230955726\n",
            "0.2894250501299758\n",
            "0.2911707073678751\n",
            "0.2924165313353624\n",
            "0.2932568838071945\n",
            "0.2944228100349836\n",
            "0.2956262652373985\n",
            "0.2967042470222239\n",
            "0.2974313359583735\n",
            "0.29842900406674044\n",
            "0.29932923542569057\n",
            "0.3012374880368752\n",
            "0.3023088056108226\n",
            "0.30387128420802945\n",
            "0.304988400100747\n",
            "0.306148982871219\n",
            "0.3072211268307913\n",
            "0.3085157323981185\n",
            "0.3094985405045092\n",
            "0.31074430288561167\n",
            "0.3119080180249861\n",
            "0.31335002603128437\n",
            "0.31446013235679976\n",
            "0.31543096762788875\n",
            "0.3169699450740424\n",
            "0.31787845294188966\n",
            "0.31849275860944976\n",
            "0.31955566003804314\n",
            "0.3206409077967524\n",
            "0.32153867493809946\n",
            "0.3226373111043135\n",
            "0.3236655820818508\n",
            "0.32496612936334535\n",
            "0.32602519078937636\n",
            "0.3272705087271493\n",
            "0.32861191903233833\n",
            "0.3295333913677489\n",
            "0.33104532736036785\n",
            "0.33237453182335097\n",
            "0.33325939478776645\n",
            "0.33454566622329185\n",
            "0.33569189913742375\n",
            "0.33695857192549256\n",
            "0.33809548098108044\n",
            "0.33943344732684555\n",
            "0.3406207441064098\n",
            "0.34199438985351405\n",
            "0.34320069639883993\n",
            "0.34459753048694347\n",
            "0.34592207328742725\n",
            "0.34707877916448254\n",
            "0.34861906760793815\n",
            "0.349707653836521\n",
            "0.3511996584017868\n",
            "0.3524850918661298\n",
            "0.35375337984860705\n",
            "0.3549932038692562\n",
            "0.3561134684421217\n",
            "0.3571348205551772\n",
            "0.3581724813222275\n",
            "0.3595010805922701\n",
            "0.361043344830613\n",
            "0.3619965006933188\n",
            "0.36310354172421233\n",
            "0.36405069832606696\n",
            "0.3652453039155897\n",
            "0.3663749419667227\n",
            "0.36755460980908033\n",
            "0.36889613154904005\n",
            "0.3702139417686121\n",
            "0.37163123869530074\n",
            "0.3731389162333115\n",
            "0.37446340430728003\n",
            "0.37557554244995117\n",
            "0.3767418586994376\n",
            "0.37767319720419473\n",
            "0.3789408722954333\n",
            "0.379779679162423\n",
            "0.3806568743170375\n",
            "0.38188966720000556\n",
            "0.38301697152349956\n",
            "0.38426869749413123\n",
            "0.3856729952728047\n",
            "0.3871905115406836\n",
            "0.3885276842757564\n",
            "0.3899032844759314\n",
            "0.3908618846360375\n",
            "0.3922293418661103\n",
            "0.39338099659251435\n",
            "0.39432263313351995\n",
            "0.3953452727678792\n",
            "0.3964298247833691\n",
            "0.39788804357618934\n",
            "0.39878915299844864\n",
            "0.3998953911197155\n",
            "0.40076014247086955\n",
            "0.4016007859536144\n",
            "0.4026971350393027\n",
            "0.4039415519713136\n",
            "0.4051389263566498\n",
            "0.4060953530051824\n",
            "0.40738984546088197\n",
            "0.40830452675404755\n",
            "0.40927754224413804\n",
            "0.41016995441883114\n",
            "0.41114469455636066\n",
            "0.4121543836715581\n",
            "0.4132012155507227\n",
            "0.4148618875409636\n",
            "0.4162692126563138\n",
            "0.4173165725167755\n",
            "0.4188370542300632\n",
            "0.4200513166234926\n",
            "0.4211799564873776\n",
            "0.42246307070602845\n",
            "0.42331638680699535\n",
            "0.42406193595713054\n",
            "0.42478440934434875\n",
            "0.42575740265419415\n",
            "0.42681022419039244\n",
            "0.42807839014341154\n",
            "0.42907728426291813\n",
            "0.4301510322124452\n",
            "0.43165858733989393\n",
            "0.4326973195423556\n",
            "0.43344532048610773\n",
            "0.4344917927556636\n",
            "0.436085257231427\n",
            "0.437084097386626\n",
            "0.43819164452345477\n",
            "0.4389132727747378\n",
            "0.439760845716652\n",
            "0.4408690260956659\n",
            "0.44206824593836697\n",
            "0.4434237569341879\n",
            "0.44477430748207797\n",
            "0.4457102074190174\n",
            "0.4467837010960445\n",
            "0.44777024272457716\n",
            "0.4486628549025797\n",
            "0.4498576281778038\n",
            "0.4507464332043972\n",
            "0.45215539310289465\n",
            "0.45346555350076817\n",
            "0.45466892813782556\n",
            "0.45575367718401466\n",
            "0.45680233028233813\n",
            "0.45818585256481414\n",
            "0.45905205287287\n",
            "0.45997147265907445\n",
            "0.4611081813302491\n",
            "0.4621576925220392\n",
            "0.46333934934547794\n",
            "0.46421830687681426\n",
            "0.46528776977068326\n",
            "0.466770335460258\n",
            "0.46772940559765264\n",
            "0.468914758061509\n",
            "0.4700209002208222\n",
            "0.4710976347289122\n",
            "0.47205709023853704\n",
            "0.4732865258251005\n",
            "0.47403079461868464\n",
            "0.47507837963531085\n",
            "0.4763089844485378\n",
            "0.4775391406262927\n",
            "0.47883296355871896\n",
            "0.48001546719494986\n",
            "0.48135191720464954\n",
            "0.4827328894449317\n",
            "0.4839342267007169\n",
            "0.48488899623341575\n",
            "0.48624004374074814\n",
            "0.4870735770448699\n",
            "0.4878087902770323\n",
            "0.48924848345844335\n",
            "0.49029444238109054\n",
            "0.49190403547738215\n",
            "0.49303098110591664\n",
            "0.4943389311013624\n",
            "0.49518650899762695\n",
            "0.4964873097894137\n",
            "0.49762785137461885\n",
            "0.498922707860732\n",
            "0.5002180500256131\n",
            "0.5013807832889849\n",
            "0.503063392532451\n",
            "0.5043629140347776\n",
            "0.5051897624721917\n",
            "0.5060083622975118\n",
            "0.5068865236266494\n",
            "0.5083025423309687\n",
            "0.5092751619303623\n",
            "0.5103937135175671\n",
            "0.5120022913531574\n",
            "0.512947057061793\n",
            "0.5141790145650849\n",
            "0.5149558963220747\n",
            "0.5160277763291088\n",
            "0.5170889505949776\n",
            "0.5180922040853964\n",
            "0.5189994541580415\n",
            "0.5200405128471687\n",
            "0.5211733393656933\n",
            "0.5224293979537457\n",
            "0.5232827738880197\n",
            "0.5245793273534312\n",
            "0.5256419746619662\n",
            "0.5270137876043539\n",
            "0.528266107029927\n",
            "0.5297580227217711\n",
            "0.5311330421196531\n",
            "0.5321987841440283\n",
            "0.533495065806162\n",
            "0.5346897540190031\n",
            "0.5363282642096204\n",
            "0.5376979576047424\n",
            "0.5387101062880758\n",
            "0.540060520553223\n",
            "0.5411514540767426\n",
            "0.5423839426101627\n",
            "0.5436831973397823\n",
            "0.5446989618603836\n",
            "0.5457228275058824\n",
            "0.5466884142145172\n",
            "0.5478580146646865\n",
            "0.5489352887396313\n",
            "0.5498919119615384\n",
            "0.550691548058444\n",
            "0.552013526334787\n",
            "0.5531983935772\n",
            "0.5547789233877226\n",
            "0.5561264618430906\n",
            "0.5572618680536899\n",
            "0.5584601368135809\n",
            "0.5597068726101799\n",
            "0.5609038570500395\n",
            "0.5622548271170662\n",
            "0.5634849646969524\n",
            "0.5646534519427268\n",
            "0.5659573128461228\n",
            "0.5669897159042261\n",
            "0.5683119048547867\n",
            "0.5693839111596423\n",
            "0.5702476547959515\n",
            "0.5711170396079188\n",
            "0.5720242952446803\n",
            "0.5731486870199823\n",
            "0.5740829277069063\n",
            "0.5752218480762619\n",
            "0.5760411879290706\n",
            "0.5773421364367161\n",
            "0.5783616471321077\n",
            "0.579378417538255\n",
            "0.5807906572166306\n",
            "0.5815838549448096\n",
            "0.5827449112749465\n",
            "0.5836847042641067\n",
            "0.5848909048625576\n",
            "0.5861820716534734\n",
            "0.5873504066863633\n",
            "0.5887773595655056\n",
            "0.5904319547783689\n",
            "0.5916449580808429\n",
            "0.593096012578291\n",
            "0.5946027129659872\n",
            "0.5957638402576642\n",
            "0.5970592244964121\n",
            "0.598324626607968\n",
            "0.5995619445658096\n",
            "0.6009570185638144\n",
            "0.6021912313635697\n",
            "0.6035419072184112\n",
            "0.6044608394203284\n",
            "0.605811354144455\n",
            "0.6069591778623479\n",
            "0.6082137009829206\n",
            "0.6097138084261619\n",
            "0.6107254704398573\n",
            "0.6116552321654757\n",
            "0.612412701558579\n",
            "0.61338646416469\n",
            "0.6144731965516229\n",
            "0.6155724986587339\n",
            "0.6165253709801628\n",
            "0.6174433652854636\n",
            "0.6184946840528942\n",
            "0.6196069821829686\n",
            "0.6204692674110003\n",
            "0.621827872665337\n",
            "0.6230820518777803\n",
            "0.624517044066773\n",
            "0.6255517233820522\n",
            "0.6271243826355166\n",
            "0.6279055162921281\n",
            "0.6287271753144081\n",
            "0.6299586159646359\n",
            "0.6310867831072844\n",
            "0.6318524142208002\n",
            "0.6326938828696376\n",
            "0.6338967068878281\n",
            "0.6349825824008268\n",
            "0.6362420073555558\n",
            "0.6376225742537652\n",
            "0.6386459677115731\n",
            "0.6398637799350807\n",
            "0.6406446890452938\n",
            "0.6413993577823005\n",
            "0.6425510215027558\n",
            "0.6440460191053503\n",
            "0.6454341768303795\n",
            "0.6468919137554705\n",
            "0.6483266876481683\n",
            "0.6494079074438881\n",
            "0.6503637869797094\n",
            "0.6518536027892471\n",
            "0.6530266763151759\n",
            "0.6542020360069811\n",
            "0.655424517667507\n",
            "0.6565829336338336\n",
            "0.6574008175936501\n",
            "0.658440145995001\n",
            "0.6596806724662976\n",
            "0.660543197561103\n",
            "0.6615110984848588\n",
            "0.6626360007106801\n",
            "0.6642331925347028\n",
            "0.6650521600490336\n",
            "0.6662078179666758\n",
            "0.6673035616307612\n",
            "0.6681922671892454\n",
            "0.6694199772899413\n",
            "0.670835540803802\n",
            "0.6720454081549974\n",
            "0.6733030407782405\n",
            "0.6742838570833816\n",
            "0.675282696095269\n",
            "0.6763639560593363\n",
            "0.6773819986664121\n",
            "0.6782222034986062\n",
            "0.6794270943192875\n",
            "0.6806362173746309\n",
            "0.6817109683895355\n",
            "0.6827413367340937\n",
            "0.6837936568900448\n",
            "0.6851947418106791\n",
            "0.6861692097638269\n",
            "0.6875489475324635\n",
            "0.6887694634592442\n",
            "0.6900990368307703\n",
            "0.6911743009639213\n",
            "0.6922677789655183\n",
            "0.6936713073717053\n",
            "0.6952398269225264\n",
            "0.6965983129675736\n",
            "0.6978942322761507\n",
            "0.6996918043974415\n",
            "0.7006945971332853\n",
            "0.7020576998705754\n",
            "0.7034274365590967\n",
            "0.7045990227128539\n",
            "0.7054767439432461\n",
            "0.7067044240129573\n",
            "0.7077813160693859\n",
            "0.7090333986770162\n",
            "0.7101840835702998\n",
            "0.7116807830303221\n",
            "0.7128702664314328\n",
            "0.7142528051610493\n",
            "0.7154044483018958\n",
            "0.716986464417499\n",
            "0.7181397238960656\n",
            "0.719289372339273\n",
            "0.7204762168247681\n",
            "0.7216380012919531\n",
            "0.7229204563533559\n",
            "0.7241319322677524\n",
            "0.7256469149571245\n",
            "0.7268216106135522\n",
            "0.7282458235083333\n",
            "0.7293872112965645\n",
            "0.7306059482304946\n",
            "0.7318910245242936\n",
            "0.732919042906188\n",
            "0.7337297140942205\n",
            "0.7347047069036138\n",
            "0.7362011703841217\n",
            "0.7371455410404888\n",
            "0.738406274248572\n",
            "0.7394529403475545\n",
            "0.7403383764921857\n",
            "0.7414759939436413\n",
            "0.7430403844627274\n",
            "0.7440836210842328\n",
            "0.7453269142171611\n",
            "0.7463426205050915\n",
            "0.7474463096512552\n",
            "0.7483960285668483\n",
            "0.7495234939448364\n",
            "0.7506765557067169\n",
            "0.751902077966334\n",
            "0.7531930060337877\n",
            "0.753877870109685\n",
            "0.75476079836221\n",
            "0.7557849593632057\n",
            "0.7572670046173399\n",
            "0.7584897562518449\n",
            "0.759874467425944\n",
            "0.7611606377927239\n",
            "0.76237649647781\n",
            "0.7633654309813019\n",
            "0.7643368621463971\n",
            "0.7655123485171277\n",
            "0.7663797717112715\n",
            "0.7674863908601843\n",
            "0.7682663741166634\n",
            "0.7694794520392747\n",
            "0.7706585916716729\n",
            "0.7718670322462116\n",
            "0.7734429560353994\n",
            "0.7745360956929833\n",
            "0.7756396729470519\n",
            "0.7769843022079419\n",
            "0.7780614683542715\n",
            "0.7793637249628296\n",
            "0.7806384429297484\n",
            "0.7815350941989733\n",
            "0.7827195340715101\n",
            "0.7842330362485803\n",
            "0.7852830932573285\n",
            "0.7865090924303245\n",
            "0.7878118257235993\n",
            "0.7892032152094195\n",
            "0.7902412066221847\n",
            "0.7917984108943159\n",
            "0.7928038502440733\n",
            "0.7941512010438972\n",
            "0.7953280189915386\n",
            "0.7965085767876462\n",
            "0.7976038846975703\n",
            "0.7985643321443396\n",
            "0.7996026897217001\n",
            "0.8008291910371512\n",
            "0.8021199224550096\n",
            "0.8035776727949567\n",
            "0.8049655414908133\n",
            "0.8061778850262732\n",
            "0.8069058806085221\n",
            "0.8083815269762903\n",
            "0.8097977028478442\n",
            "0.8106820286844697\n",
            "0.8119327189672328\n",
            "0.8130489228021763\n",
            "0.8141297521950949\n",
            "0.8151141717610761\n",
            "0.816089970224044\n",
            "0.817124582312601\n",
            "0.8181760777597842\n",
            "0.8192233097217881\n",
            "0.8201474774524075\n",
            "0.8213457258613518\n",
            "0.8224899089702254\n",
            "0.8240345369671922\n",
            "0.8248694335560665\n",
            "0.8257851538908146\n",
            "0.8268374684064285\n",
            "0.8276281981059658\n",
            "0.8288937883303903\n",
            "0.8302077231809611\n",
            "0.8312584826403566\n",
            "0.8323488215656232\n",
            "0.833526344631639\n",
            "0.8345276317785463\n",
            "0.8357172633528405\n",
            "0.8367124047425701\n",
            "0.8377684171090041\n",
            "0.8389239580277592\n",
            "0.8405794610300332\n",
            "0.8416453009218816\n",
            "0.8428029822723945\n",
            "0.8443786179470589\n",
            "0.8457931558341931\n",
            "0.8469526201410367\n",
            "0.8479807777782841\n",
            "0.8489412156974568\n",
            "0.8498417857815238\n",
            "0.8510372695105765\n",
            "0.8520191333178059\n",
            "0.8530915342176052\n",
            "0.8538019615213585\n",
            "0.8549786819826306\n",
            "0.8560897084453222\n",
            "0.857084074593566\n",
            "0.8584708170512753\n",
            "0.8593555113391194\n",
            "0.8603210151195526\n",
            "0.8614690803048556\n",
            "0.8630168787811113\n",
            "0.8640222129462015\n",
            "0.8652053803891477\n",
            "0.8660809052417345\n",
            "0.8671269443486352\n",
            "0.8682209631366193\n",
            "0.8692203871429424\n",
            "0.8705376217432339\n",
            "0.8718380173453895\n",
            "0.8731834580526328\n",
            "0.8745760923761237\n",
            "0.875774304808863\n",
            "0.8770528367108397\n",
            "0.8784226865109885\n",
            "0.8799505427365413\n",
            "0.8812698477979206\n",
            "0.8825329152671882\n",
            "0.8833639114866476\n",
            "0.8842911759148473\n",
            "0.8855952389557343\n",
            "0.8872628189871073\n",
            "0.8883766256787283\n",
            "0.8894065288479066\n",
            "0.8904963018339308\n",
            "0.891481540392122\n",
            "0.892560346657053\n",
            "0.8934424124715273\n",
            "0.8945623768869874\n",
            "0.8962970494919116\n",
            "0.8974393134379326\n",
            "0.8987508086139894\n",
            "0.8999838733002353\n",
            "0.900953490487145\n",
            "0.902069515889258\n",
            "0.903013204903249\n",
            "0.0008372533351868925\n",
            "0.0020332433988371162\n",
            "0.0033837569034312997\n",
            "0.004585797295850866\n",
            "0.005686978016363081\n",
            "0.006679162116306822\n",
            "0.00795831544624875\n",
            "0.009110655092522311\n",
            "0.009908092296336922\n",
            "0.010549091088497425\n",
            "0.011422767434888483\n",
            "0.012574573840631549\n",
            "0.013460571732362517\n",
            "0.01436123114717586\n",
            "0.015470479988990843\n",
            "0.016662930512367308\n",
            "0.0178326260098411\n",
            "0.019099079586965655\n",
            "0.020233257804685236\n",
            "0.021134953212250226\n",
            "0.022280497929019392\n",
            "0.02295861494205797\n",
            "0.02399560649071813\n",
            "0.02498022156298313\n",
            "0.02606640173041302\n",
            "0.027474479602121026\n",
            "0.028702409523527334\n",
            "0.029837768934571834\n",
            "0.030624409618280123\n",
            "0.031781738371495394\n",
            "0.03337446259110785\n",
            "0.034183922249947665\n",
            "0.0353220132610682\n",
            "0.03648146224753631\n",
            "0.03798068949328664\n",
            "0.039252126765678\n",
            "0.04043054740752101\n",
            "0.04151717773483842\n",
            "0.042692430412677854\n",
            "0.04380327318330555\n",
            "0.04484859566249506\n",
            "0.04578146513770608\n",
            "0.04695606003027133\n",
            "0.04811088737014614\n",
            "0.04915516440520811\n",
            "0.04995050950123526\n",
            "0.05114298433903843\n",
            "0.05236321001711404\n",
            "0.05338032120633918\n",
            "0.05413209530703552\n",
            "0.055570156754130295\n",
            "0.05689741316658761\n",
            "0.057599064105611934\n",
            "0.05865652733446692\n",
            "0.05984172811898429\n",
            "0.061054777916130204\n",
            "0.061967113133891465\n",
            "0.06289521621926056\n",
            "0.06445035689017352\n",
            "0.06546696715647607\n",
            "0.06649753779096676\n",
            "0.06784761119681551\n",
            "0.06908432655322277\n",
            "0.07017033834896429\n",
            "0.07110985724822334\n",
            "0.07208058154186629\n",
            "0.07283028472415017\n",
            "0.07392443926133158\n",
            "0.0749707843946374\n",
            "0.0762076759734727\n",
            "0.07687395026006967\n",
            "0.07813958408277663\n",
            "0.07897892716290701\n",
            "0.0802896451919585\n",
            "0.0812539238759014\n",
            "0.0823224890415016\n",
            "0.08330624007507968\n",
            "0.08415329662125434\n",
            "0.08535756837681431\n",
            "0.08629029913021781\n",
            "0.08747312007352824\n",
            "0.08859216511401984\n",
            "0.08981049022711146\n",
            "0.09096996596707102\n",
            "0.09233126501598017\n",
            "0.0935427422261299\n",
            "0.0942441382066673\n",
            "0.09585516654012148\n",
            "0.09721451403234926\n",
            "0.09867330387120357\n",
            "0.09963796083884471\n",
            "0.10076373251502777\n",
            "0.10199293517090781\n",
            "0.10308925041457272\n",
            "0.1042416165856754\n",
            "0.10516836088331764\n",
            "0.10614739064975164\n",
            "0.10776437380734612\n",
            "0.1088411716548988\n",
            "0.10954655299101339\n",
            "0.1105883775464714\n",
            "0.11181358455696984\n",
            "0.11274472252487222\n",
            "0.11404541096723902\n",
            "0.11469999588359042\n",
            "0.11598482263057738\n",
            "0.1168134605793087\n",
            "0.11813567331075059\n",
            "0.11916581314543019\n",
            "0.12044015938363721\n",
            "0.12137955076554242\n",
            "0.12231735157234894\n",
            "0.12326324740639123\n",
            "0.12449107442975349\n",
            "0.125653767448557\n",
            "0.1266548949891649\n",
            "0.1279622192120613\n",
            "0.12924094738252936\n",
            "0.1302287125069162\n",
            "0.13143116319575882\n",
            "0.13292964835605964\n",
            "0.13421607772102745\n",
            "0.13520432745709138\n",
            "0.13663759416021654\n",
            "0.1380034951145387\n",
            "0.13913386839125164\n",
            "0.13997221579941946\n",
            "0.14083579784768926\n",
            "0.14191552463090024\n",
            "0.14316219312455647\n",
            "0.14423377998649617\n",
            "0.14530941958317672\n",
            "0.14637237192724672\n",
            "0.14754899680766914\n",
            "0.1484923673712689\n",
            "0.14962216556224678\n",
            "0.15085427550708547\n",
            "0.1520010733695896\n",
            "0.15267300956389485\n",
            "0.15385194141846484\n",
            "0.15491391036211682\n",
            "0.1561898827705237\n",
            "0.15728385651203067\n",
            "0.15875094694554653\n",
            "0.15987954061964285\n",
            "0.1612547039223449\n",
            "0.16266463609302745\n",
            "0.16388202681565833\n",
            "0.1648618291558512\n",
            "0.16600929928557648\n",
            "0.16726227687752765\n",
            "0.16870662425180225\n",
            "0.1699834244940287\n",
            "0.17100774472021996\n",
            "0.1721619176285346\n",
            "0.1733054081192407\n",
            "0.17458977449275648\n",
            "0.17535336151757203\n",
            "0.17682085988466698\n",
            "0.17794474021857962\n",
            "0.17912410325406458\n",
            "0.1803488171161593\n",
            "0.18129922628707593\n",
            "0.18223504832638499\n",
            "0.18390633070560367\n",
            "0.18497054396992754\n",
            "0.18589655167001592\n",
            "0.18718406123578396\n",
            "0.1887327813736313\n",
            "0.19008895167914194\n",
            "0.1912896915165055\n",
            "0.1925084396548893\n",
            "0.1940333420205909\n",
            "0.1950831261589704\n",
            "0.19619051879629149\n",
            "0.197192002973898\n",
            "0.1983945747013287\n",
            "0.19951601414119496\n",
            "0.200872115695568\n",
            "0.2020511800218421\n",
            "0.2031873712301864\n",
            "0.2044301412599471\n",
            "0.20557544877766953\n",
            "0.2068071812772385\n",
            "0.20782110659057831\n",
            "0.20925904928570818\n",
            "0.2105044169956461\n",
            "0.21160815362735172\n",
            "0.21303181193978585\n",
            "0.21375105188935614\n",
            "0.21502422341300398\n",
            "0.21637954797281328\n",
            "0.21733324576521773\n",
            "0.21860935460880895\n",
            "0.21962652403070493\n",
            "0.22063243709256886\n",
            "0.22170909880982032\n",
            "0.222750411923889\n",
            "0.22428037504405926\n",
            "0.22521107398030704\n",
            "0.22619209448089989\n",
            "0.227036340233615\n",
            "0.2283486335936105\n",
            "0.22942815221788937\n",
            "0.2306111260600712\n",
            "0.23175164310218732\n",
            "0.23288365832680022\n",
            "0.2343026793673825\n",
            "0.23519194880714805\n",
            "0.2361275662699014\n",
            "0.23698211150705967\n",
            "0.2381031539891382\n",
            "0.23953614309620674\n",
            "0.24077216530090098\n",
            "0.24161952048006569\n",
            "0.24294118869030262\n",
            "0.2439198355235712\n",
            "0.24493003836678118\n",
            "0.24594191219800574\n",
            "0.24736326864308408\n",
            "0.2486467642704849\n",
            "0.249898741998331\n",
            "0.2507397774082925\n",
            "0.25152126091825383\n",
            "0.25260335976815285\n",
            "0.25355533215091053\n",
            "0.25497413611473024\n",
            "0.25609826508080563\n",
            "0.25704732240008576\n",
            "0.2585493774365281\n",
            "0.2593171430365814\n",
            "0.2607022113812244\n",
            "0.2618966591937463\n",
            "0.2627545785721001\n",
            "0.2639412803723074\n",
            "0.26531147758674134\n",
            "0.26642307966871337\n",
            "0.2679178080595363\n",
            "0.26894130395806354\n",
            "0.2700830614170455\n",
            "0.2717173767211797\n",
            "0.2729908145602097\n",
            "0.27420028274321495\n",
            "0.2754737931444212\n",
            "0.27666159694456993\n",
            "0.2782524145777573\n",
            "0.27959465386007754\n",
            "0.2809455495356294\n",
            "0.28214947776416377\n",
            "0.2833741126615373\n",
            "0.28449343949022804\n",
            "0.2854365630985221\n",
            "0.2864947122381166\n",
            "0.2882323164464263\n",
            "0.28942279544327876\n",
            "0.2906245388033445\n",
            "0.29139659944397717\n",
            "0.2924520446516364\n",
            "0.293884371857509\n",
            "0.29486883662240887\n",
            "0.2960523880656113\n",
            "0.2968813376048642\n",
            "0.2978673524716321\n",
            "0.2987744313524202\n",
            "0.30002651838085537\n",
            "0.30076048540337313\n",
            "0.3019140339111123\n",
            "0.3029578772499738\n",
            "0.30404346335269605\n",
            "0.3054060979419962\n",
            "0.30651051621607805\n",
            "0.3077492798533281\n",
            "0.3093654623879191\n",
            "0.3102072836340541\n",
            "0.311395954064396\n",
            "0.3123518430515933\n",
            "0.3134317101572481\n",
            "0.3145343761157502\n",
            "0.3155327188541822\n",
            "0.31644884178705535\n",
            "0.31774840810719657\n",
            "0.3189765346781982\n",
            "0.3201997896746906\n",
            "0.32157603287330977\n",
            "0.32279234888303615\n",
            "0.3239828117210847\n",
            "0.32508794586067\n",
            "0.3266244840896343\n",
            "0.32827470834602784\n",
            "0.3296387482939474\n",
            "0.33075646846495627\n",
            "0.33262417192959115\n",
            "0.3340198588950555\n",
            "0.3349065756249001\n",
            "0.33619965037421495\n",
            "0.3374961751806157\n",
            "0.33881527718985477\n",
            "0.3401315386033119\n",
            "0.3411993456771002\n",
            "0.34198894029688043\n",
            "0.34292613370034397\n",
            "0.34376015961932405\n",
            "0.34516895610048337\n",
            "0.3461744547500025\n",
            "0.3470777032320457\n",
            "0.34829097330722664\n",
            "0.34984477371206063\n",
            "0.35083183074546287\n",
            "0.35195858345922\n",
            "0.35325326097895726\n",
            "0.3544101296635845\n",
            "0.35562451965058856\n",
            "0.35667806482680925\n",
            "0.35742962787218413\n",
            "0.3585406936647947\n",
            "0.35962995932535136\n",
            "0.36068223108111136\n",
            "0.3618318173281677\n",
            "0.36291259428119416\n",
            "0.3641381303367712\n",
            "0.3656986384745449\n",
            "0.36686702670953464\n",
            "0.36796159222912606\n",
            "0.3690094587290683\n",
            "0.3700990375807828\n",
            "0.3713397015543545\n",
            "0.37276433122432445\n",
            "0.37389438635552935\n",
            "0.3749044742578131\n",
            "0.3765985352151534\n",
            "0.37757560793701034\n",
            "0.37846422378364425\n",
            "0.37968201969590637\n",
            "0.3811602743385393\n",
            "0.38221084797168936\n",
            "0.3834729344796037\n",
            "0.3845764935931281\n",
            "0.3859511041427817\n",
            "0.38690015460219224\n",
            "0.38829737955042165\n",
            "0.3892061743132599\n",
            "0.39025875918395686\n",
            "0.3916227640703206\n",
            "0.39269740563219463\n",
            "0.3941073439005391\n",
            "0.39538758840707255\n",
            "0.3970053676144241\n",
            "0.39846697914630863\n",
            "0.3994037258960402\n",
            "0.400605700967257\n",
            "0.40150093010929233\n",
            "0.4027558565901978\n",
            "0.40395679086675423\n",
            "0.40515267711771114\n",
            "0.40607919160972167\n",
            "0.4069987466115781\n",
            "0.4084694489951024\n",
            "0.40961875726499825\n",
            "0.41052323351125886\n",
            "0.41187864298100973\n",
            "0.41340973035758716\n",
            "0.41437178949260955\n",
            "0.4157381494484289\n",
            "0.41693146507758316\n",
            "0.4180566187557357\n",
            "0.4195274074211755\n",
            "0.42073923372246724\n",
            "0.4221093094409884\n",
            "0.4232346568723469\n",
            "0.4241695042004061\n",
            "0.42570527793501345\n",
            "0.4269559865869829\n",
            "0.42834887091461044\n",
            "0.4293943473597622\n",
            "0.43068566079944604\n",
            "0.4318800743126198\n",
            "0.4331032559085075\n",
            "0.4346240361023437\n",
            "0.4355945939298176\n",
            "0.4367698879193162\n",
            "0.4377984630177393\n",
            "0.4390706625740851\n",
            "0.44023588200664276\n",
            "0.44148434237446016\n",
            "0.4429640252419445\n",
            "0.4439603475963368\n",
            "0.44497880438709503\n",
            "0.4462143736117331\n",
            "0.44732585465511704\n",
            "0.4483176067357173\n",
            "0.4494467142140469\n",
            "0.4505689009223753\n",
            "0.4515961811823003\n",
            "0.45286740816157794\n",
            "0.45388721550821953\n",
            "0.45507998219536394\n",
            "0.4560256434218658\n",
            "0.45725294726583965\n",
            "0.45856147181347506\n",
            "0.4595419784336139\n",
            "0.4608399880206798\n",
            "0.46198400908418935\n",
            "0.463461137793558\n",
            "0.4644817639799679\n",
            "0.4657213469143109\n",
            "0.46658093110679666\n",
            "0.46773755115926113\n",
            "0.46915232030022175\n",
            "0.470183450395189\n",
            "0.471329550761396\n",
            "0.4727099284796459\n",
            "0.4735417434626528\n",
            "0.4748403253152852\n",
            "0.4758198717061211\n",
            "0.47677211855988366\n",
            "0.4773283164443262\n",
            "0.47875041466997104\n",
            "0.48032024064484763\n",
            "0.481183809049599\n",
            "0.482220799988493\n",
            "0.4830190019153268\n",
            "0.48435987364453126\n",
            "0.4856341715587679\n",
            "0.4868472040537983\n",
            "0.48787472372317253\n",
            "0.48905775823708997\n",
            "0.49025766124658265\n",
            "0.49152873319280727\n",
            "0.4925873914871679\n",
            "0.493699326501478\n",
            "0.4948689040854154\n",
            "0.49599043499020967\n",
            "0.49688227497555715\n",
            "0.4980616114099922\n",
            "0.4994014593417687\n",
            "0.5006470244635096\n",
            "0.5015580092016083\n",
            "0.502624133854266\n",
            "0.5039327806600219\n",
            "0.5051586654256371\n",
            "0.5062919262501285\n",
            "0.5075729189397734\n",
            "0.5085708141098242\n",
            "0.5094888962976768\n",
            "0.5107207457961329\n",
            "0.511984821742453\n",
            "0.5132133318563862\n",
            "0.5140845817906777\n",
            "0.515512716854015\n",
            "0.5165316937753307\n",
            "0.5177653450566484\n",
            "0.5189108694986919\n",
            "0.5200971074192725\n",
            "0.5212889093800884\n",
            "0.5223008274193615\n",
            "0.523692615692268\n",
            "0.5245744474327473\n",
            "0.5262413762338326\n",
            "0.5273265176645631\n",
            "0.5283014480491428\n",
            "0.5293186546667762\n",
            "0.5301094765172285\n",
            "0.5312057284976516\n",
            "0.5325746307211459\n",
            "0.5336743214779802\n",
            "0.5350044078153112\n",
            "0.5362336562798761\n",
            "0.5375694363089778\n",
            "0.5383766859464938\n",
            "0.5393454986231406\n",
            "0.540650445062791\n",
            "0.5423124359772943\n",
            "0.5436264598537284\n",
            "0.5448935659568938\n",
            "0.5456869734446411\n",
            "0.547042604869284\n",
            "0.547847661330267\n",
            "0.5491703850457735\n",
            "0.5502653824322669\n",
            "0.5515251913872521\n",
            "0.5532477538832619\n",
            "0.5545622645055547\n",
            "0.5558656574134022\n",
            "0.5571599151091198\n",
            "0.5579502246797542\n",
            "0.5592461421590327\n",
            "0.5601213128899064\n",
            "0.5611507261119535\n",
            "0.5623015498032655\n",
            "0.5630740243226976\n",
            "0.5643428602563146\n",
            "0.5656454304371343\n",
            "0.5669410155938409\n",
            "0.5679395062005733\n",
            "0.5690844258307801\n",
            "0.5702978091700303\n",
            "0.5713041313850057\n",
            "0.5722840323548792\n",
            "0.5736018011103505\n",
            "0.5745450112292224\n",
            "0.5756848879406214\n",
            "0.576900534274633\n",
            "0.577763320501808\n",
            "0.5789655760654708\n",
            "0.5802951236743756\n",
            "0.581247959486054\n",
            "0.582258826593304\n",
            "0.583358136703596\n",
            "0.5843513869797178\n",
            "0.5855283787107224\n",
            "0.586605801111292\n",
            "0.5878487805195172\n",
            "0.5890556244212953\n",
            "0.5902845276819776\n",
            "0.5913500881484707\n",
            "0.5924448335490873\n",
            "0.5936069586469085\n",
            "0.5944595607116704\n",
            "0.5955044999528114\n",
            "0.5966159224586414\n",
            "0.5978787540246153\n",
            "0.5991289007770436\n",
            "0.6002906520119713\n",
            "0.6014266246191377\n",
            "0.6024221302679432\n",
            "0.6034594952221721\n",
            "0.6051625729445607\n",
            "0.6065254067749624\n",
            "0.6076670527991737\n",
            "0.6085210451308418\n",
            "0.6096741414969534\n",
            "0.6110536091391693\n",
            "0.6122780370209223\n",
            "0.6134826261597826\n",
            "0.614394375468459\n",
            "0.6153075469805457\n",
            "0.6163023802096886\n",
            "0.6175745661987369\n",
            "0.6192474316834183\n",
            "0.6202759939767516\n",
            "0.6215965966205768\n",
            "0.6229271674171433\n",
            "0.6238698551188344\n",
            "0.6250612869515748\n",
            "0.6263572486389019\n",
            "0.6279316560920242\n",
            "0.6289532970818107\n",
            "0.6302309316366225\n",
            "0.6311727460006924\n",
            "0.6319959777624101\n",
            "0.6327736955088423\n",
            "0.6341992904386862\n",
            "0.6357283995813116\n",
            "0.6362520204023328\n",
            "0.6373155323593208\n",
            "0.6380922872849437\n",
            "0.6396504121515757\n",
            "0.6407293248207063\n",
            "0.642149074531882\n",
            "0.6433866762596628\n",
            "0.6443013775988918\n",
            "0.6454708269794883\n",
            "0.6465344961037112\n",
            "0.6478209344627303\n",
            "0.649055816023551\n",
            "0.6502890704995226\n",
            "0.6514041346052418\n",
            "0.6525280794981495\n",
            "0.6538654520840901\n",
            "0.6548078838364243\n",
            "0.6558249413662249\n",
            "0.657251523355084\n",
            "0.658281422560782\n",
            "0.6592764698178567\n",
            "0.6606821485645021\n",
            "0.6619455595607953\n",
            "0.6628676438727952\n",
            "0.6641970353815562\n",
            "0.6652961796354455\n",
            "0.6665477184840786\n",
            "0.6676260883088612\n",
            "0.6687859270121436\n",
            "0.6702515072072558\n",
            "0.6711737104236623\n",
            "0.672482563787714\n",
            "0.6736453812750404\n",
            "0.6748785990888201\n",
            "0.6758065991237036\n",
            "0.6766160727309449\n",
            "0.677455326099225\n",
            "0.678523493468609\n",
            "0.6795522486767196\n",
            "0.6806192786035026\n",
            "0.6816123871852064\n",
            "0.6824124550727932\n",
            "0.6836181712120085\n",
            "0.6845220216857199\n",
            "0.6852670529157\n",
            "0.6871328124457308\n",
            "0.6882882320210147\n",
            "0.6889576520913702\n",
            "0.689913193816724\n",
            "0.6911071221846754\n",
            "0.6920044458735629\n",
            "0.6930992306040986\n",
            "0.6940723448000905\n",
            "0.6955054000667904\n",
            "0.6965077039988145\n",
            "0.6974601625938854\n",
            "0.6989286408552429\n",
            "0.7003612589195866\n",
            "0.7018096212993192\n",
            "0.7028994250023152\n",
            "0.7040964154636159\n",
            "0.7053753750403519\n",
            "0.7063955645579512\n",
            "0.7074095180729771\n",
            "0.7084481651368348\n",
            "0.7097211316266023\n",
            "0.7112226166841015\n",
            "0.712350999074214\n",
            "0.7132641192134994\n",
            "0.7143214210067563\n",
            "0.7155767394910992\n",
            "0.7168373716304369\n",
            "0.7181760584149519\n",
            "0.719538092689441\n",
            "0.7206867797600339\n",
            "0.7220452955311827\n",
            "0.7234369885281223\n",
            "0.7245036768333991\n",
            "0.7257580074202984\n",
            "0.7266727490040957\n",
            "0.7276166363445389\n",
            "0.7285791797101345\n",
            "0.7298399568213831\n",
            "0.7310078834633693\n",
            "0.7319094109565706\n",
            "0.7329185519681867\n",
            "0.7341159217802765\n",
            "0.7357214276138169\n",
            "0.7368131313482513\n",
            "0.7380204827279386\n",
            "0.7391223634600335\n",
            "0.7402330237581297\n",
            "0.7410571456260389\n",
            "0.7419764093120994\n",
            "0.7427921380533282\n",
            "0.7436921974582136\n",
            "0.7447717270582838\n",
            "0.7456635841933053\n",
            "0.7470167047532318\n",
            "0.7482733884278465\n",
            "0.7493818244513344\n",
            "0.7507471970432554\n",
            "0.7519049988225903\n",
            "0.7530479498988832\n",
            "0.7540353881885938\n",
            "0.7550463636055627\n",
            "0.7565997566865839\n",
            "0.7576983764653316\n",
            "0.7590971566222208\n",
            "0.7601283511237416\n",
            "0.7610236929963007\n",
            "0.7622113894776005\n",
            "0.7636669225552503\n",
            "0.7646786469175383\n",
            "0.7659376372614175\n",
            "0.7677600247323361\n",
            "0.7687641256453132\n",
            "0.7699115127706162\n",
            "0.7712618921266492\n",
            "0.7722104029429843\n",
            "0.7733616652848471\n",
            "0.7742074675419751\n",
            "0.7757211707894455\n",
            "0.7768863298551506\n",
            "0.7777893661385606\n",
            "0.7789078033183847\n",
            "0.7803987517686146\n",
            "0.781526437348417\n",
            "0.7824494004859339\n",
            "0.7835812384210279\n",
            "0.7849509623044592\n",
            "0.7863072672158556\n",
            "0.7875111470441989\n",
            "0.7887210943509856\n",
            "0.7898113665830754\n",
            "0.7910897270645327\n",
            "0.792836679445813\n",
            "0.7941105087547351\n",
            "0.7951877483778902\n",
            "0.7960539585946466\n",
            "0.7972365665008955\n",
            "0.7981091792626149\n",
            "0.7990791430253812\n",
            "0.8000119724846861\n",
            "0.8008951492931532\n",
            "0.8020246015180408\n",
            "0.8031049824279287\n",
            "0.8039262790966522\n",
            "0.8050930186000931\n",
            "0.8062988980042051\n",
            "0.8074086149177893\n",
            "0.8085675122945205\n",
            "0.8095134654465843\n",
            "0.8108282231766245\n",
            "0.8119760615289059\n",
            "0.8133466018892616\n",
            "0.8143795479441542\n",
            "0.8157412177309051\n",
            "0.8168695301503477\n",
            "0.8178088652813221\n",
            "0.8187355671239935\n",
            "0.8195726039921841\n",
            "0.8205514452646455\n",
            "0.8213500234934376\n",
            "0.8229875501311953\n",
            "0.8238822989299169\n",
            "0.8251719507567413\n",
            "0.8262062019399364\n",
            "0.826953603102423\n",
            "0.8286416859120664\n",
            "0.8296213248349211\n",
            "0.8307895436311317\n",
            "0.8317015578069955\n",
            "0.8333254610486043\n",
            "0.8349326887094152\n",
            "0.8366388169395954\n",
            "0.8381295373372715\n",
            "0.8395222448327048\n",
            "0.8406886568154825\n",
            "0.8417232054883562\n",
            "0.8429706070734106\n",
            "0.8447702850222283\n",
            "0.8458659483496186\n",
            "0.8472023908896824\n",
            "0.8483751093792489\n",
            "0.8496049022887979\n",
            "0.8506675253591269\n",
            "0.8515832459987582\n",
            "0.8527794702888449\n",
            "0.8537550562482965\n",
            "0.8549764835468644\n",
            "0.8561248016326933\n",
            "0.8575604046549639\n",
            "0.8591604736607398\n",
            "0.8598937806113601\n",
            "0.8612799774807738\n",
            "0.8624568121207644\n",
            "0.8636364259988146\n",
            "0.8644177152982453\n",
            "0.8656761557854655\n",
            "0.8672216682482863\n",
            "0.8679770991930267\n",
            "0.8691370967404007\n",
            "0.8706702524438843\n",
            "0.8717245926027712\n",
            "0.8729435055304671\n",
            "0.8743715102562819\n",
            "0.8753417675452464\n",
            "0.8763569177264143\n",
            "0.8775893930736405\n",
            "0.8787650719780447\n",
            "0.8798210512646629\n",
            "0.8809997238923827\n",
            "0.8821863636488805\n",
            "0.8833972547212829\n",
            "0.8845684083221513\n",
            "0.8856332991891505\n",
            "0.8871603662248158\n",
            "0.8882857668582741\n",
            "0.8892816764771786\n",
            "0.8903555416542551\n",
            "0.8913793896165345\n",
            "0.8924271707492106\n",
            "0.893577445117409\n",
            "0.8946831243879655\n",
            "0.896050614697854\n",
            "0.8969587039612138\n",
            "0.8982066184358524\n",
            "0.8994180232362674\n",
            "0.0015090662805015778\n",
            "0.002797554673441231\n",
            "0.003903798968590739\n",
            "0.00505514865945977\n",
            "0.005873545203977228\n",
            "0.00693236081801412\n",
            "0.007956689580931993\n",
            "0.008859406651743233\n",
            "0.010072322147886467\n",
            "0.011324366721350823\n",
            "0.01241204401721125\n",
            "0.013366306910429464\n",
            "0.014347521469111333\n",
            "0.015550011182989916\n",
            "0.016868301745875718\n",
            "0.01811913783897829\n",
            "0.019208814634386537\n",
            "0.02026752559730159\n",
            "0.021491895475046104\n",
            "0.02286012969968264\n",
            "0.023959704143616853\n",
            "0.025054178350721785\n",
            "0.02628662938352131\n",
            "0.027166552174731594\n",
            "0.028012894837142865\n",
            "0.029133903324756474\n",
            "0.030393910682414805\n",
            "0.03165825164836386\n",
            "0.033222562592962517\n",
            "0.03413189295917521\n",
            "0.03570306560267573\n",
            "0.03670875297482971\n",
            "0.037635230880869015\n",
            "0.03909278762005174\n",
            "0.04052306326758831\n",
            "0.041845044745203785\n",
            "0.04318237205600495\n",
            "0.0442733591627282\n",
            "0.04552161533509374\n",
            "0.04658380379457303\n",
            "0.0477546387926087\n",
            "0.048811638446720054\n",
            "0.04983749726544256\n",
            "0.05107185427490098\n",
            "0.05231526928484592\n",
            "0.053278408239564624\n",
            "0.05469457572683349\n",
            "0.055976256842503465\n",
            "0.0569670158426475\n",
            "0.05786978382893535\n",
            "0.058918700574913906\n",
            "0.05976336424612938\n",
            "0.06091793411223175\n",
            "0.06211344482343825\n",
            "0.06356360784272098\n",
            "0.06462149272489426\n",
            "0.06553558201131308\n",
            "0.06654332246621857\n",
            "0.0672985561515974\n",
            "0.06854429094077986\n",
            "0.06968358067600318\n",
            "0.07105199523898952\n",
            "0.07214125731716985\n",
            "0.07321356111170386\n",
            "0.07408139505959532\n",
            "0.07525447018616035\n",
            "0.07655717146671032\n",
            "0.07772152747034722\n",
            "0.07878223053939507\n",
            "0.08047275058448772\n",
            "0.08197478641329518\n",
            "0.08301553442655012\n",
            "0.08420083666091685\n",
            "0.08546188649009256\n",
            "0.08648542812108384\n",
            "0.08745191728367525\n",
            "0.08913008925859886\n",
            "0.09011835957427158\n",
            "0.09108621385091406\n",
            "0.09241569164159048\n",
            "0.09365201781472891\n",
            "0.09458714563523413\n",
            "0.09553627620267746\n",
            "0.09651909780014506\n",
            "0.09777598239271842\n",
            "0.09875050613947232\n",
            "0.10028300558209724\n",
            "0.10135122783043805\n",
            "0.10229638142658927\n",
            "0.10355974348914593\n",
            "0.1047310782667926\n",
            "0.10638189460615367\n",
            "0.10753940293551101\n",
            "0.10863468089067113\n",
            "0.10949389350688671\n",
            "0.11062292552665066\n",
            "0.11157174976280583\n",
            "0.11304699094094278\n",
            "0.11435796903527301\n",
            "0.1157397505877268\n",
            "0.1168023992682357\n",
            "0.11767787488220292\n",
            "0.11865297302870495\n",
            "0.11991266795741323\n",
            "0.12118449974852755\n",
            "0.12250221484457441\n",
            "0.12352009083303954\n",
            "0.12470404434082148\n",
            "0.12591605662080027\n",
            "0.12731470522063468\n",
            "0.12813182026528946\n",
            "0.1290781263957548\n",
            "0.13011177215734712\n",
            "0.1311826064915913\n",
            "0.13235854607104036\n",
            "0.1338870317277396\n",
            "0.13484270455282363\n",
            "0.13595656719049223\n",
            "0.13718930793845135\n",
            "0.1381825006678891\n",
            "0.13899428315479737\n",
            "0.1399705476315735\n",
            "0.1408863935781562\n",
            "0.1418384671058801\n",
            "0.14305607002714407\n",
            "0.14421779077376246\n",
            "0.14523078047710916\n",
            "0.14681280817826994\n",
            "0.1477618351616823\n",
            "0.14888166832497052\n",
            "0.15026983855020665\n",
            "0.1514438062982486\n",
            "0.1525676444058528\n",
            "0.15340307522612764\n",
            "0.15479497783019414\n",
            "0.15556803947824346\n",
            "0.15667167214481423\n",
            "0.15769729246873684\n",
            "0.15896871670737595\n",
            "0.16036934598022715\n",
            "0.16202331877425505\n",
            "0.16322769800110545\n",
            "0.16444181168780608\n",
            "0.16582172918502633\n",
            "0.16706924296705924\n",
            "0.16799758378502047\n",
            "0.16914915055265206\n",
            "0.17038639869226518\n",
            "0.17161691829066753\n",
            "0.17252421348601046\n",
            "0.1737744340201473\n",
            "0.17463790371899715\n",
            "0.1756953555909569\n",
            "0.17697535329462621\n",
            "0.17792939491893933\n",
            "0.17899808805921805\n",
            "0.1802203709359669\n",
            "0.18140004243692168\n",
            "0.18247423490599904\n",
            "0.18390458570721815\n",
            "0.18519844415852482\n",
            "0.1865370425269427\n",
            "0.18768680453910241\n",
            "0.18856462272231841\n",
            "0.1894078567204878\n",
            "0.19082073893998286\n",
            "0.19181019791861628\n",
            "0.19331150644880427\n",
            "0.19462081347889912\n",
            "0.1959002093433419\n",
            "0.19708397816818998\n",
            "0.1982528558929863\n",
            "0.199681897526202\n",
            "0.20076831931348346\n",
            "0.20184789418869312\n",
            "0.20299508588393325\n",
            "0.20399715627550774\n",
            "0.20486748363355847\n",
            "0.20625559013822806\n",
            "0.20722835737725961\n",
            "0.20849081226017163\n",
            "0.20941375161680725\n",
            "0.2103735029392535\n",
            "0.21125573102775438\n",
            "0.21232283244962277\n",
            "0.213835424855542\n",
            "0.21503296181978776\n",
            "0.21620445749948702\n",
            "0.21739882954856013\n",
            "0.2184002053402269\n",
            "0.21936140615312036\n",
            "0.22029224746977277\n",
            "0.22142502650275558\n",
            "0.22250303161113769\n",
            "0.22366170390792514\n",
            "0.22487590921199535\n",
            "0.22639336236907392\n",
            "0.22757316969544686\n",
            "0.22881332329472007\n",
            "0.22988579744268256\n",
            "0.2309308665640214\n",
            "0.23202046005012433\n",
            "0.233245061943903\n",
            "0.23452237789588207\n",
            "0.2355828872879448\n",
            "0.23671974314143285\n",
            "0.23765851942169697\n",
            "0.23855398484812979\n",
            "0.23958672548803833\n",
            "0.24092672784310168\n",
            "0.24224052824022824\n",
            "0.24327065260209085\n",
            "0.24429684641111232\n",
            "0.24582288233215546\n",
            "0.24692216164925518\n",
            "0.24820357431536136\n",
            "0.24919207138783486\n",
            "0.25015031834087714\n",
            "0.2516332578171245\n",
            "0.25301323247992474\n",
            "0.25404711864183627\n",
            "0.25534718436048465\n",
            "0.2564818725713988\n",
            "0.25779048782175457\n",
            "0.2593327813288745\n",
            "0.26078358963322457\n",
            "0.26174127522027096\n",
            "0.2628649767402493\n",
            "0.26340268731422134\n",
            "0.2645854309696676\n",
            "0.2659710720372017\n",
            "0.26731184345986836\n",
            "0.2686038148372679\n",
            "0.2697649902242529\n",
            "0.2710924231640213\n",
            "0.27233204359898483\n",
            "0.27331567801477963\n",
            "0.2745593725263005\n",
            "0.27595743681768625\n",
            "0.277013798351483\n",
            "0.2777933681102665\n",
            "0.2788804141456819\n",
            "0.27987424362346036\n",
            "0.28157642331269694\n",
            "0.2828199179733501\n",
            "0.28378964552793967\n",
            "0.2848067234849076\n",
            "0.286391626538523\n",
            "0.2877913705833123\n",
            "0.2892459792554226\n",
            "0.2906524061851794\n",
            "0.2920049544795395\n",
            "0.29295126426860196\n",
            "0.29393778684194133\n",
            "0.2952621267427264\n",
            "0.29634097340466725\n",
            "0.29734208211874413\n",
            "0.2982861859261837\n",
            "0.2996400439983134\n",
            "0.30056950472809774\n",
            "0.30189538733733584\n",
            "0.30307378549405073\n",
            "0.3045247716976858\n",
            "0.305843242751363\n",
            "0.3070277321674025\n",
            "0.3085194155383293\n",
            "0.30978504333959517\n",
            "0.31104322360909503\n",
            "0.3122267457835205\n",
            "0.31326434610749754\n",
            "0.31420459764083025\n",
            "0.31525110390485095\n",
            "0.31639759718914473\n",
            "0.3178110437472458\n",
            "0.31877145582757643\n",
            "0.31993163752433895\n",
            "0.32136283468102556\n",
            "0.32246434574236954\n",
            "0.3234281417201547\n",
            "0.32462896280886266\n",
            "0.3261317340156916\n",
            "0.32749050604108043\n",
            "0.3287130306901224\n",
            "0.3298058044117735\n",
            "0.33095347873695063\n",
            "0.3323909028259385\n",
            "0.33352887546619797\n",
            "0.33501834721516466\n",
            "0.3363657076188061\n",
            "0.337014004473796\n",
            "0.33835986912098076\n",
            "0.3398103383953309\n",
            "0.34075221800438277\n",
            "0.3419063017343926\n",
            "0.34313113869303635\n",
            "0.344249446831091\n",
            "0.34551465564676564\n",
            "0.34645994209572484\n",
            "0.34758216859129687\n",
            "0.3489968105960075\n",
            "0.35030547630451525\n",
            "0.3512268638824258\n",
            "0.35237721096524194\n",
            "0.35365010916119644\n",
            "0.35521035830078224\n",
            "0.3564433992823676\n",
            "0.3578036538017985\n",
            "0.35913377184697126\n",
            "0.36009043683786224\n",
            "0.36129473443226434\n",
            "0.3621958952273249\n",
            "0.36395106664703936\n",
            "0.3649346292628657\n",
            "0.3658734455590358\n",
            "0.36721528452985425\n",
            "0.36859050789452574\n",
            "0.3695531363225044\n",
            "0.3710084265607702\n",
            "0.3719743123597196\n",
            "0.37261890549488996\n",
            "0.37376707525509395\n",
            "0.37486451909975016\n",
            "0.37575851255060766\n",
            "0.3770852073684068\n",
            "0.3783155003624499\n",
            "0.37929526383004836\n",
            "0.3803564166016591\n",
            "0.38172225109146685\n",
            "0.38307919503782717\n",
            "0.38399684284349234\n",
            "0.38524616587802274\n",
            "0.3867891932387486\n",
            "0.38831594014716575\n",
            "0.3893637930035896\n",
            "0.39042305641467007\n",
            "0.3915946142142996\n",
            "0.3924821446771207\n",
            "0.39338872675090797\n",
            "0.3951549137492314\n",
            "0.39626356364820925\n",
            "0.39757126302975215\n",
            "0.3986967288319717\n",
            "0.39942523105370115\n",
            "0.40066009493130245\n",
            "0.40194381449533545\n",
            "0.4029142590587401\n",
            "0.404033021167721\n",
            "0.4053178291643977\n",
            "0.40642640322370605\n",
            "0.4075973049149184\n",
            "0.40892077879527644\n",
            "0.4107036290266325\n",
            "0.4120949651579113\n",
            "0.4132115774599792\n",
            "0.4145477943865539\n",
            "0.41549305926503427\n",
            "0.41643699279526614\n",
            "0.41747078139458776\n",
            "0.4187302148860434\n",
            "0.41995510664742314\n",
            "0.4211001866461371\n",
            "0.42215085235398137\n",
            "0.423524078581949\n",
            "0.4250813827032933\n",
            "0.42600269276467734\n",
            "0.42716088273641095\n",
            "0.4280727790749591\n",
            "0.42956357447387616\n",
            "0.43094619277798\n",
            "0.4323144193805392\n",
            "0.43348864483101596\n",
            "0.43463656595905725\n",
            "0.43559886389376257\n",
            "0.4372735266642802\n",
            "0.43836859432632663\n",
            "0.43935310390904125\n",
            "0.44018603720323507\n",
            "0.44118064687684977\n",
            "0.4425064962538307\n",
            "0.4438848797317661\n",
            "0.4453148797649862\n",
            "0.44657468201254336\n",
            "0.44785316886804294\n",
            "0.4489889996283499\n",
            "0.4502208912768937\n",
            "0.4514530925342189\n",
            "0.45260043476548645\n",
            "0.4536666517977215\n",
            "0.45444852380496464\n",
            "0.45536094499975827\n",
            "0.4564399107185471\n",
            "0.4573551167155166\n",
            "0.45860772174032755\n",
            "0.45965597506069467\n",
            "0.4606082380732612\n",
            "0.4617019269015173\n",
            "0.4628845601892837\n",
            "0.4642227869052106\n",
            "0.4654386980301889\n",
            "0.4669268779894885\n",
            "0.4681111542160249\n",
            "0.4693515018733871\n",
            "0.4703840320677404\n",
            "0.4713534248606933\n",
            "0.4721575178911009\n",
            "0.4733634623877533\n",
            "0.4746353488291621\n",
            "0.4755814715724467\n",
            "0.476797185590505\n",
            "0.4780304178862316\n",
            "0.47901929453815645\n",
            "0.48018254625522877\n",
            "0.48116041571282975\n",
            "0.48207678964070955\n",
            "0.48259331998617755\n",
            "0.4837015294053061\n",
            "0.4847981796392699\n",
            "0.4857220026233312\n",
            "0.4869015360884654\n",
            "0.4881670609916872\n",
            "0.4891009928320375\n",
            "0.49025641736167164\n",
            "0.4912453576579423\n",
            "0.4922671352353547\n",
            "0.49333054917242825\n",
            "0.49465380704311457\n",
            "0.4958764263583571\n",
            "0.496733047849382\n",
            "0.4977207265394118\n",
            "0.4989417345475053\n",
            "0.4998083348621798\n",
            "0.5011041558459591\n",
            "0.5022980166823053\n",
            "0.5036890529610617\n",
            "0.5048764370896323\n",
            "0.5059423730196551\n",
            "0.5074055461627444\n",
            "0.5085185380542979\n",
            "0.5096066823548369\n",
            "0.5107550159134828\n",
            "0.5119463911141886\n",
            "0.5131390035304877\n",
            "0.514192102205418\n",
            "0.5157189378348153\n",
            "0.5168804084248555\n",
            "0.5184306947471541\n",
            "0.519606384779791\n",
            "0.5205969089437323\n",
            "0.5214721418707572\n",
            "0.5227531656584776\n",
            "0.5239751163650962\n",
            "0.5254204850977339\n",
            "0.5269128529312056\n",
            "0.5281450467951158\n",
            "0.5290639286151018\n",
            "0.5302989947826356\n",
            "0.5311879848732668\n",
            "0.5328009452508844\n",
            "0.5341263949261297\n",
            "0.534912910836432\n",
            "0.5361012549656431\n",
            "0.5370933732108387\n",
            "0.5381072880819325\n",
            "0.5390547847046572\n",
            "0.5401988736046549\n",
            "0.5412440224529227\n",
            "0.5423353131469864\n",
            "0.5431836563760363\n",
            "0.5446423106943555\n",
            "0.545763071007131\n",
            "0.5471368766654178\n",
            "0.5483100422660409\n",
            "0.5491248726692346\n",
            "0.550053309982695\n",
            "0.5513626343911261\n",
            "0.5527663733190893\n",
            "0.5537300040502378\n",
            "0.5547121312002392\n",
            "0.5558336418303077\n",
            "0.5570622685620242\n",
            "0.5577828370396744\n",
            "0.5592315068940068\n",
            "0.5600791138303859\n",
            "0.5616202021346373\n",
            "0.5627279850985388\n",
            "0.5638757129306988\n",
            "0.5650411713916017\n",
            "0.5661813387328096\n",
            "0.5673554291962968\n",
            "0.5680331978041803\n",
            "0.5693716033340415\n",
            "0.5704290449161968\n",
            "0.5715361784791093\n",
            "0.572691582276693\n",
            "0.5737954978747746\n",
            "0.5747789503515833\n",
            "0.5753925408015166\n",
            "0.5761573448434205\n",
            "0.5768895730414354\n",
            "0.5778692117356279\n",
            "0.5790192693319467\n",
            "0.5804898349754036\n",
            "0.5818834359307423\n",
            "0.5827364771796004\n",
            "0.5837120986579324\n",
            "0.5849426257259706\n",
            "0.5865188487579146\n",
            "0.5875960693258764\n",
            "0.5888591955613602\n",
            "0.5902659913996602\n",
            "0.5916586400526563\n",
            "0.5926460495309147\n",
            "0.5935646340898846\n",
            "0.5949072691104601\n",
            "0.5965209276703618\n",
            "0.5976823848455458\n",
            "0.598888136046317\n",
            "0.6000885083471112\n",
            "0.601025405709091\n",
            "0.6022350749624964\n",
            "0.6034597772390337\n",
            "0.6049269435884398\n",
            "0.606012284184051\n",
            "0.6071151794146394\n",
            "0.6082869555105639\n",
            "0.6091403373900581\n",
            "0.6103636032861212\n",
            "0.611547229959227\n",
            "0.6130230949281732\n",
            "0.6144213886440867\n",
            "0.6159660114580409\n",
            "0.6170461736143092\n",
            "0.6184865715329909\n",
            "0.6198782732190988\n",
            "0.6210859498328261\n",
            "0.6223142333424\n",
            "0.6236547021685964\n",
            "0.624592998334209\n",
            "0.6256565964968918\n",
            "0.626778898984575\n",
            "0.6275385636883928\n",
            "0.629002821148204\n",
            "0.6301149428271882\n",
            "0.6309912227989768\n",
            "0.6321593079801715\n",
            "0.6334213928874496\n",
            "0.6344199838007197\n",
            "0.6353939880266823\n",
            "0.63640081901532\n",
            "0.6371892840432389\n",
            "0.638102724462214\n",
            "0.6392144884371087\n",
            "0.640163236604932\n",
            "0.641307333508111\n",
            "0.6423314987012493\n",
            "0.6434465740114221\n",
            "0.6446623625733968\n",
            "0.6459922414377827\n",
            "0.6468500845191424\n",
            "0.6479017906405432\n",
            "0.6493827576756173\n",
            "0.650390590205217\n",
            "0.6517166235791448\n",
            "0.6527093439684499\n",
            "0.6536420169465073\n",
            "0.6546191440137756\n",
            "0.6556489793464656\n",
            "0.6566786536246615\n",
            "0.6579222847204989\n",
            "0.6591229519956862\n",
            "0.6601181757038511\n",
            "0.6613686468518908\n",
            "0.6626965761413355\n",
            "0.663962969199166\n",
            "0.664962617904329\n",
            "0.6661232290289286\n",
            "0.6676930327473394\n",
            "0.6686438227934606\n",
            "0.669973753945297\n",
            "0.6717111500708953\n",
            "0.672842677330117\n",
            "0.67416866390449\n",
            "0.6753717219204549\n",
            "0.6764591903713963\n",
            "0.6776845158289766\n",
            "0.6787633429783995\n",
            "0.6797960292728965\n",
            "0.6815836980291035\n",
            "0.6826133271083807\n",
            "0.6842314262905389\n",
            "0.6857226470013713\n",
            "0.6868406104690888\n",
            "0.6881871127792637\n",
            "0.6894934618335855\n",
            "0.6902829522595686\n",
            "0.6911756714515369\n",
            "0.6921969406363909\n",
            "0.6933200741591661\n",
            "0.6941497590764404\n",
            "0.6957292803336897\n",
            "0.6965422157360159\n",
            "0.6975224981146395\n",
            "0.6992063630191262\n",
            "0.7003363440637393\n",
            "0.7016666971737772\n",
            "0.7025773450160575\n",
            "0.7034832221239119\n",
            "0.7046795967975845\n",
            "0.7058558734252934\n",
            "0.7066640994509162\n",
            "0.7078923905825676\n",
            "0.7088434406177467\n",
            "0.7105017261355734\n",
            "0.7118145961819402\n",
            "0.7129479154677647\n",
            "0.7138616162569017\n",
            "0.7148064087190287\n",
            "0.7160174318820315\n",
            "0.7171573080598851\n",
            "0.718590965318253\n",
            "0.7197890695937149\n",
            "0.721256538996916\n",
            "0.7227219210180176\n",
            "0.7239152179730822\n",
            "0.7251362685123672\n",
            "0.7264615282454454\n",
            "0.7280689551473578\n",
            "0.7293152440615627\n",
            "0.7306653624376678\n",
            "0.7316593448905384\n",
            "0.7330664160382717\n",
            "0.7342509941752914\n",
            "0.7353596787547212\n",
            "0.7363515498159486\n",
            "0.7375292234942127\n",
            "0.7391043632765255\n",
            "0.740195137422408\n",
            "0.7412303176606098\n",
            "0.742369200453124\n",
            "0.7435632253927953\n",
            "0.7445973098811591\n",
            "0.7455067648683362\n",
            "0.7467849719935976\n",
            "0.7476743150626302\n",
            "0.749088354008582\n",
            "0.7504376822801502\n",
            "0.7514489971844437\n",
            "0.7523461041014518\n",
            "0.7535177308046604\n",
            "0.7549843822065216\n",
            "0.7564910385386109\n",
            "0.757874313256015\n",
            "0.758865221572654\n",
            "0.7596093142200309\n",
            "0.7605979464319356\n",
            "0.7618753005705221\n",
            "0.7631244887705044\n",
            "0.7639849823530372\n",
            "0.7648880021727603\n",
            "0.7659848029427516\n",
            "0.7674903025483841\n",
            "0.7688405849515935\n",
            "0.7699802713397214\n",
            "0.7711882464339971\n",
            "0.7725307623977247\n",
            "0.7734824359188299\n",
            "0.7746178977705939\n",
            "0.7760234170633814\n",
            "0.7771952472760549\n",
            "0.7784775837379343\n",
            "0.7796819775229524\n",
            "0.7807877858352783\n",
            "0.7822733558428562\n",
            "0.783252662138256\n",
            "0.7844914437636085\n",
            "0.7857936915305569\n",
            "0.7868557430975273\n",
            "0.7881796104295175\n",
            "0.7891865089497603\n",
            "0.7899808061625951\n",
            "0.7915650511260532\n",
            "0.7925442528465519\n",
            "0.7936277264143194\n",
            "0.7948862453327155\n",
            "0.7961181608383613\n",
            "0.7972327577869606\n",
            "0.79819741262042\n",
            "0.7990341941490198\n",
            "0.7998908320275109\n",
            "0.8009210841353896\n",
            "0.8021043395371084\n",
            "0.8032387554111993\n",
            "0.8045455288627873\n",
            "0.8054908042597344\n",
            "0.8067237857891165\n",
            "0.8075721210149853\n",
            "0.8086571580994769\n",
            "0.8097559858084945\n",
            "0.8111083932468653\n",
            "0.8124942677024075\n",
            "0.8134876557094667\n",
            "0.8146873101630174\n",
            "0.8156841274188913\n",
            "0.8170018076058239\n",
            "0.8179528662539504\n",
            "0.8193110811054859\n",
            "0.8203333897130264\n",
            "0.8216773921342762\n",
            "0.8230050275164187\n",
            "0.8241552365634143\n",
            "0.8253682003835278\n",
            "0.8268468400935078\n",
            "0.8281923088499957\n",
            "0.8293978912979746\n",
            "0.8310195797163508\n",
            "0.832102061880519\n",
            "0.8331954802012504\n",
            "0.8343090391753579\n",
            "0.8354150304175398\n",
            "0.8364857165786006\n",
            "0.8377891783900273\n",
            "0.838901474081037\n",
            "0.8401848229072283\n",
            "0.8414555973637744\n",
            "0.8423201530562032\n",
            "0.8436839414755707\n",
            "0.845294063696471\n",
            "0.8462019627127806\n",
            "0.8473009578788372\n",
            "0.8483758739879369\n",
            "0.8495405874670009\n",
            "0.8503950054154676\n",
            "0.8514081353650373\n",
            "0.8526441321882141\n",
            "0.8538096901172262\n",
            "0.8548856254505075\n",
            "0.8561040671051615\n",
            "0.8572132083994654\n",
            "0.858653922481915\n",
            "0.8596247738355871\n",
            "0.8605319363305636\n",
            "0.8616835661327747\n",
            "0.8624509646535834\n",
            "0.8639009810240982\n",
            "0.8648095706006145\n",
            "0.8659932579454559\n",
            "0.8670455198687361\n",
            "0.8679756218438868\n",
            "0.8692913768465257\n",
            "0.870505776132464\n",
            "0.8715938001566226\n",
            "0.872695154050732\n",
            "0.8737511292976492\n",
            "0.8752312097326874\n",
            "0.8763138080573143\n",
            "0.877535437874477\n",
            "0.8785541501572675\n",
            "0.8798274448536851\n",
            "0.8813366455876309\n",
            "0.8823100799870918\n",
            "0.8833437588284997\n",
            "0.8848633774177498\n",
            "0.8864247716982346\n",
            "0.8873244455784483\n",
            "0.888604821184712\n",
            "0.8896711521670032\n",
            "0.8904200316313893\n",
            "0.8913527536194038\n",
            "0.8922976890717016\n",
            "0.8935057532680614\n",
            "0.8946882365533458\n",
            "0.8959450008695388\n",
            "0.8968446784845704\n",
            "0.8983113780579604\n",
            "0.8995697871422219\n",
            "0.9007437062614104\n",
            "0.9018685940815054\n",
            "0.9034016115205062\n",
            "0.9042418589601127\n",
            "0.905396339038144\n",
            "0.9065753189118012\n",
            "0.9083500025446153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ2rXxD0hqnl",
        "outputId": "ce9bbe3c-7679-42d7-bc82-d97a565866e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 75.70%\n"
          ]
        }
      ]
    }
  ]
}